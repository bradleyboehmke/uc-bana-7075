[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Design for Business",
    "section": "",
    "text": "Welcome\nWelcome to Machine Learning Design for Business! This is the the online textbook that compliments the UC BANA 7085 course. This course aims to provide a framework for developing real-world machine learning systems that are deployable, reliable, and scalable. You will be introduced to MLOps as the intersection of DataOps, ModelOps, and DevOps; combined, these concepts provide scalable, reliable, and repeatable machine learning workflows. You will learn how MLOps enables organizations to overcome challenges in operationalizing machine learning models. Along the way, you will learn about important organizational issues including privacy, fairness, security, stakeholder management, project collaboration, and more!\nThroughout this textbook you’ll find embedded lectures, hands-on exercises, and additional resources that will allow you to explore the tools, techniques, and best practices required to successfully design and deploy machine learning systems in today’s organizations.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-should-read-this",
    "href": "index.html#who-should-read-this",
    "title": "Machine Learning Design for Business",
    "section": "Who should read this",
    "text": "Who should read this\nThis book is ideal for graduate students, data scientists, engineers, and analytics professionals who want to bridge the gap between building machine learning models and deploying them into reliable, scalable production environments. Those looking to deepen their understanding of operationalizing ML models will benefit from hands-on experience with MLOps workflows, including data management, model deployment, and monitoring. Individuals with a basic background in machine learning or software development will gain practical skills to streamline the machine learning lifecycle and align their work with organizational and business goals.\nThis book is also valuable for those interested in cross-functional collaboration, providing insights into how data science teams can work alongside engineering and business units to achieve successful machine learning outcomes.\nThis book is technical in nature, designed to provide hands-on experience in deploying, monitoring, and managing machine learning systems. While it’s possible to successfully complete the book with minimal coding experience, having some familiarity with Python and basic machine learning concepts will be beneficial for understanding and applying the material. The course introduces technical tools and workflows, but it’s structured to guide students of various backgrounds through each step. Those with prior experience in Python or ML will have a smoother time grasping the concepts and may find it easier to implement projects independently. However, foundational knowledge in programming or machine learning is not strictly required, as core principles and step-by-step instructions are provided to support all readers in building effective MLOps pipelines.\n\n\n\n\n\n\nIf you are new to Python, I recommend you check out https://bradleyboehmke.github.io/uc-bana-6043 to get up to speed with the basics.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "Machine Learning Design for Business",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nThe following typographical conventions are used in this book:\n\nstrong italic: indicates new terms,\nbold: indicates package & file names,\ninline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,\ncode chunk: indicates commands or other text that could be typed literally by the user\n\n\n1 + 2\n\n3\n\n\nIn addition to the general text used throughout, you will notice the following code chunks with images:\n\n\n\n\n\n\nSignifies a tip or suggestion\n\n\n\n\n\n\n\n\n\nSignifies a general note\n\n\n\n\n\n\n\n\n\nSignifies a warning or caution",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#software-used-throughout-this-book",
    "href": "index.html#software-used-throughout-this-book",
    "title": "Machine Learning Design for Business",
    "section": "Software used throughout this book",
    "text": "Software used throughout this book\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Machine Learning Design for Business",
    "section": "Additional resources",
    "text": "Additional resources\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html",
    "href": "01-intro-ml-system.html",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "",
    "text": "1.1 ML System Design\nMachine Learning (ML) System Design is the discipline of creating reliable, scalable, and maintainable machine learning systems that solve real-world business problems. Unlike standard machine learning modeling, which focuses primarily on algorithm development and evaluation, ML System Design considers the broader ecosystem in which a model must operate. This involves defining the architecture, infrastructure, and processes that make a model functional and sustainable within production environments. As organizations increasingly rely on machine learning to make data-driven decisions, robust system design has become essential for delivering models that consistently meet organizational needs and can adapt to change.\nEffective ML system design enables organizations to avoid many pitfalls of deploying machine learning in production. Poorly designed systems can lead to issues like inaccurate predictions due to data drift, high operational costs, or significant downtime during retraining or model updates. By adhering to design principles such as modularity, scalability, and reproducibility, ML engineers and data scientists can develop systems that are more resilient and easier to manage. These principles form the backbone of good ML system design and help ensure that models can not only be deployed but also monitored, improved, and scaled as needed.\nThis section will explore these design principles in detail and examine how they align with key stages of the ML system lifecycle, from data processing and model training to deployment, monitoring, and iteration. As Chip Huyen points out, “Machine learning in production is 10% modeling and 90% engineering” (Huyen 2022), highlighting how technical choices impact the system’s reliability and long-term value for the organization. Understanding ML System Design and the lifecycle of ML systems is foundational for anyone working in machine learning, as it highlights how technical choices impact the system’s overall reliability and long-term value for the organization.\nML system design refers to the process of building, structuring, and integrating machine learning models into larger production environments to deliver reliable, scalable, and sustainable solutions. While traditional machine learning focuses on developing high-performing models, system design extends the focus to creating an entire ecosystem where these models can continuously operate, adapt, and provide value over time. It encompasses everything from data pipelines and infrastructure to model versioning, monitoring, and scaling, ensuring that machine learning projects align with organizational goals and can be maintained as data, requirements, and technology evolve.\nTherefore, ML system design includes multiple facets beyond the algorithms themselves:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#ml-system-design",
    "href": "01-intro-ml-system.html#ml-system-design",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "",
    "text": "TODO\n\n\n\nAdd wire diagram to depict ML system in an organization - showing business requirements driving MLOps components leading to the end user.\n\n\n\n1.1.1 DataOps: Data Management and Pipelines\nAt the heart of any ML system is data, which is processed, transformed, and delivered to the model in a way that supports both training and inference. A well-designed ML system typically includes data pipelines that handle:\n\nData ingestion: Collecting data from various sources, including databases, APIs, and real-time streams.\nData processing: Cleaning, transforming, and structuring data for model consumption.\nData validation: Ensuring that data quality is maintained and meets the standards required by the model.\nData versioning and lineage: Tracking data versions and maintaining a record of data transformations, which are critical for reproducibility and regulatory compliance.\n\n\n\n1.1.2 ModelOps: Model Lifecycle Management\nA central aspect of ML system design is managing the entire model lifecycle, from development and testing to deployment and retirement. This includes:\n\nExperiment tracking and versioning: Maintaining a record of model versions, hyperparameters, and evaluation metrics to track performance and support reproducibility.\nDeployment pipelines: Automating model deployment processes to reduce manual errors and accelerate time-to-production.\nMonitoring and alerting: Establishing mechanisms for monitoring model performance in production and detecting issues such as data drift, concept drift, or system degradation.\nContinuous improvement: Supporting processes for regular retraining, fine-tuning, and updating models as data and requirements change.\n\n\n\n1.1.3 DevOps Practices\nGood ML system design brings DevOps practices to the world of machine learning. DevOps, short for “Development and Operations,” is a set of practices and principles that combines software development (Dev) and IT operations (Ops) with the goal of shortening the software development lifecycle and delivering high-quality, reliable software more efficiently. At its core, DevOps is about fostering a culture of collaboration between development and operations teams, integrating automated processes, and leveraging continuous integration and continuous deployment (CI/CD) practices to streamline workflows. This includes:\n\nAutomating code and model updates to streamline deployment and minimize downtime.\nAutomation of testing and validation: Implementing automated tests to validate model performance, accuracy, and reliability.\nCompute resources: Setting up scalable, flexible compute resources (such as cloud instances, GPUs, or Kubernetes clusters) to handle model training and inference.\nStorage solutions: Designing data storage strategies that balance cost, speed, and accessibility.\nNetworking and security: Ensuring secure data transfer and protecting sensitive information while allowing fast access for machine learning processes.\nCollaboration between teams: Encouraging collaboration across data science, engineering, and business teams to ensure that models meet organizational requirements and add real value.\n\n\n\n\n\n\n\nDriven by business requirements\n\n\n\nThe design of each key element in an ML system is ultimately driven by business requirements, as they define the purpose and value that the system should deliver. Business goals shape how data is processed, what metrics the model optimizes, and the reliability, scalability, and security standards it must meet. For example, a financial institution deploying a fraud detection model may prioritize stringent data validation and versioning to comply with regulatory standards, while a retail organization focused on real-time recommendations may emphasize rapid data ingestion and low-latency deployment.\nModel lifecycle management is similarly guided by business needs; frequent retraining cycles, experiment tracking, and monitoring capabilities are crucial when a system must adapt to fast-changing consumer behavior or market trends. Infrastructure choices, such as selecting between cloud and on-premise solutions, also reflect organizational constraints, like budget or resource availability.\nBy aligning these elements with clear business objectives, organizations can create ML systems that are not only technically sound but also effective in achieving their strategic goals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#why-ml-system-design-matters",
    "href": "01-intro-ml-system.html#why-ml-system-design-matters",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.2 Why ML System Design Matters",
    "text": "1.2 Why ML System Design Matters\nML system design addresses the challenges of productionizing machine learning. In a laboratory or research setting, machine learning models are often developed with a focus on achieving high accuracy or minimizing error on a given dataset. However, deploying these models in production involves a different set of concerns. In real-world applications, ML systems need to handle fluctuating data distributions, evolving user needs, and high availability. Without a robust design, machine learning models can quickly degrade in performance, lead to incorrect predictions, or even disrupt business operations.\nBy prioritizing system design, organizations can build ML workflows that are both responsive to change and maintainable over time. This approach reduces technical debt—the accumulation of outdated or poorly designed code, data dependencies, and complex interactions that are difficult to fix — highlighted by the authors in Hidden Technical Debt in Machine Learning Systems as a critical challenge in ML applications (Sculley et al. 2015). Poorly managed dependencies and hidden feedback loops, for instance, can lead to unpredictable model behavior and complicate future updates. Thoughtful ML system design mitigates these issues, allowing models to deliver consistent, reliable results while aligning predictions more closely with organizational metrics and objectives. Ultimately, well-architected ML systems are more adaptable, enabling models to evolve as business goals and data distributions shift over time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#design-principles-for-good-ml-system-design",
    "href": "01-intro-ml-system.html#design-principles-for-good-ml-system-design",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.3 Design Principles for Good ML System Design",
    "text": "1.3 Design Principles for Good ML System Design\nCreating an effective ML system involves more than just building an accurate model; it requires thoughtful design to ensure that the system remains reliable, scalable, and adaptable over time. Good ML system design incorporates a set of principles that help address real-world challenges in production environments, such as evolving data, model drift, and changing business requirements. The following design principles guide ML engineers and data scientists in building systems that meet these demands and deliver sustained business value.\n\n\n\n\n\n\nTODO\n\n\n\nAdd image that shows all principles\n\n\n\n1.3.1 Modularity and Abstraction\nModularity is the practice of dividing a system into independent, self-contained components, each responsible for a specific function. In an ML system, this could mean separating data ingestion, preprocessing, training, and monitoring into distinct modules that can operate independently (Sculley et al. 2015). This modularity simplifies updates and maintenance, as each component can be modified or scaled without disrupting the entire system. For example, if a new feature engineering technique improves model accuracy, it can be implemented in the data preprocessing module without affecting the deployment pipeline.\nAbstraction, on the other hand, hides complex details within each module, making the system easier to manage and more accessible to team members who don’t need to understand every technical detail. As noted by Huyen (2022), abstraction helps bridge the gap between data scientists, engineers, and business stakeholders, allowing each to focus on high-level functionality without needing an in-depth understanding of every process. Together, modularity and abstraction improve collaboration, flexibility, and ease of scaling within ML systems.\n\n\n1.3.2 Scalability\nScalability ensures that an ML system can handle increased demand—whether in terms of data volume, number of users, or computational load—without requiring extensive re-engineering. Designing for scalability involves choosing infrastructure and tools that allow for efficient resource allocation and growth. For instance, deploying a model on cloud infrastructure, where compute resources can be dynamically allocated, allows an ML system to scale up to handle peak loads and scale down during quieter periods, optimizing costs and performance (Levine et al. 2020).\nAnother aspect of scalability is planning for future expansion. An ML system might start with a few users or limited data, but as it proves valuable, the system may need to accommodate more data sources, larger user bases, or additional model versions. Scalable design considers these future needs from the outset, making it easier to extend the system’s capabilities without significant architectural changes.\n\n\n1.3.3 Reproducibility\nReproducibility is critical for ML systems, particularly in production settings where consistent results and traceability are essential. Reproducibility ensures that anyone with access to the same code, data, and configuration can recreate a model with the same results. This is essential for troubleshooting, compliance, and validating model performance. As described by Amershi et al. (2019), reproducibility allows teams to understand how specific predictions are made and to resolve any issues that arise in production more efficiently.\nAchieving reproducibility requires careful versioning of data, models, and code. Version control tools like Git and DVC (Data Version Control) help keep track of changes to code and data over time, while tools like MLflow or Weights & Biases allow for experiment tracking and versioning of model parameters. Reproducibility is especially important in regulated industries, where organizations may need to demonstrate how a model made a particular prediction or prove compliance with specific standards.\n\n\n1.3.4 Automation\nAutomation is a foundational principle in ML system design, as it reduces manual intervention, minimizes human error, and accelerates workflows. As described by Polyzotis et al. (2019), automation is particularly valuable in environments where frequent model retraining, testing, or deployment is necessary. For instance, an automated pipeline can retrain a model when new data is available, perform validation checks, and deploy the updated model to production, all without requiring human involvement.\nKey areas for automation in ML systems include data ingestion and preprocessing, model training and evaluation, deployment, and monitoring. Automating these stages not only saves time but also ensures that the system operates consistently and can adapt to changing data and conditions. This is crucial in fast-paced environments, such as e-commerce or finance, where systems must quickly respond to new information.\n\n\n1.3.5 Monitoring and Maintenance\nMonitoring and maintenance are essential for keeping an ML system aligned with its intended goals over time. Once a model is deployed, its performance can be affected by data drift (changes in data distribution) or concept drift (changes in the underlying relationships within the data) (Gama et al. 2014). Without monitoring, these issues can lead to model degradation, resulting in inaccurate predictions or decisions that don’t meet business objectives.\nGood ML system design includes monitoring dashboards and automated alerts that track metrics such as prediction accuracy, latency, data drift, and system errors. Tools like Prometheus and Grafana can help set up performance monitoring, while specialized ML monitoring solutions can track model-specific metrics. Regular maintenance practices, such as retraining or recalibrating models, are also essential for sustaining system performance and adapting to new data patterns or business requirements.\n\n\n1.3.6 Security and Compliance\nSecurity and compliance are often overlooked in ML system design but are increasingly important, especially in regulated industries like healthcare, finance, and government. An ML system that processes sensitive data must include security measures to protect data integrity and confidentiality. Security considerations include encrypting data in transit and at rest, managing user access controls, and safeguarding against adversarial attacks (Papernot et al. 2017).\nCompliance involves ensuring that the ML system adheres to industry regulations and organizational policies. For instance, GDPR compliance may require an ML system to provide transparency into how predictions are made and to allow users to request data deletions. Designing with security and compliance in mind not only protects the organization but also builds trust with users and stakeholders.\n\n\n\n\n\n\nWhat’s GDPR?\n\n\n\n\n\nThe General Data Protection Regulation (GDPR) is a comprehensive privacy law enacted by the European Union in 2018, aimed at protecting the personal data and privacy of individuals within the EU and the European Economic Area (EEA). GDPR sets strict guidelines on how organizations must collect, store, process, and share personal data, giving individuals more control over their information. Key principles of GDPR include data minimization, purpose limitation, and transparency, along with rights for individuals such as the right to access, correct, and delete their data. Non-compliance with GDPR can lead to significant fines, making it essential for companies, especially those dealing with machine learning systems, to prioritize data security and privacy to ensure compliance.\n\n\n\n\n\n1.3.7 Adaptability and Flexibility\nAdaptability is the ability of an ML system to evolve as data, business requirements, and technologies change. In a dynamic business environment, the needs of an ML system may shift over time, requiring the model to be updated, new features to be added, or the infrastructure to be reconfigured (Sculley et al. 2015). Designing for adaptability involves using flexible frameworks, modular components, and tools that support rapid changes without extensive rework.\nFor example, a well-designed ML system might use containerization (e.g., Docker) to allow models to be easily moved across environments or use APIs to integrate new data sources without significant restructuring. This flexibility enables teams to quickly implement changes, test new ideas, and stay aligned with business goals, making the system more resilient to future changes.\n\n\n1.3.8 Putting It All Together\nThese principles — modularity, scalability, reproducibility, automation, monitoring and maintenance, security, and adaptability — form the foundation of good ML system design. Each principle contributes to creating a system that is not only technically robust but also capable of delivering sustained business value. By applying these principles, ML engineers and data scientists can build systems that are flexible, resilient, and scalable, ensuring that machine learning models remain effective and reliable over time. Good ML system design allows organizations to move beyond individual models and create a stable, agile infrastructure that continuously adapts to meet evolving business demands.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#relation-to-ml-system-lifecycle-stages",
    "href": "01-intro-ml-system.html#relation-to-ml-system-lifecycle-stages",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.4 Relation to ML system lifecycle stages",
    "text": "1.4 Relation to ML system lifecycle stages\nGood ML system design requires a holistic approach that considers how key elements and design principles intersect with each stage of the ML system lifecycle. The ML lifecycle can be broken down into distinct stages: data processing, model development, deployment, monitoring, and continuous improvement. By aligning design principles with these stages, organizations can create systems that are not only effective at deployment but sustainable, adaptable, and closely aligned with business objectives over the long term.\n\n\n\n\n\n\nTODO\n\n\n\nAdd image that shows ML system lifecycle stages\n\n\n\n1.4.1 Data Processing\nThe data processing stage encompasses everything from data ingestion and transformation to validation and storage. Principles like modularity and automation are especially valuable here, as they allow data pipelines to handle diverse data sources and adapt to changes without disrupting downstream processes. DataOps practices ensure the accuracy and integrity of data, including validation and lineage tracking, so the right data reaches the model reliably. Reproducibility also plays a crucial role at this stage, as versioned datasets and pipelines help maintain consistency, which is vital for model retraining and compliance.\n\n\n1.4.2 Model Development\nModel development involves experimentation, model training, and evaluation. Here, modularity and reproducibility are essential for managing experiments and enabling easy comparison of results across different model versions. ModelOps practices, including experiment tracking and model versioning, facilitate tracking hyperparameters, code versions, and results, ensuring models can be accurately reproduced and refined. Scalability is another key consideration, especially for large datasets or complex models that require significant computational resources. In this stage, infrastructure choices, such as cloud or GPU-based solutions, allow the system to scale up as needed.\n\n\n1.4.3 Deployment\nThe deployment stage focuses on moving models from development to production in a seamless, automated, and reliable manner. Automation is paramount, with CI/CD pipelines automating the testing, validation, and deployment processes to minimize human error and expedite time-to-production. DevOps principles enable automated deployment processes that integrate testing and validation steps, reducing downtime and increasing reliability. Security also becomes critical, ensuring that data access and model endpoints are protected against unauthorized access or adversarial attacks.\n\n\n1.4.4 Monitoring and Maintenance\nOnce a model is live, continuous monitoring is essential to track key metrics such as accuracy, latency, and model drift, ensuring that the model performs as expected over time. Monitoring and maintenance principles are implemented through tools and dashboards that monitor data drift and system performance. Alerts can notify teams of anomalies or degradation, enabling quick action to retrain or update the model if needed. Automation also supports regular retraining workflows, particularly in dynamic environments where data patterns shift frequently. Maintenance ensures that the ML system remains robust and responsive to changing data and requirements.\n\n\n1.4.5 Continuous Improvement\nThe continuous improvement stage focuses on enhancing the model based on feedback from monitoring and evolving business needs. Adaptability and flexibility are essential here, allowing the system to integrate new data, retrain models, and test enhancements with minimal friction. Scalability and modularity enable models to grow and improve without requiring complete redesigns of the system. In practice, this means designing systems with modular components that can easily integrate new models or data sources, ensuring that the system remains aligned with both technical advancements and organizational goals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#summary",
    "href": "01-intro-ml-system.html#summary",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we introduced the concept of Machine Learning (ML) System Design, focusing on how it creates a foundation for building reliable, scalable, and adaptable machine learning systems that can deliver value in real-world production environments. Unlike traditional model development, ML system design considers the entire ecosystem, encompassing data pipelines, model lifecycle management, and operational infrastructure, all shaped by business requirements. By following core design principles — such as modularity, scalability, reproducibility, automation, monitoring, and security — ML systems can avoid common production pitfalls like model drift, high operational costs, and downtime, ensuring that models are not only deployed successfully but also perform reliably over time.\nThe chapters that follow will expand on each element of ML system design, providing readers with a deep understanding of the key concepts and principles that enable the creation of robust ML systems. Readers will gain the ability to identify essential design decisions that align with business goals, as well as the tools and practices necessary to implement these considerations effectively. By the end of this book, readers will be equipped to build, manage, and maintain ML systems that are both technically sound and responsive to evolving business needs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#exercise",
    "href": "01-intro-ml-system.html#exercise",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.6 Exercise",
    "text": "1.6 Exercise\nRead one or more of the following case studies. These case studies cover different perspectives, elements, and principles of ML system design.\n\nWhat are the primary business objective(s) for the ML system discussed? How does the system add value for both the organization and its customers?\nAnalyze key elements and design principles discussed. For example:\n\nDataOps: How does the organization handle data ingestion, processing, and versioning to support a constantly changing catalog and user preferences?\nModelOps: What strategies does the organization use to manage the lifecycle of its models, including retraining, versioning, and experimentation?\nDevOps: How does the organization ensure continuous integration and deployment for its models? What automation, testing, and monitoring practices are used to maintain system reliability?\nDesign Principles: Identify examples of modularity, scalability, automation, and monitoring within the organization’s ML system. Explain how each principle contributes to the system’s success.\n\n\n\n\n\n\n\n\nNote that these articles will likely mention technical concepts and tools that you may not be familiar with and feel like they are over your head. Do not worry about this! This book will help to bridge some of these gaps.\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\n\n\nNetflix: Netflix’s Recommendation System: Algorithms, Business Value, and Innovation\nUber: Scaling Machine Learning at Uber with Michelangelo\nSpotify: Inside Spotify’s Recommender System: A Complete Guide to Spotify Recommendation Algorithms\nAirbnb: Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform\nGoogle: TFX - an end-to-end machine learning platform for TensorFlow\nLinkedIn: Scaling Machine Learning Productivity at LinkedIn\nFacebook: Introducing FBLearner Flow: Facebook’s AI backbone\nBooking.com: 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com\nTwitter: Twitter’s Recommendation Algorithm\n\n\n\n\n\n\n\n\nAmershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019. “Software Engineering for Machine Learning: A Case Study.” In 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 291–300. IEEE.\n\n\nGama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. 2014. “A Survey on Concept Drift Adaptation.” ACM Computing Surveys (CSUR) 46 (4): 1–37.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. \" O’Reilly Media, Inc.\".\n\n\nLevine, Sergey, Aviral Kumar, George Tucker, and Justin Fu. 2020. “Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.” arXiv Preprint arXiv:2005.01643.\n\n\nPapernot, Nicolas, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. “Practical Black-Box Attacks Against Machine Learning.” In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 506–19.\n\n\nPolyzotis, Neoklis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven Whang. 2019. “Data Validation for Machine Learning.” Proceedings of Machine Learning and Systems 1: 334–47.\n\n\nSculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. 2015. “Hidden Technical Debt in Machine Learning Systems.” Advances in Neural Information Processing Systems 28.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html",
    "href": "02-before-we-build.html",
    "title": "2  Before We Build",
    "section": "",
    "text": "2.1 Working with Stakeholders to Understand the Business Problem\nBuilding an effective machine learning (ML) system requires more than just technical knowledge and model-building skills. The success of an ML project depends heavily on the foundational planning that happens long before the first line of code is written or the first dataset is processed. This planning phase is essential to ensuring that the project aligns with the business goals, addresses the right problem, and is both technically feasible and valuable. In this chapter, we’ll explore the key considerations and actions to take before starting the development of an ML system.\nThe first step in any ML project is to collaborate closely with stakeholders to fully understand the business problem. This stage is critical because a thorough understanding of the problem directly impacts the translation of business needs into a solvable analytics problem, helps scope the project realistically, and ensures the ML system will truly meet organizational goals. Stakeholders include anyone with a vested interest in the project’s success—such as executives, department leads, data science teams, IT, and end users. Each group brings a unique perspective, and understanding their needs and expectations provides clarity on objectives, potential constraints, and the broader impact of the solution.\nEffective communication with stakeholders helps to align everyone on the project’s purpose and desired outcomes. By having these early conversations, ML practitioners gain insight into business priorities, potential data requirements, and assumptions being made about data availability, user behavior, and expected outcomes. Common assumptions that come to light in these initial meetings might include beliefs about data quality or completeness, end-user expectations of functionality, or even implicit views on the model’s potential accuracy and impact. Identifying and addressing these assumptions early on prevents misalignment and reduces the risk of costly adjustments later in the project.\nTo gain a comprehensive understanding of the business problem, ML practitioners should consider key questions with stakeholders, such as:\nThese questions help not only to clarify the business goals but also to highlight how the problem should be reframed into an analytics problem. For instance, a high-level business problem like “improving customer retention” can translate into an analytics problem focused on predicting customer churn and identifying key factors contributing to it. Defining the problem at this level makes it easier to identify relevant data, narrow down model requirements, and set realistic goals.\nAdditionally, early discussions often uncover essential requirements and constraints, such as data dependencies, privacy concerns, or specific regulatory requirements. By clarifying these aspects, ML teams can make more informed decisions on technical design, data preprocessing, and timeline estimation. Furthermore, gaining a clear understanding of the end-users’ needs helps in defining how the ML system should be integrated into existing workflows, ensuring the solution is not only technically robust but also user-friendly and practical.\nIn summary, investing time upfront to deeply understand the business problem and address underlying assumptions results in a more targeted, relevant ML solution. This shared understanding among stakeholders lays the groundwork for a smooth translation of business objectives into technical requirements, helping to scope the project effectively and set it up for long-term success.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#working-with-stakeholders-to-understand-the-business-problem",
    "href": "02-before-we-build.html#working-with-stakeholders-to-understand-the-business-problem",
    "title": "2  Before We Build",
    "section": "",
    "text": "What specific problem are we trying to solve?\nWhat are the business objectives associated with addressing this problem?\nWho will use the ML system, and what actions or decisions will it facilitate?\nWhat are the potential risks or consequences if the system produces inaccurate or unintended results?\nAre there existing solutions or processes in place, and what are their limitations?\n\n\n\n\n\n\n\n\n\n\nIsn’t this a project manager responsibility?\n\n\n\n\n\nData scientists should be involved in the early planning process of an ML project because their expertise in data and analytics enables them to assess the feasibility of proposed solutions, clarify data requirements, and identify potential technical challenges. While some ML practitioners may balk at including planning, communication, brainstorming, and other project-management-focused elements in discussions on ML projects, these early stages are often where critical project alignment occurs. In my experience, the most successful projects have involved ML team leads working closely not only with project managers and other team leads but also with representatives from the department requesting the solution. Data scientists bring essential knowledge about data limitations, model requirements, and potential roadblocks that might otherwise go unnoticed. Their involvement helps refine the project’s scope to align with both the business goals and technical realities, setting realistic expectations and paving the way for a solution that is both effective and feasible. This collaborative approach during planning ultimately saves time, enhances clarity, and boosts the likelihood of delivering a meaningful, successful ML solution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#determining-if-the-problem-requires-an-ml-solution",
    "href": "02-before-we-build.html#determining-if-the-problem-requires-an-ml-solution",
    "title": "2  Before We Build",
    "section": "2.2 Determining if the Problem Requires an ML Solution",
    "text": "2.2 Determining if the Problem Requires an ML Solution\nNow that we understand the importance of working with stakeholders to fully grasp the business problem, the next step is to translate the business problem into an analytics problem. This process will help us determine whether machine learning (ML) is the right approach to address the issue. ML can be a powerful tool, but it’s not always the best solution. Determining if ML is suitable requires thoughtful consideration, as unnecessary ML implementation can lead to wasted resources, increased complexity, and maintenance challenges.\n\n2.2.1 Key Considerations to Determine ML Suitability\nDetermining if a problem requires an ML solution involves assessing whether the business objectives demand certain characteristics that ML can uniquely provide.\n\nSome essential considerations include:\n\nLearning Complex Patterns from Data: ML excels when the solution requires identifying complex, often non-linear relationships within data that are difficult to capture using rules-based or traditional statistical approaches. If the problem involves intricate patterns that need to be learned from large volumes of data, ML may be appropriate. For example, ML can be effective at analyzing the complex and non-linear relationships between a customer’s purchase history, browsing behavior, and demographic characteristics to generate personalized product recommendations.\nAvailability of Quality Data: ML solutions require substantial and high-quality data for training and testing. Without sufficient data, the model cannot learn the patterns or features necessary to make accurate predictions. Data availability should include not only a large enough sample size but also a diverse dataset that represents the full range of scenarios the model will encounter in production. If quality data is unavailable, ML is unlikely to perform well and may not be worth pursuing.\nMaking Predictions on Unseen Data: ML is suitable for problems where we need the system to make predictions on new, unseen data that will continue to arrive over time. For instance, in fraud detection, the goal is for the system to recognize fraudulent behavior in real-time as new transactions occur. This is an inherently predictive task and well-suited to ML, where models trained on historical fraud data can be applied to identify anomalies in new transactions.\n\n\n\n2.2.2 ML is Not Always the Answer\nMachine learning is a powerful tool, but it’s not a one-size-fits-all solution. Yet, in today’s fast-paced tech landscape, it’s common for business leaders to latch onto popular trends like ML, AI, and automation, believing that these technologies can solve a wide range of problems or offer an innovative edge. However, the reality is that many problems don’t require the complexity or resources of an ML system and can actually be addressed more effectively with simpler approaches. Below are examples of problems that often don’t meet the suitability criteria for ML, illustrating why a different solution may be more appropriate.\n\nRule-Based Decision Systems: If a problem can be solved with straightforward rules or conditions, then ML is likely unnecessary. For example, an e-commerce site offering a standard discount based on membership level (e.g., 10% off for premium members) may not need a predictive model to calculate discounts. A simple rule-based system is faster to implement, more interpretable, and easier to maintain. Business leaders sometimes assume that ML will increase personalization or provide better insights, but in some cases, it may only add complexity without significant, meaningful benefits.\nBasic Data Retrieval and Filtering: When a task involves retrieving specific records or filtering data based on fixed criteria, ML is overkill. For instance, if a business wants a list of customers who made a purchase within the last month, a structured query in a database will suffice. ML could add unnecessary processing time, reduce transparency, and complicate data workflows without providing any added value. Leaders may be drawn to the idea of “automating data processes with ML,” but it’s critical to recognize that traditional data querying and reporting tools are often more efficient and will suffice when the goal is to simply slice and dice our data.\nStatic Reporting and Descriptive Analytics: ML is valuable for uncovering patterns, making predictions, or adapting to new data, but if the goal is simply to summarize historical data or calculate aggregate statistics (e.g., total sales last quarter or average customer lifetime value), traditional data analysis tools are more suitable. ML brings no additional value to tasks that involve reporting known facts. Although ML may sound impressive, using it for simple reporting can lead to unnecessary costs in computation, maintenance, and monitoring.\nTasks with Deterministic Outputs: When a problem has predictable, deterministic outputs based on clear rules, ML can be redundant. For example, payroll tax calculation, which follows specific formulas, does not require a model to “learn” the process. Hard-coded logic or rule-based systems are not only more accurate in such cases but also eliminate potential errors introduced by an ML model. Business leaders sometimes believe that ML will “future-proof” deterministic tasks, but simpler systems are usually more reliable, cost-effective, and transparent.\nHeavily Regulated Decisions Requiring Full Transparency: In industries such as finance or healthcare, strict regulations often require full transparency for every decision, making ML—especially complex models—an unsuitable choice. For instance, mortgage approvals based on a set of criteria are better handled by rules-based systems to ensure regulatory compliance and easy interpretability. While ML can provide valuable insights, its lack of interpretability in some models makes it challenging to justify for highly regulated tasks. Business leaders may be drawn to ML’s perceived sophistication, but in some cases, a rules-based approach may be more practical and reliable.\n\nThese examples, although not comprehensive, highlight the importance of carefully evaluating whether ML is truly the right tool for the job. ML can be ideal when there’s a need to learn from complex patterns in data, make predictions on new data, or adapt dynamically over time. However, using ML for problems that don’t meet these criteria or have additional concerns as mentioned above, can waste resources, reduce transparency, and increase project complexity. Recognizing when a simpler method will be more effective is a critical skill for data scientists and ML practitioners. Avoiding unnecessary ML solutions allows teams to focus their resources where they will add the most value, while helping business leaders make more informed, strategic technology decisions.\n\n\n2.2.3 Pitfalls of Using ML Unnecessarily\nUsing ML when it’s not needed can create more problems than it solves. ML is powerful but also complex, requiring specialized skills, infrastructure, and ongoing maintenance. When ML is applied unnecessarily, organizations risk:\n\nIncreased Complexity and Maintenance Costs: ML systems require regular monitoring, retraining, and validation to ensure they remain accurate and reliable. For example, if an ML model is deployed to handle a task that could be addressed with simple rules, the organization may end up dedicating time and resources to monitor and maintain an over-engineered solution that adds little value. Traditional software solutions often require far less maintenance and are more stable over time.\nReduced Interpretability and Transparency: ML models, especially complex ones like neural networks, can be difficult to interpret. In situations where transparency is crucial—such as compliance with regulations or gaining end-user trust—a rules-based approach might be preferable. Using ML without necessity can lead to resistance from stakeholders who need clear explanations for the system’s outputs.\nWaste of Time and Resources: Developing an ML system demands resources for data preparation, model selection, and deployment, as well as ongoing maintenance and infrastructure support. If the problem does not require ML, these resources could be better spent on other areas, such as enhancing customer experience or improving data quality.\nPotential for Errors and Misinterpretations: ML models are not infallible and can produce unexpected or incorrect results, especially when faced with data outside their training set. For example, if an ML model is applied to a problem that is straightforward or deterministic, it may introduce unnecessary variability or errors, which could lead to costly mistakes. For example, a recommendation system that misclassifies products might negatively impact user experience and decrease trust in the system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#defining-performance-metrics-for-the-ml-system",
    "href": "02-before-we-build.html#defining-performance-metrics-for-the-ml-system",
    "title": "2  Before We Build",
    "section": "2.3 Defining Performance Metrics for the ML System",
    "text": "2.3 Defining Performance Metrics for the ML System\nEstablishing comprehensive performance metrics is critical for evaluating how effectively an ML system solves the intended business problem. Defining these metrics early not only provides a target for model performance but also ensures alignment between technical teams and business stakeholders. Performance metrics serve as both a development benchmark and a standard for continuous monitoring in production, helping teams maintain an effective and valuable ML system over time.\nA robust approach to performance metrics includes:\n\nModel performance metrics which measure predictive accuracy and reliability\nSystem performance metrics which gauge how well the overall ML system operates in a real-world environment\nBusiness performance metrics which measure the impact the ML system provides to the business.\n\nBy incorporating all three, ML practitioners can ensure that the system is not only technically accurate but also capable of meeting the demands of production use and drives measurable improvements for the business. Let’s dive into each category in more detail.\n\n2.3.1 Technical Performance Metrics\nTechnical performance metrics are essential for evaluating both the model’s accuracy and the system’s operational efficiency. These metrics allow teams to optimize not only the predictive power of the model but also its usability within a production environment. By considering both model and system performance metrics, teams can ensure the ML system is not only accurate but also performant, reliable, and capable of meeting operational requirements in production.\n\nModel Performance Metrics: These metrics evaluate the accuracy and reliability of the ML model in making predictions. These metrics include common metrics you likely have heard of before if you’ve done any ML tasks such as mean squared error, \\(R^2\\), and mean absolute error for regression problems; cross-entropy, gini index, precision, and recall for classification problems; or BLEU, BERTScore, and perplexity for large language models. The choice of metric depends on the type of ML task (e.g., classification, regression) and the consequences of different kinds of errors.\nSystem Performance Metrics: While model accuracy is critical, it’s equally important to assess how well the overall ML system functions in real-world conditions. These metrics track the system’s efficiency, responsiveness, and reliability, ensuring that it can handle production demands. Metrics we often consider to measure system performance include:\n\nLatency: Measures the time required for the system to make a prediction once data is available. Low latency is critical in real-time applications like fraud detection and autonomous vehicles. Imagine a pedestrian steps out in front of an autonomous vehicle, there needs to be nearly zero time (or nearly no latency) between the car’s ML system taking in that input and making a prediction to stop otherwise the consequence will be catostrophic! 1\nThroughput: Indicates the number of predictions the system can process per unit of time. High throughput is essential for large-scale applications, such as recommendation systems used by Netflix and Spotify. For example, Netflix has around 280 million customers that they need to produce recommendation predictions for on a daily to weekly basis. 2\nUptime: Tracks the percentage of time the system is fully operational. Uptime is especially important for mission-critical systems, where outages can disrupt service and lead to lost revenue.\nScalability: Assesses the system’s ability to maintain performance as user demand or data volume increases, especially while needing to maintain real-time inference requirements. For example, imagine how user demand for Amazon surges on Black Friday! 3\nResource Utilization: Tracks CPU, memory, and GPU usage to ensure efficient resource allocation, particularly relevant for cloud-based ML systems. Efficient resource utilization can significantly reduce costs, as discussed in Ramesh et al., 2020.\n\n\nConsidering both model and system performance metrics enables teams to build ML systems that are not only accurate but also highly responsive, efficient, and reliable in a production environment.\n\n\n2.3.2 Business Performance Metrics\nWhile technical metrics are essential for evaluating the functionality of the ML system, business metrics ensure that the ML system drives meaningful impact for the organization. These metrics translate technical outputs into measurable business outcomes, aligning the ML system with the broader organizational goals. Some examples follow.\n\nRevenue Impact: For systems designed to drive sales or conversions, revenue-focused metrics quantify effectiveness. For example:\n\nIncremental Revenue: measures the additional revenue generated by the ML system compared to a baseline. This can demonstrate the impact of a recommendation system on customer purchases (McKinsey, 2018).\nConversion Rate: measures the percentage of users who complete a desired action after interacting with the ML system (e.g., making a purchase after receiving a recommendation).\n\nOperational Efficiency: For applications focused on cost reduction or improving efficiency, business will tend to use metrics that capture the system’s ability to streamline processes such as:\n\nCost Savings: measures the reduction in operational costs achieved by using the ML system, such as fewer maintenance visits with predictive maintenance models (Deloitte, 2020).\nProcess Time Reduction: measures the time saved in completing a process using the ML system, relevant in areas such as automated customer support ticket resolution (IBM, 2019).\n\nCustomer Engagement: For ML systems that serve customers directly, engagement metrics help gauge relevance and user experience. For example:\n\nClick-Through Rate (CTR): measures the percentage of users who engage with personalized content or recommendations. CTR is a common metric for digital content recommendations (Covington et al., 2016).\nUser Retention Rate: measures the percentage of users who continue to engage with the platform over time, providing a longer-term view of the system’s relevance.\n\nAccuracy in High-Stakes Decisions: For applications in areas like healthcare or finance, additional metrics measure the system’s ability to make accurate, high-impact decisions. For example:\n\nReduction in Error Rates for High-Risk Cases: measures the reduction in error rates (false-positives or false-negatives) in high-stakes predictions (e.g., fraud detection), which can demonstrate the value of the ML system in risk mitigation (Fawcett & Provost, 1997).\nCompliance and Risk Management Impact: measures the system’s role in meeting regulatory standards and reducing compliance risks, especially critical in regulated industries (EU GDPR, 2018).\n\n\nBy defining and monitoring both technical and business metrics, ML teams can ensure their system is effective from both a technical and strategic perspective, delivering real business value.\n\n\n2.3.3 Best Practices for Implementing Performance Metrics\nSuccessfully implementing performance metrics requires thoughtful integration into the ML system lifecycle. Here are some best practices commonly used to make metrics more effective. Later chapters will dive into the best practices focused on monitoring and setting thresholds and alerts.\n\nSet Benchmarks and Targets: Establish target values for each metric based on baseline performance, industry standards, and stakeholder expectations. Benchmarks help set realistic goals for model and system improvement (Sculley et al., 2015).\nUse Multiple Metrics for a Holistic View: Relying on a single metric can lead to misleading conclusions. Using a combination of metrics—such as precision and latency, or recall and business impact—provides a balanced understanding of performance (Saito & Rehmsmeier, 2015).\nMonitor Metrics Continuously: Performance metrics should be monitored throughout the model lifecycle, including in production. Continuous monitoring helps detect issues like data drift or system degradation (Breck et al., 2017).\nSet Thresholds and Alerts: Define acceptable thresholds for critical metrics and set up alerts if the system’s performance falls below these levels. This allows for quick intervention to resolve potential issues before they impact users or business outcomes.\nRegularly Re-Evaluate Metrics: As business needs and data sources evolve, so should the metrics. Periodic re-evaluation ensures that metrics remain relevant and continue to align with the organization’s priorities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#understanding-the-potential-value-of-a-solution",
    "href": "02-before-we-build.html#understanding-the-potential-value-of-a-solution",
    "title": "2  Before We Build",
    "section": "2.4 Understanding the Potential Value of a Solution",
    "text": "2.4 Understanding the Potential Value of a Solution\nBefore embarking on building an ML system, it is essential to evaluate the potential value that the solution will bring to the organization. Understanding this value helps justify the investment of time, resources, and budget, and ensures that the ML system aligns with the organization’s business objectives. Evaluating potential value involves asking the right questions, considering both tangible and intangible benefits, and aligning with stakeholder priorities to secure buy-in.\n\n2.4.1 Evaluating Impact on Business Objectives\nThe success of an ML system is ultimately measured by its impact on business objectives. By evaluating how the system will improve key metrics — whether increasing revenue, reducing costs, enhancing customer satisfaction, or improving operational efficiency — teams can prioritize the features and use cases that will generate the greatest return on investment. The goal is to clearly articulate how the ML solution will contribute to the broader goals of the business, whether it’s through improved decision-making, optimized processes, or creating better customer experiences.\n\n\n2.4.2 Questions to Estimate and Define the System’s Value\nTo estimate and define the value of an ML system, it’s useful to ask the following questions:\n\nHow Will the Solution Improve Business Outcomes? Consider the specific business outcomes that the ML system is intended to influence. For example, will a recommendation system increase sales by suggesting more relevant products to customers? And if so, what does the business reasonably expect those incremental sales to equate to? Or maybe the improved recommendation system will be able to make more time relevant predictions based on today’s trends or news, which could increase daily active users which could increase potential ad revenue. Or maybe the improved recommendation system is designed to reduce customer churn, which could lead to lower new customer acquisition costs. Answering these question helps determine just how the ML system is expected to benefit the organization and if the potential benefits justify the investment and ensures the solution is well-targeted.\nWhat Are the Tangible and Intangible Benefits? When evaluating the potential value, it’s important to look beyond the immediate financial benefits. Tangible benefits, such as cost savings, increased revenue, or reduced processing time, are easier to quantify and often form the basis for return on investment analyses. However, intangible benefits, like improved customer satisfaction, increased user engagement, enhanced employee productivity, or reduced risk, are equally important for long-term success but not as easy to quantify. For example, a fraud detection model may not only save money but also enhance customer trust—an intangible yet valuable outcome for the business. Although these are difficult to quantify, it is important to identify these potential benefits and determine if there is a way to quantify and track them.\n\n\n\n2.4.3 Aligning Value Assessment with Stakeholder Priorities\nTo secure buy-in, it’s critical to align the value assessment with stakeholder priorities. This involves understanding the goals and pain points of various stakeholders and clearly articulating how the ML solution addresses them. Stakeholders, such as business executives, team leads, and end users, each have different perspectives on what constitutes value. By tailoring the value assessment to address these perspectives, ML practitioners can better communicate the expected benefits and ensure everyone is aligned on the importance of the project.\nInvolving stakeholders in this value assessment also provides an opportunity to identify potential risks, set realistic expectations, and agree on success criteria from the outset. When stakeholders see how the solution directly supports their goals, they are more likely to support the project and allocate the necessary resources to ensure its success.\nUnderstanding and clearly articulating the potential value of the ML solution lays a strong foundation for the project. It ensures that the system is designed to address the right problem, prioritizes outcomes that matter most to the business, and secures the support needed for successful development and deployment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#understanding-technical-requirements-to-determine-feasibility",
    "href": "02-before-we-build.html#understanding-technical-requirements-to-determine-feasibility",
    "title": "2  Before We Build",
    "section": "2.5 Understanding Technical Requirements to Determine Feasibility",
    "text": "2.5 Understanding Technical Requirements to Determine Feasibility\nBefore building an ML system, it’s crucial to assess the technical requirements to determine the feasibility of the project. Defining these requirements upfront helps ensure that the solution is practical, achievable, and that the necessary resources are available. Technical requirements span several areas, including data, algorithms, infrastructure, deployment, and scalability. Understanding these requirements early allows teams to identify potential challenges and make informed decisions regarding the scope and direction of the project.\n\n\n\n\n\n\nIt’s important to note that you will not have complete information on the requirements that follow. However, getting as much understanding upfront can help drive key decisions to mitigate risks, and set the foundation for a successful project early in the process.\n\n\n\n\n2.5.1 Defining Key Technical Requirements\n\nData Requirements: Data is the foundation of any ML system, and understanding data requirements is vital for project success. This involves identifying data sources (e.g., internal databases, third-party APIs), assessing data quality (e.g., completeness, accuracy, consistency), and determining preprocessing needs, such as data cleaning, transformation, or augmentation. Ensuring that sufficient, high-quality data is available early in the project is crucial, as poor data quality can significantly impact model performance.\nAlgorithm Requirements: Choosing the appropriate algorithm involves considering the problem type, the complexity of potential solutions, and the interpretability needs of stakeholders. Some problems may require simple, interpretable models like linear regression, while others may benefit from more complex models, such as deep learning. The algorithm choice also impacts computation time, required data, and the ability to provide explanations for predictions—all of which influence feasibility.\nInfrastructure Requirements: ML systems often demand significant computational resources for training and inference. Understanding infrastructure requirements includes determining compute needs (e.g., CPUs, GPUs), selecting appropriate software tools (e.g., TensorFlow, PyTorch), and evaluating cloud vs. on-premises options for scalability and cost-effectiveness. For projects with heavy resource demands, cloud services can provide a flexible solution, while smaller projects may be better suited to local infrastructure.\nDeployment Requirements: Deployment considerations include the delivery method and the integration of the model within existing systems. This could involve setting up an API endpoint to deliver predictions, integrating the model into an application, or deploying the model on edge devices. A well-planned deployment pipeline is key to automating processes such as validation, testing, and deployment, ensuring that the model moves from development to production efficiently.\nScale Requirements: It’s important to understand the anticipated demand for the ML system and whether it will operate in a batch or real-time setting. For systems that require low latency (e.g., fraud detection), scalability must be a priority, ensuring the system can handle large numbers of requests without degrading performance. For batch processing tasks, the focus may be more on data throughput and the ability to handle large datasets effectively.\n\n\n\n2.5.2 Addressing Gaps in Resources or Expertise\nOnce the technical requirements have been defined, teams must assess whether they have the necessary resources and expertise to proceed. Gaps may exist in areas such as data availability, technical skills, or infrastructure capabilities. Addressing these gaps might involve acquiring additional data, seeking third-party services, or upskilling team members to meet project needs. Making feasibility adjustments—such as narrowing the project scope, choosing simpler algorithms, or relying on cloud infrastructure to meet compute demands—can also ensure that the project remains viable within existing constraints.\nEvaluating technical requirements in these areas ensures that the ML system is both feasible and aligned with available resources. By addressing these aspects early, teams can make informed decisions, mitigate risks, and set the foundation for a successful project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#recognizing-ml-system-development-as-an-iterative-process",
    "href": "02-before-we-build.html#recognizing-ml-system-development-as-an-iterative-process",
    "title": "2  Before We Build",
    "section": "2.6 Recognizing ML System Development as an Iterative Process",
    "text": "2.6 Recognizing ML System Development as an Iterative Process\nDeveloping an ML system is inherently an iterative process, involving continuous refinement, experimentation, and adaptation. Unlike traditional software development, where requirements and specifications are typically well-defined from the outset, ML systems evolve as they learn from data, integrate feedback, and adapt to changing business needs. Successful ML projects often require multiple rounds of model tuning, retraining, and adjustments to meet both technical and business objectives. Embracing this iterative nature helps ensure that the final solution is robust, scalable, and well-aligned with its intended purpose.\nAs an ML project progresses, many of the topics discussed in this chapter—including performance metrics, the valuation of the project, technical requirements, and even the business problem itself—are likely to evolve. For instance:\n\nMetrics to Assess: Initial metrics defined to assess model performance or system efficiency may need to change if they are found to be insufficient, misleading, or if stakeholder priorities shift. For example, a metric like accuracy may become less useful if a focus on precision and recall better reflects the business goal.\nValuation of the Project: The estimated value of the project may evolve as new opportunities or limitations arise during development. The intended business impact, such as increasing customer engagement, may need revaluation if customer behaviors change or if new data sources reveal additional insights.\nTechnical Requirements: Technical needs such as data quality, infrastructure, or deployment pipelines often need adjustments based on model experiments, results, and feasibility assessments. Iterative model training may reveal gaps in data that require new sources or different preprocessing techniques, or identify infrastructure constraints that necessitate changes to compute resources.\nBusiness Problem: As the project progresses, the understanding of the business problem may change. Engaging with stakeholders and observing early results may lead to a deeper understanding of what needs to be solved, prompting refinements to the problem’s scope.\n\nThis iterative process requires finding a balance between being flexible enough to adapt to new information while avoiding uncontrolled changes, known as scope creep. Scope creep can derail a project by causing a continuous expansion of objectives, leading to wasted resources, delays, and reduced effectiveness. To manage this, teams should establish clear, well-documented milestones and revisit them periodically to assess if changes are necessary and justified. Any adjustments should be based on insights gained during iterations, with stakeholders involved in the decision-making process to ensure alignment.\nRecognizing ML system development as an iterative process allows teams to embrace change when needed while maintaining focus on the ultimate goals. It fosters continuous improvement, ensuring that the final system is effective, meets business objectives, and is resilient to the dynamic nature of data and business environments. By embracing iteration thoughtfully and managing scope effectively, ML practitioners can deliver solutions that are both impactful and sustainable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#summary",
    "href": "02-before-we-build.html#summary",
    "title": "2  Before We Build",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nThis chapter has outlined the crucial planning steps required before diving into the development of a machine learning (ML) system. Effective ML projects start with a deep understanding of the business problem, achieved by collaborating closely with stakeholders to ensure alignment between business needs and technical efforts. It is vital to evaluate whether the problem truly requires an ML solution, as not all issues benefit from the complexity and resource demands of machine learning. By avoiding unnecessary ML implementation, teams can focus resources on areas that provide meaningful impact.\nWe also explored the importance of defining performance metrics to assess both technical and business outcomes, as well as evaluating the potential value of the solution to determine if it justifies the investment. Technical requirements, such as data availability, infrastructure, and deployment needs, must be understood early to ensure feasibility and identify gaps that could impede progress. Finally, we recognized that ML system development is inherently iterative. Changes to metrics, requirements, or even the business problem are likely as the project progresses, requiring a careful balance between flexibility and avoiding scope creep.\nThe chapters that follow will equip you with the knowledge to understand and apply these foundational concepts, identify key decisions when designing an ML system, and effectively implement the tools and best practices required to build successful ML solutions. By building on this strong foundation, you will be better prepared to create ML systems that are impactful, adaptable, and well-aligned with organizational goals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#exercise",
    "href": "02-before-we-build.html#exercise",
    "title": "2  Before We Build",
    "section": "2.8 Exercise",
    "text": "2.8 Exercise\nConsider a scenario where a healthcare organization wants to build an ML system to improve patient outcomes by predicting hospital readmissions. To help guide this example, read the first three pages (sections 1-1.4) of this real-life case study of Mount Sinai Hospital in New York City. Use this example to help guide your answers to the following:\n\nStakeholder Engagement\n\nWho would the key stakeholders be for this project?\nWhat questions would you ask these stakeholders to ensure you understand the business problem?\nWhat assumptions might be uncovered during these discussions?\n\nEvaluating ML Suitability\n\nAssess whether ML is a suitable solution. What factors would you consider to determine if ML is appropriate for this problem?\nProvide an example of an alternative, non-ML approach that could be considered. What are the limitations of this approach compared to an ML approach?\n\nDefine Performance Metrics\n\nDefine three performance metrics for the ML system. Include at least one technical metric (e.g., accuracy), one system performance metric (e.g., latency), and one business metric (e.g., reduction in readmission rates).\nExplain why each of these metrics is important for evaluating the success of the ML system.\n\nUnderstanding Value and Feasibility\n\nWrite a paragraph that outlines the potential value of the ML system to the healthcare organization. Consider both tangible (e.g., cost savings) and intangible (e.g., improved patient satisfaction) benefits.\nList some key technical requirements that would be helpful to understand early on before developing the solution (e.g., data, infrastructure). What gaps might exist, and how would you address them?\n\nThe Iterative Process\n\nDescribe why the development of this ML system would be an iterative process. Provide an example of something that could change during development (e.g., a performance metric, a technical requirement) and how you would manage this change to avoid scope creep.\n\n\n\n\n\n\nMattson, Peter, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, et al. 2020. “Mlperf Training Benchmark.” Proceedings of Machine Learning and Systems 2: 336–49.\n\n\nTolomei, Gabriele, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. 2017. “Interpretable Predictions of Tree-Based Ensembles via Actionable Feature Tweaking.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 465–74.\n\n\nVemulapalli, Gopichand. 2023. “Operationalizing Machine Learning Best Practices for Scalable Production Deployments.” International Machine Learning Journal and Computer Engineering 6 (6): 1–21.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#footnotes",
    "href": "02-before-we-build.html#footnotes",
    "title": "2  Before We Build",
    "section": "",
    "text": "Latency is well-covered in Tolomei et al. (2017), which discusses latency optimization for ML infrastructure.↩︎\nMattson et al. (2020) explores techniques for optimizing throughput in production ML systems.↩︎\nVemulapalli (2023) discusses some machine learning best practices for scalable production deployments.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html",
    "href": "03-dataops-role.html",
    "title": "3  The Role of DataOps",
    "section": "",
    "text": "3.1 Introduction to DataOps",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#introduction-to-dataops",
    "href": "03-dataops-role.html#introduction-to-dataops",
    "title": "3  The Role of DataOps",
    "section": "",
    "text": "Definition of DataOps and how it fits into the MLOps lifecycle.\nThe goals of DataOps: efficient, reliable, and high-quality data management.\nHow DataOps enables scalable and reliable ML systems.\nCore components of DataOps",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-ingestion",
    "href": "03-dataops-role.html#data-ingestion",
    "title": "3  The Role of DataOps",
    "section": "3.2 Data Ingestion",
    "text": "3.2 Data Ingestion\n\nTechniques for gathering data from multiple sources (e.g., databases, APIs, data lakes).\nOverview of batch vs. streaming data ingestion.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-processing",
    "href": "03-dataops-role.html#data-processing",
    "title": "3  The Role of DataOps",
    "section": "3.3 Data Processing",
    "text": "3.3 Data Processing\n\nData transformation, feature engineering, and cleaning.\nStructuring data for model consumption.\nTools and methods for handling large-scale data processing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-validation",
    "href": "03-dataops-role.html#data-validation",
    "title": "3  The Role of DataOps",
    "section": "3.4 Data Validation",
    "text": "3.4 Data Validation\n\nEnsuring data quality through checks and validation procedures.\nExamples of common data validation practices and pitfalls.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-versioning-and-lineage",
    "href": "03-dataops-role.html#data-versioning-and-lineage",
    "title": "3  The Role of DataOps",
    "section": "3.5 Data Versioning and Lineage",
    "text": "3.5 Data Versioning and Lineage\n\nTracking data versions to ensure reproducibility.\nUnderstanding data lineage for transparency and compliance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#summary",
    "href": "03-dataops-role.html#summary",
    "title": "3  The Role of DataOps",
    "section": "3.6 Summary",
    "text": "3.6 Summary\n\nTBD",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html",
    "href": "04-dataops-build.html",
    "title": "4  Building and Managing Data Pipelines",
    "section": "",
    "text": "4.1 Introduction to Data Pipelines",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#introduction-to-data-pipelines",
    "href": "04-dataops-build.html#introduction-to-data-pipelines",
    "title": "4  Building and Managing Data Pipelines",
    "section": "",
    "text": "Definition of data pipelines and their importance in ML system design.\nThe role of DataOps in constructing scalable and reliable data workflows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#tools-for-dataops",
    "href": "04-dataops-build.html#tools-for-dataops",
    "title": "4  Building and Managing Data Pipelines",
    "section": "4.2 Tools for DataOps",
    "text": "4.2 Tools for DataOps\nOverview (and examples) of commonly used tools for DataOps:\n\nData Ingestion: Apache Kafka, Apache NiFi, Airbyte.\nData Processing: Apache Spark, DBT, Pandas.\nData Validation: Great Expectations, TFDV (TensorFlow Data Validation).\nData Versioning: DVC, Delta Lake.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#building-data-pipelines",
    "href": "04-dataops-build.html#building-data-pipelines",
    "title": "4  Building and Managing Data Pipelines",
    "section": "4.3 Building Data Pipelines",
    "text": "4.3 Building Data Pipelines\nStep-by-step guide to creating the components of a DataOps system. This part will show system architecture diagrams to conceptually build a DataOps system\n\nDesigning an ingestion framework that accommodates multiple data sources.\nData cleaning, transformation, and feature extraction.\nIncorporating data validation to ensure quality.\nVersioning datasets for consistency and reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#creating-scalable-data-pipelines",
    "href": "04-dataops-build.html#creating-scalable-data-pipelines",
    "title": "4  Building and Managing Data Pipelines",
    "section": "4.4 Creating Scalable Data Pipelines",
    "text": "4.4 Creating Scalable Data Pipelines\n\nHow to build data pipelines that scale to accommodate increasing data volumes.\nKey considerations for scalability, such as distributed processing and fault tolerance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#hands-on-example-simple-data-pipeline-for-ml-workflows",
    "href": "04-dataops-build.html#hands-on-example-simple-data-pipeline-for-ml-workflows",
    "title": "4  Building and Managing Data Pipelines",
    "section": "4.5 Hands-On Example: Simple Data Pipeline for ML Workflows",
    "text": "4.5 Hands-On Example: Simple Data Pipeline for ML Workflows\n\nDemonstrate a simple end-to-end data pipeline that ingests, processes, validates, and versions a dataset.\nWalkthrough of using tools like Pandas, Great Expectations, and DVC to create a pipeline.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#conclusion-and-key-takeaways",
    "href": "04-dataops-build.html#conclusion-and-key-takeaways",
    "title": "4  Building and Managing Data Pipelines",
    "section": "4.6 Conclusion and Key Takeaways",
    "text": "4.6 Conclusion and Key Takeaways\n\nSummary of the key principles of DataOps and how to apply them in building effective data pipelines for ML.\nReflection on the importance of data quality, management, and scalability in ML workflows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Building and Managing Data Pipelines</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald\nGall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas\nZimmermann. 2019. “Software Engineering for Machine Learning: A\nCase Study.” In 2019 IEEE/ACM 41st International Conference\non Software Engineering: Software Engineering in Practice\n(ICSE-SEIP), 291–300. IEEE.\n\n\nGama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and\nAbdelhamid Bouchachia. 2014. “A Survey on Concept Drift\nAdaptation.” ACM Computing Surveys (CSUR) 46 (4): 1–37.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. \"\nO’Reilly Media, Inc.\".\n\n\nLevine, Sergey, Aviral Kumar, George Tucker, and Justin Fu. 2020.\n“Offline Reinforcement Learning: Tutorial, Review, and\nPerspectives on Open Problems.” arXiv Preprint\narXiv:2005.01643.\n\n\nMattson, Peter, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius\nMicikevicius, David Patterson, Hanlin Tang, et al. 2020. “Mlperf\nTraining Benchmark.” Proceedings of Machine Learning and\nSystems 2: 336–49.\n\n\nPapernot, Nicolas, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z\nBerkay Celik, and Ananthram Swami. 2017. “Practical Black-Box\nAttacks Against Machine Learning.” In Proceedings of the 2017\nACM on Asia Conference on Computer and Communications Security,\n506–19.\n\n\nPolyzotis, Neoklis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven\nWhang. 2019. “Data Validation for Machine Learning.”\nProceedings of Machine Learning and Systems 1: 334–47.\n\n\nSculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd\nPhillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois\nCrespo, and Dan Dennison. 2015. “Hidden Technical Debt in Machine\nLearning Systems.” Advances in Neural Information Processing\nSystems 28.\n\n\nTolomei, Gabriele, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas.\n2017. “Interpretable Predictions of Tree-Based Ensembles via\nActionable Feature Tweaking.” In Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data\nMining, 465–74.\n\n\nVemulapalli, Gopichand. 2023. “Operationalizing Machine Learning\nBest Practices for Scalable Production Deployments.”\nInternational Machine Learning Journal and Computer Engineering\n6 (6): 1–21.",
    "crumbs": [
      "References"
    ]
  }
]