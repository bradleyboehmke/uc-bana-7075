[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Design for Business",
    "section": "",
    "text": "Welcome\nWelcome to Machine Learning Design for Business! This is the the online textbook that compliments the UC BANA 7085 course. This course aims to provide a framework for developing real-world machine learning systems that are deployable, reliable, and scalable. You will be introduced to MLOps as the intersection of DataOps, ModelOps, and DevOps; combined, these concepts provide scalable, reliable, and repeatable machine learning workflows. You will learn how MLOps enables organizations to overcome challenges in operationalizing machine learning models. Along the way, you will learn about important organizational issues including privacy, fairness, security, stakeholder management, project collaboration, and more!\nThroughout this textbook you’ll find embedded lectures, hands-on exercises, and additional resources that will allow you to explore the tools, techniques, and best practices required to successfully design and deploy machine learning systems in today’s organizations.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-should-read-this",
    "href": "index.html#who-should-read-this",
    "title": "Machine Learning Design for Business",
    "section": "Who should read this",
    "text": "Who should read this\nThis book is ideal for graduate students, data scientists, engineers, and analytics professionals who want to bridge the gap between building machine learning models and deploying them into reliable, scalable production environments. Those looking to deepen their understanding of operationalizing ML models will benefit from hands-on experience with MLOps workflows, including data management, model deployment, and monitoring. Individuals with a basic background in machine learning or software development will gain practical skills to streamline the machine learning lifecycle and align their work with organizational and business goals.\nThis book is also valuable for those interested in cross-functional collaboration, providing insights into how data science teams can work alongside engineering and business units to achieve successful machine learning outcomes.\nThis book is technical in nature, designed to provide hands-on experience in deploying, monitoring, and managing machine learning systems. While it’s possible to successfully complete the book with minimal coding experience, having some familiarity with Python and basic machine learning concepts will be beneficial for understanding and applying the material. The course introduces technical tools and workflows, but it’s structured to guide students of various backgrounds through each step. Those with prior experience in Python or ML will have a smoother time grasping the concepts and may find it easier to implement projects independently. However, foundational knowledge in programming or machine learning is not strictly required, as core principles and step-by-step instructions are provided to support all readers in building effective MLOps pipelines.\n\n\n\n\n\n\nIf you are new to Python, I recommend you check out https://bradleyboehmke.github.io/uc-bana-6043 to get up to speed with the basics.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "Machine Learning Design for Business",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\nThe following typographical conventions are used in this book:\n\nstrong italic: indicates new terms,\nbold: indicates package & file names,\ninline code: monospaced highlighted text indicates functions or other commands that could be typed literally by the user,\ncode chunk: indicates commands or other text that could be typed literally by the user\n\n\n1 + 2\n\n3\n\n\nIn addition to the general text used throughout, you will notice the following code chunks with images:\n\n\n\n\n\n\nSignifies a tip or suggestion\n\n\n\n\n\n\n\n\n\nSignifies a general note\n\n\n\n\n\n\n\n\n\nSignifies a warning or caution",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#software-used-throughout-this-book",
    "href": "index.html#software-used-throughout-this-book",
    "title": "Machine Learning Design for Business",
    "section": "Software used throughout this book",
    "text": "Software used throughout this book\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Machine Learning Design for Business",
    "section": "Additional resources",
    "text": "Additional resources\nTBD",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html",
    "href": "01-intro-ml-system.html",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "",
    "text": "1.1 ML System Design\nMachine Learning (ML) System Design is the discipline of creating reliable, scalable, and maintainable machine learning systems that solve real-world business problems. Unlike standard machine learning modeling, which focuses primarily on algorithm development and evaluation, ML System Design considers the broader ecosystem in which a model must operate. This involves defining the architecture, infrastructure, and processes that make a model functional and sustainable within production environments. As organizations increasingly rely on machine learning to make data-driven decisions, robust system design has become essential for delivering models that consistently meet organizational needs and can adapt to change.\nEffective ML system design enables organizations to avoid many pitfalls of deploying machine learning in production. Poorly designed systems can lead to issues like inaccurate predictions due to data drift, high operational costs, or significant downtime during retraining or model updates. By adhering to design principles such as modularity, scalability, and reproducibility, ML engineers and data scientists can develop systems that are more resilient and easier to manage. These principles form the backbone of good ML system design and help ensure that models can not only be deployed but also monitored, improved, and scaled as needed.\nThis section will explore these design principles in detail and examine how they align with key stages of the ML system lifecycle, from data processing and model training to deployment, monitoring, and iteration. As Chip Huyen points out, “Machine learning in production is 10% modeling and 90% engineering” (Huyen 2022), highlighting how technical choices impact the system’s reliability and long-term value for the organization. Understanding ML System Design and the lifecycle of ML systems is foundational for anyone working in machine learning, as it highlights how technical choices impact the system’s overall reliability and long-term value for the organization.\nML system design refers to the process of building, structuring, and integrating machine learning models into larger production environments to deliver reliable, scalable, and sustainable solutions. While traditional machine learning focuses on developing high-performing models, system design extends the focus to creating an entire ecosystem where these models can continuously operate, adapt, and provide value over time. It encompasses everything from data pipelines and infrastructure to model versioning, monitoring, and scaling, ensuring that machine learning projects align with organizational goals and can be maintained as data, requirements, and technology evolve.\nTherefore, ML system design includes multiple facets beyond the algorithms themselves:\nflowchart LR\n  subgraph ML[ML System]\n    direction TB\n    subgraph DataOps\n    end\n    subgraph ModelOps\n    end\n    subgraph DevOps\n    end\n  end\n  A[Stakeholders] --&gt; ML\n  B[Business Requirements] --&gt; ML\n  ML --&gt; C[End Users]\n  DataOps --&gt; ModelOps --&gt; DevOps\n\n\n\n\nFigure 1.1: Components of an ML System.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#ml-system-design",
    "href": "01-intro-ml-system.html#ml-system-design",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "",
    "text": "Complexities of an ML System\n\n\n\n\n\nML systems are very complex and require various engineering tasks such as:\n\nData Engineering: data acquisition & data preparation,\nML Model Engineering: ML model training & serving, and\nCode Engineering: integrating ML model into the final product.\n\nConsequently, ML System Design and ML Engineering are often used synonymously.\n\n\n\nThe Complexities of an ML System (https://ml-ops.org)\n\n\n\n\n\n\n\n\nDataOps: Data Management and Pipelines\nAt the heart of any ML system is data, which is processed, transformed, and delivered to the model in a way that supports both training and inference. A well-designed ML system typically includes data pipelines that handle:\n\nData ingestion: Collecting data from various sources, including databases, APIs, and real-time streams.\nData processing: Cleaning, transforming, and structuring data for model consumption.\nData validation: Ensuring that data quality is maintained and meets the standards required by the model.\nData versioning and lineage: Tracking data versions and maintaining a record of data transformations, which are critical for reproducibility and regulatory compliance.\n\n\n\nModelOps: Model Lifecycle Management\nA central aspect of ML system design is managing the entire model lifecycle, from development and testing to deployment and retirement. This includes:\n\nExperiment tracking and versioning: Maintaining a record of model versions, hyperparameters, and evaluation metrics to track performance and support reproducibility.\nDeployment pipelines: Automating model deployment processes to reduce manual errors and accelerate time-to-production.\nMonitoring and alerting: Establishing mechanisms for monitoring model performance in production and detecting issues such as data drift, concept drift, or system degradation.\nContinuous improvement: Supporting processes for regular retraining, fine-tuning, and updating models as data and requirements change.\n\n\n\nDevOps Practices\nGood ML system design brings DevOps practices to the world of machine learning. DevOps, short for “Development and Operations,” is a set of practices and principles that combines software development (Dev) and IT operations (Ops) with the goal of shortening the software development lifecycle and delivering high-quality, reliable software more efficiently. At its core, DevOps is about fostering a culture of collaboration between development and operations teams, integrating automated processes, and leveraging continuous integration and continuous deployment (CI/CD) practices to streamline workflows. This includes:\n\nAutomating code and model updates to streamline deployment and minimize downtime.\nAutomation of testing and validation: Implementing automated tests to validate model performance, accuracy, and reliability.\nCompute resources: Setting up scalable, flexible compute resources (such as cloud instances, GPUs, or Kubernetes clusters) to handle model training and inference.\nStorage solutions: Designing data storage strategies that balance cost, speed, and accessibility.\nNetworking and security: Ensuring secure data transfer and protecting sensitive information while allowing fast access for machine learning processes.\nCollaboration between teams: Encouraging collaboration across data science, engineering, and business teams to ensure that models meet organizational requirements and add real value.\n\n\n\n\n\n\n\nDriven by business requirements\n\n\n\nThe design of each key element in an ML system is ultimately driven by business requirements, as they define the purpose and value that the system should deliver. Business goals shape how data is processed, what metrics the model optimizes, and the reliability, scalability, and security standards it must meet. For example, a financial institution deploying a fraud detection model may prioritize stringent data validation and versioning to comply with regulatory standards, while a retail organization focused on real-time recommendations may emphasize rapid data ingestion and low-latency deployment.\nModel lifecycle management is similarly guided by business needs; frequent retraining cycles, experiment tracking, and monitoring capabilities are crucial when a system must adapt to fast-changing consumer behavior or market trends. Infrastructure choices, such as selecting between cloud and on-premise solutions, also reflect organizational constraints, like budget or resource availability.\nBy aligning these elements with clear business objectives, organizations can create ML systems that are not only technically sound but also effective in achieving their strategic goals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#why-ml-system-design-matters",
    "href": "01-intro-ml-system.html#why-ml-system-design-matters",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.2 Why ML System Design Matters",
    "text": "1.2 Why ML System Design Matters\nML system design addresses the challenges of productionizing machine learning. In a laboratory or research setting, machine learning models are often developed with a focus on achieving high accuracy or minimizing error on a given dataset. However, deploying these models in production involves a different set of concerns. In real-world applications, ML systems need to handle fluctuating data distributions, evolving user needs, and high availability. Without a robust design, machine learning models can quickly degrade in performance, lead to incorrect predictions, or even disrupt business operations.\nBy prioritizing system design, organizations can build ML workflows that are both responsive to change and maintainable over time. This approach reduces technical debt—the accumulation of outdated or poorly designed code, data dependencies, and complex interactions that are difficult to fix — highlighted by the authors in Hidden Technical Debt in Machine Learning Systems as a critical challenge in ML applications (Sculley et al. 2015). Poorly managed dependencies and hidden feedback loops, for instance, can lead to unpredictable model behavior and complicate future updates. Thoughtful ML system design mitigates these issues, allowing models to deliver consistent, reliable results while aligning predictions more closely with organizational metrics and objectives. Ultimately, well-architected ML systems are more adaptable, enabling models to evolve as business goals and data distributions shift over time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#design-principles-for-good-ml-system-design",
    "href": "01-intro-ml-system.html#design-principles-for-good-ml-system-design",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.3 Design Principles for Good ML System Design",
    "text": "1.3 Design Principles for Good ML System Design\nCreating an effective ML system involves more than just building an accurate model; it requires thoughtful design to ensure that the system remains reliable, scalable, and adaptable over time. Good ML system design incorporates a set of principles that help address real-world challenges in production environments, such as evolving data, model drift, and changing business requirements. The following design principles guide ML engineers and data scientists in building systems that meet these demands and deliver sustained business value.\n\n\n\n\n\n\nclassDiagram\n    class Principles {\n      Modularity and Abstraction\n      Scalability\n      Reproducibility\n      Automation\n      Monitoring and Maintenance\n      Security and Compliance\n      Adaptability and Flexibility\n    }\n\n\n\n\nFigure 1.2: Principles that create reliable, scalable, and adaptable machine learning systems.\n\n\n\n\n\n\nModularity and Abstraction\nModularity is the practice of dividing a system into independent, self-contained components, each responsible for a specific function. In an ML system, this could mean separating data ingestion, preprocessing, training, and monitoring into distinct modules that can operate independently (Sculley et al. 2015). This modularity simplifies updates and maintenance, as each component can be modified or scaled without disrupting the entire system. For example, if a new feature engineering technique improves model accuracy, it can be implemented in the data preprocessing module without affecting the deployment pipeline.\nAbstraction, on the other hand, hides complex details within each module, making the system easier to manage and more accessible to team members who don’t need to understand every technical detail. As noted by Huyen (2022), abstraction helps bridge the gap between data scientists, engineers, and business stakeholders, allowing each to focus on high-level functionality without needing an in-depth understanding of every process. Together, modularity and abstraction improve collaboration, flexibility, and ease of scaling within ML systems.\n\n\nScalability\nScalability ensures that an ML system can handle increased demand—whether in terms of data volume, number of users, or computational load—without requiring extensive re-engineering. Designing for scalability involves choosing infrastructure and tools that allow for efficient resource allocation and growth. For instance, deploying a model on cloud infrastructure, where compute resources can be dynamically allocated, allows an ML system to scale up to handle peak loads and scale down during quieter periods, optimizing costs and performance (Levine et al. 2020).\nAnother aspect of scalability is planning for future expansion. An ML system might start with a few users or limited data, but as it proves valuable, the system may need to accommodate more data sources, larger user bases, or additional model versions. Scalable design considers these future needs from the outset, making it easier to extend the system’s capabilities without significant architectural changes.\n\n\nReproducibility\nReproducibility is critical for ML systems, particularly in production settings where consistent results and traceability are essential. Reproducibility ensures that anyone with access to the same code, data, and configuration can recreate a model with the same results. This is essential for troubleshooting, compliance, and validating model performance. As described by Amershi et al. (2019), reproducibility allows teams to understand how specific predictions are made and to resolve any issues that arise in production more efficiently.\nAchieving reproducibility requires careful versioning of data, models, and code. Version control tools like Git and DVC (Data Version Control) help keep track of changes to code and data over time, while tools like MLflow or Weights & Biases allow for experiment tracking and versioning of model parameters. Reproducibility is especially important in regulated industries, where organizations may need to demonstrate how a model made a particular prediction or prove compliance with specific standards.\n\n\nAutomation\nAutomation is a foundational principle in ML system design, as it reduces manual intervention, minimizes human error, and accelerates workflows. As described by Polyzotis et al. (2019), automation is particularly valuable in environments where frequent model retraining, testing, or deployment is necessary. For instance, an automated pipeline can retrain a model when new data is available, perform validation checks, and deploy the updated model to production, all without requiring human involvement.\nKey areas for automation in ML systems include data ingestion and preprocessing, model training and evaluation, deployment, and monitoring. Automating these stages not only saves time but also ensures that the system operates consistently and can adapt to changing data and conditions. This is crucial in fast-paced environments, such as e-commerce or finance, where systems must quickly respond to new information.\n\n\nMonitoring and Maintenance\nMonitoring and maintenance are essential for keeping an ML system aligned with its intended goals over time. Once a model is deployed, its performance can be affected by data drift (changes in data distribution) or concept drift (changes in the underlying relationships within the data) (Gama et al. 2014). Without monitoring, these issues can lead to model degradation, resulting in inaccurate predictions or decisions that don’t meet business objectives.\nGood ML system design includes monitoring dashboards and automated alerts that track metrics such as prediction accuracy, latency, data drift, and system errors. Tools like Prometheus and Grafana can help set up performance monitoring, while specialized ML monitoring solutions can track model-specific metrics. Regular maintenance practices, such as retraining or recalibrating models, are also essential for sustaining system performance and adapting to new data patterns or business requirements.\n\n\nSecurity and Compliance\nSecurity and compliance are often overlooked in ML system design but are increasingly important, especially in regulated industries like healthcare, finance, and government. An ML system that processes sensitive data must include security measures to protect data integrity and confidentiality. Security considerations include encrypting data in transit and at rest, managing user access controls, and safeguarding against adversarial attacks (Papernot et al. 2017).\nCompliance involves ensuring that the ML system adheres to industry regulations and organizational policies. For instance, GDPR compliance may require an ML system to provide transparency into how predictions are made and to allow users to request data deletions. Designing with security and compliance in mind not only protects the organization but also builds trust with users and stakeholders.\n\n\n\n\n\n\nWhat’s GDPR?\n\n\n\n\n\nThe General Data Protection Regulation (GDPR) is a comprehensive privacy law enacted by the European Union in 2018, aimed at protecting the personal data and privacy of individuals within the EU and the European Economic Area (EEA). GDPR sets strict guidelines on how organizations must collect, store, process, and share personal data, giving individuals more control over their information. Key principles of GDPR include data minimization, purpose limitation, and transparency, along with rights for individuals such as the right to access, correct, and delete their data. Non-compliance with GDPR can lead to significant fines, making it essential for companies, especially those dealing with machine learning systems, to prioritize data security and privacy to ensure compliance.\n\n\n\n\n\nAdaptability and Flexibility\nAdaptability is the ability of an ML system to evolve as data, business requirements, and technologies change. In a dynamic business environment, the needs of an ML system may shift over time, requiring the model to be updated, new features to be added, or the infrastructure to be reconfigured (Sculley et al. 2015). Designing for adaptability involves using flexible frameworks, modular components, and tools that support rapid changes without extensive rework.\nFor example, a well-designed ML system might use containerization (e.g., Docker) to allow models to be easily moved across environments or use APIs to integrate new data sources without significant restructuring. This flexibility enables teams to quickly implement changes, test new ideas, and stay aligned with business goals, making the system more resilient to future changes.\n\n\nPutting It All Together\nThese principles — modularity, scalability, reproducibility, automation, monitoring and maintenance, security, and adaptability — form the foundation of good ML system design. Each principle contributes to creating a system that is not only technically robust but also capable of delivering sustained business value. By applying these principles, ML engineers and data scientists can build systems that are flexible, resilient, and scalable, ensuring that machine learning models remain effective and reliable over time. Good ML system design allows organizations to move beyond individual models and create a stable, agile infrastructure that continuously adapts to meet evolving business demands.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#relation-to-ml-system-lifecycle-stages",
    "href": "01-intro-ml-system.html#relation-to-ml-system-lifecycle-stages",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.4 Relation to ML system lifecycle stages",
    "text": "1.4 Relation to ML system lifecycle stages\nGood ML system design requires a holistic approach that considers how key elements and design principles intersect with each stage of the ML system lifecycle. The ML lifecycle can be broken down into distinct stages: data processing, model development, deployment, monitoring, and continuous improvement. By aligning design principles with these stages, organizations can create systems that are not only effective at deployment but sustainable, adaptable, and closely aligned with business objectives over the long term.\n\n\n\n\n\n\nsequenceDiagram\n    participant dp as Data Ingestion & Processing\n    participant md as Model Development\n    participant dep as Deployment\n\n    Note over dp,dep: Modularity and Abstraction\n    Note over dp,dep: Scalability\n    Note over dp,dep: Reproducibility\n    Note over dp,dep: Automation\n    Note over dp,dep: Monitoring and Maintenance\n    Note over dp,dep: Security and Compliance\n    Note over dp,dep: Adaptability and Flexibility\n\n\n\n\nFigure 1.3: ML design principles should be considered across all stages of the ML lifecycle. By doing so, organizations can create systems that are not only effective at deployment but sustainable, adaptable, and closely aligned with business objectives over the long term.\n\n\n\n\n\n\nData Processing\nThe data processing stage encompasses everything from data ingestion and transformation to validation and storage. Principles like modularity and automation are especially valuable here, as they allow data pipelines to handle diverse data sources and adapt to changes without disrupting downstream processes. DataOps practices ensure the accuracy and integrity of data, including validation and lineage tracking, so the right data reaches the model reliably. Reproducibility also plays a crucial role at this stage, as versioned datasets and pipelines help maintain consistency, which is vital for model retraining and compliance.\n\n\nModel Development\nModel development involves experimentation, model training, and evaluation. Here, modularity and reproducibility are essential for managing experiments and enabling easy comparison of results across different model versions. ModelOps practices, including experiment tracking and model versioning, facilitate tracking hyperparameters, code versions, and results, ensuring models can be accurately reproduced and refined. Scalability is another key consideration, especially for large datasets or complex models that require significant computational resources. In this stage, infrastructure choices, such as cloud or GPU-based solutions, allow the system to scale up as needed.\n\n\nDeployment\nThe deployment stage focuses on moving models from development to production in a seamless, automated, and reliable manner. Automation is paramount, with CI/CD pipelines automating the testing, validation, and deployment processes to minimize human error and expedite time-to-production. DevOps principles enable automated deployment processes that integrate testing and validation steps, reducing downtime and increasing reliability. Security also becomes critical, ensuring that data access and model endpoints are protected against unauthorized access or adversarial attacks.\n\n\nMonitoring and Maintenance\nOnce a model is live, continuous monitoring is essential to track key metrics such as accuracy, latency, and model drift, ensuring that the model performs as expected over time. Monitoring and maintenance principles are implemented through tools and dashboards that monitor data drift and system performance. Alerts can notify teams of anomalies or degradation, enabling quick action to retrain or update the model if needed. Automation also supports regular retraining workflows, particularly in dynamic environments where data patterns shift frequently. Maintenance ensures that the ML system remains robust and responsive to changing data and requirements.\n\n\nContinuous Improvement\nThe continuous improvement stage focuses on enhancing the model based on feedback from monitoring and evolving business needs. Adaptability and flexibility are essential here, allowing the system to integrate new data, retrain models, and test enhancements with minimal friction. Scalability and modularity enable models to grow and improve without requiring complete redesigns of the system. In practice, this means designing systems with modular components that can easily integrate new models or data sources, ensuring that the system remains aligned with both technical advancements and organizational goals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#summary",
    "href": "01-intro-ml-system.html#summary",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.5 Summary",
    "text": "1.5 Summary\nIn this chapter, we introduced the concept of Machine Learning (ML) System Design, focusing on how it creates a foundation for building reliable, scalable, and adaptable machine learning systems that can deliver value in real-world production environments. Unlike traditional model development, ML system design considers the entire ecosystem, encompassing data pipelines, model lifecycle management, and operational infrastructure, all shaped by business requirements. By following core design principles — such as modularity, scalability, reproducibility, automation, monitoring, and security — ML systems can avoid common production pitfalls like model drift, high operational costs, and downtime, ensuring that models are not only deployed successfully but also perform reliably over time.\nThe chapters that follow will expand on each element of ML system design, providing readers with a deep understanding of the key concepts and principles that enable the creation of robust ML systems. Readers will gain the ability to identify essential design decisions that align with business goals, as well as the tools and practices necessary to implement these considerations effectively. By the end of this book, readers will be equipped to build, manage, and maintain ML systems that are both technically sound and responsive to evolving business needs.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\n\n\nMotivation for MLOps\nHidden Technical Debt in Machine Learning Systems",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "01-intro-ml-system.html#exercise",
    "href": "01-intro-ml-system.html#exercise",
    "title": "1  Introduction to Machine Learning System Design",
    "section": "1.6 Exercise",
    "text": "1.6 Exercise\nRead one or more of the following case studies. These case studies cover different perspectives, elements, and principles of ML system design.\n\nWhat are the primary business objective(s) for the ML system discussed? How does the system add value for both the organization and its customers?\nAnalyze key elements and design principles discussed. For example:\n\nDataOps: How does the organization handle data ingestion, processing, and versioning to support a constantly changing catalog and user preferences?\nModelOps: What strategies does the organization use to manage the lifecycle of its models, including retraining, versioning, and experimentation?\nDevOps: How does the organization ensure continuous integration and deployment for its models? What automation, testing, and monitoring practices are used to maintain system reliability?\nDesign Principles: Identify examples of modularity, scalability, automation, and monitoring within the organization’s ML system. Explain how each principle contributes to the system’s success.\n\n\n\n\n\n\n\n\nNote that these articles will likely mention technical concepts and tools that you may not be familiar with and feel like they are over your head. Do not worry about this! This book will help to bridge some of these gaps.\n\n\n\n\n\n\n\n\n\nCase Studies\n\n\n\n\n\n\nNetflix: Netflix’s Recommendation System: Algorithms, Business Value, and Innovation\nUber: Scaling Machine Learning at Uber with Michelangelo\nSpotify: Inside Spotify’s Recommender System: A Complete Guide to Spotify Recommendation Algorithms\nAirbnb: Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform\nGoogle: TFX - an end-to-end machine learning platform for TensorFlow\nLinkedIn: Scaling Machine Learning Productivity at LinkedIn\nFacebook: Introducing FBLearner Flow: Facebook’s AI backbone\nBooking.com: 150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com\nTwitter: Twitter’s Recommendation Algorithm\n\n\n\n\n\n\n\n\nAmershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas Zimmermann. 2019. “Software Engineering for Machine Learning: A Case Study.” In 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 291–300. IEEE.\n\n\nGama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. 2014. “A Survey on Concept Drift Adaptation.” ACM Computing Surveys (CSUR) 46 (4): 1–37.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. \" O’Reilly Media, Inc.\".\n\n\nLevine, Sergey, Aviral Kumar, George Tucker, and Justin Fu. 2020. “Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems.” arXiv Preprint arXiv:2005.01643.\n\n\nPapernot, Nicolas, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. “Practical Black-Box Attacks Against Machine Learning.” In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, 506–19.\n\n\nPolyzotis, Neoklis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven Whang. 2019. “Data Validation for Machine Learning.” Proceedings of Machine Learning and Systems 1: 334–47.\n\n\nSculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. 2015. “Hidden Technical Debt in Machine Learning Systems.” Advances in Neural Information Processing Systems 28.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Machine Learning System Design</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html",
    "href": "02-before-we-build.html",
    "title": "2  Before We Build",
    "section": "",
    "text": "2.1 Working with Stakeholders to Understand the Business Problem\nBuilding an effective machine learning (ML) system requires more than just technical knowledge and model-building skills. The success of an ML project depends heavily on the foundational planning that happens long before the first line of code is written or the first dataset is processed. This planning phase is essential to ensuring that the project aligns with the business goals, addresses the right problem, and is both technically feasible and valuable. In this chapter, we’ll explore the key considerations and actions to take before starting the development of an ML system.\nThe first step in any ML project is to collaborate closely with stakeholders to fully understand the business problem. This stage is critical because a thorough understanding of the problem directly impacts the translation of business needs into a solvable analytics problem, helps scope the project realistically, and ensures the ML system will truly meet organizational goals. Stakeholders include anyone with a vested interest in the project’s success—such as executives, department leads, data science teams, IT, and end users. Each group brings a unique perspective, and understanding their needs and expectations provides clarity on objectives, potential constraints, and the broader impact of the solution.\nEffective communication with stakeholders helps to align everyone on the project’s purpose and desired outcomes. By having these early conversations, ML practitioners gain insight into business priorities, potential data requirements, and assumptions being made about data availability, user behavior, and expected outcomes. Common assumptions that come to light in these initial meetings might include beliefs about data quality or completeness, end-user expectations of functionality, or even implicit views on the model’s potential accuracy and impact. Identifying and addressing these assumptions early on prevents misalignment and reduces the risk of costly adjustments later in the project.\nTo gain a comprehensive understanding of the business problem, ML practitioners should consider key questions with stakeholders, such as:\nThese questions help not only to clarify the business goals but also to highlight how the problem should be reframed into an analytics problem. For instance, a high-level business problem like “improving customer retention” can translate into an analytics problem focused on predicting customer churn and identifying key factors contributing to it. Defining the problem at this level makes it easier to identify relevant data, narrow down model requirements, and set realistic goals.\nAdditionally, early discussions often uncover essential requirements and constraints, such as data dependencies, privacy concerns, or specific regulatory requirements. By clarifying these aspects, ML teams can make more informed decisions on technical design, data preprocessing, and timeline estimation. Furthermore, gaining a clear understanding of the end-users’ needs helps in defining how the ML system should be integrated into existing workflows, ensuring the solution is not only technically robust but also user-friendly and practical.\nIn summary, investing time upfront to deeply understand the business problem and address underlying assumptions results in a more targeted, relevant ML solution. This shared understanding among stakeholders lays the groundwork for a smooth translation of business objectives into technical requirements, helping to scope the project effectively and set it up for long-term success.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#working-with-stakeholders-to-understand-the-business-problem",
    "href": "02-before-we-build.html#working-with-stakeholders-to-understand-the-business-problem",
    "title": "2  Before We Build",
    "section": "",
    "text": "What specific problem are we trying to solve?\nWhat are the business objectives associated with addressing this problem?\nWho will use the ML system, and what actions or decisions will it facilitate?\nWhat are the potential risks or consequences if the system produces inaccurate or unintended results?\nAre there existing solutions or processes in place, and what are their limitations?\n\n\n\n\n\n\n\n\n\n\nIsn’t this a project manager responsibility?\n\n\n\n\n\nData scientists should be involved in the early planning process of an ML project because their expertise in data and analytics enables them to assess the feasibility of proposed solutions, clarify data requirements, and identify potential technical challenges. While some ML practitioners may balk at including planning, communication, brainstorming, and other project-management-focused elements in discussions on ML projects, these early stages are often where critical project alignment occurs. In my experience, the most successful projects have involved ML team leads working closely not only with project managers and other team leads but also with representatives from the department requesting the solution. Data scientists bring essential knowledge about data limitations, model requirements, and potential roadblocks that might otherwise go unnoticed. Their involvement helps refine the project’s scope to align with both the business goals and technical realities, setting realistic expectations and paving the way for a solution that is both effective and feasible. This collaborative approach during planning ultimately saves time, enhances clarity, and boosts the likelihood of delivering a meaningful, successful ML solution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#determining-if-the-problem-requires-an-ml-solution",
    "href": "02-before-we-build.html#determining-if-the-problem-requires-an-ml-solution",
    "title": "2  Before We Build",
    "section": "2.2 Determining if the Problem Requires an ML Solution",
    "text": "2.2 Determining if the Problem Requires an ML Solution\nNow that we understand the importance of working with stakeholders to fully grasp the business problem, the next step is to translate the business problem into an analytics problem. This process will help us determine whether machine learning (ML) is the right approach to address the issue. ML can be a powerful tool, but it’s not always the best solution. Determining if ML is suitable requires thoughtful consideration, as unnecessary ML implementation can lead to wasted resources, increased complexity, and maintenance challenges.\n\nKey Considerations to Determine ML Suitability\nDetermining if a problem requires an ML solution involves assessing whether the business objectives demand certain characteristics that ML can uniquely provide.\n\nSome essential considerations include:\n\n\n\n\n\n\nLearning Complex Patterns from Data\n\n\n\n\n\nML excels when the solution requires identifying complex, often non-linear relationships within data that are difficult to capture using rules-based or traditional statistical approaches. If the problem involves intricate patterns that need to be learned from large volumes of data, ML may be appropriate. For example, ML can be effective at analyzing the complex and non-linear relationships between a customer’s purchase history, browsing behavior, and demographic characteristics to generate personalized product recommendations.\n\n\n\n\n\n\n\n\n\nAvailability of Quality Data\n\n\n\n\n\nML solutions require substantial and high-quality data for training and testing. Without sufficient data, the model cannot learn the patterns or features necessary to make accurate predictions. Data availability should include not only a large enough sample size but also a diverse dataset that represents the full range of scenarios the model will encounter in production. If quality data is unavailable, ML is unlikely to perform well and may not be worth pursuing.\n\n\n\n\n\n\n\n\n\nMaking Predictions on Unseen Data\n\n\n\n\n\nML is suitable for problems where we need the system to make predictions on new, unseen data that will continue to arrive over time. For instance, in fraud detection, the goal is for the system to recognize fraudulent behavior in real-time as new transactions occur. This is an inherently predictive task and well-suited to ML, where models trained on historical fraud data can be applied to identify anomalies in new transactions.\n\n\n\n\n\nML is Not Always the Answer\nMachine learning is a powerful tool, but it’s not a one-size-fits-all solution. Yet, in today’s fast-paced tech landscape, it’s common for business leaders to latch onto popular trends like ML, AI, and automation, believing that these technologies can solve a wide range of problems or offer an innovative edge. However, the reality is that many problems don’t require the complexity or resources of an ML system and can actually be addressed more effectively with simpler approaches. Below are examples of problems that often don’t meet the suitability criteria for ML, illustrating why a different solution may be more appropriate.\n\n\n\n\n\n\nRule-Based Decision Systems\n\n\n\n\n\nIf a problem can be solved with straightforward rules or conditions, then ML is likely unnecessary. For example, an e-commerce site offering a standard discount based on membership level (e.g., 10% off for premium members) may not need a predictive model to calculate discounts. A simple rule-based system is faster to implement, more interpretable, and easier to maintain. Business leaders sometimes assume that ML will increase personalization or provide better insights, but in some cases, it may only add complexity without significant, meaningful benefits.\n\n\n\n\n\n\n\n\n\nBasic Data Retrieval and Filtering\n\n\n\n\n\nWhen a task involves retrieving specific records or filtering data based on fixed criteria, ML is overkill. For instance, if a business wants a list of customers who made a purchase within the last month, a structured query in a database will suffice. ML could add unnecessary processing time, reduce transparency, and complicate data workflows without providing any added value. Leaders may be drawn to the idea of “automating data processes with ML,” but it’s critical to recognize that traditional data querying and reporting tools are often more efficient and will suffice when the goal is to simply slice and dice our data.\n\n\n\n\n\n\n\n\n\nStatic Reporting and Descriptive Analytics\n\n\n\n\n\nML is valuable for uncovering patterns, making predictions, or adapting to new data, but if the goal is simply to summarize historical data or calculate aggregate statistics (e.g., total sales last quarter or average customer lifetime value), traditional data analysis tools are more suitable. ML brings no additional value to tasks that involve reporting known facts. Although ML may sound impressive, using it for simple reporting can lead to unnecessary costs in computation, maintenance, and monitoring.\n\n\n\n\n\n\n\n\n\nTasks with Deterministic Outputs\n\n\n\n\n\nWhen a problem has predictable, deterministic outputs based on clear rules, ML can be redundant. For example, payroll tax calculation, which follows specific formulas, does not require a model to “learn” the process. Hard-coded logic or rule-based systems are not only more accurate in such cases but also eliminate potential errors introduced by an ML model. Business leaders sometimes believe that ML will “future-proof” deterministic tasks, but simpler systems are usually more reliable, cost-effective, and transparent.\n\n\n\n\n\n\n\n\n\nHeavily Regulated Decisions Requiring Full Transparency\n\n\n\n\n\nIn industries such as finance or healthcare, strict regulations often require full transparency for every decision, making ML—especially complex models—an unsuitable choice. For instance, mortgage approvals based on a set of criteria are better handled by rules-based systems to ensure regulatory compliance and easy interpretability. While ML can provide valuable insights, its lack of interpretability in some models makes it challenging to justify for highly regulated tasks. Business leaders may be drawn to ML’s perceived sophistication, but in some cases, a rules-based approach may be more practical and reliable.\n\n\n\nThese examples, although not comprehensive, highlight the importance of carefully evaluating whether ML is truly the right tool for the job. ML can be ideal when there’s a need to learn from complex patterns in data, make predictions on new data, or adapt dynamically over time. However, using ML for problems that don’t meet these criteria or have additional concerns as mentioned above, can waste resources, reduce transparency, and increase project complexity. Recognizing when a simpler method will be more effective is a critical skill for data scientists and ML practitioners. Avoiding unnecessary ML solutions allows teams to focus their resources where they will add the most value, while helping business leaders make more informed, strategic technology decisions.\n\n\nPitfalls of Using ML Unnecessarily\nUsing ML when it’s not needed can create more problems than it solves. ML is powerful but also complex, requiring specialized skills, infrastructure, and ongoing maintenance. When ML is applied unnecessarily, organizations risk:\n\n\n\n\n\n\nIncreased Complexity and Maintenance Costs\n\n\n\n\n\nML systems require regular monitoring, retraining, and validation to ensure they remain accurate and reliable. For example, if an ML model is deployed to handle a task that could be addressed with simple rules, the organization may end up dedicating time and resources to monitor and maintain an over-engineered solution that adds little value. Traditional software solutions often require far less maintenance and are more stable over time.\n\n\n\n\n\n\n\n\n\nReduced Interpretability and Transparency\n\n\n\n\n\nML models, especially complex ones like neural networks, can be difficult to interpret. In situations where transparency is crucial—such as compliance with regulations or gaining end-user trust—a rules-based approach might be preferable. Using ML without necessity can lead to resistance from stakeholders who need clear explanations for the system’s outputs.\n\n\n\n\n\n\n\n\n\nWaste of Time and Resources\n\n\n\n\n\nDeveloping an ML system demands resources for data preparation, model selection, and deployment, as well as ongoing maintenance and infrastructure support. If the problem does not require ML, these resources could be better spent on other areas, such as enhancing customer experience or improving data quality.\n\n\n\n\n\n\n\n\n\nPotential for Errors and Misinterpretations\n\n\n\n\n\nML models are not infallible and can produce unexpected or incorrect results, especially when faced with data outside their training set. For example, if an ML model is applied to a problem that is straightforward or deterministic, it may introduce unnecessary variability or errors, which could lead to costly mistakes. For example, a recommendation system that misclassifies products might negatively impact user experience and decrease trust in the system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#defining-performance-metrics-for-the-ml-system",
    "href": "02-before-we-build.html#defining-performance-metrics-for-the-ml-system",
    "title": "2  Before We Build",
    "section": "2.3 Defining Performance Metrics for the ML System",
    "text": "2.3 Defining Performance Metrics for the ML System\nEstablishing comprehensive performance metrics is critical for evaluating how effectively an ML system solves the intended business problem. Defining these metrics early not only provides a target for model performance but also ensures alignment between technical teams and business stakeholders. Performance metrics serve as both a development benchmark and a standard for continuous monitoring in production, helping teams maintain an effective and valuable ML system over time.\nA robust approach to performance metrics includes:\n\nModel performance metrics which measure predictive accuracy and reliability\nSystem performance metrics which gauge how well the overall ML system operates in a real-world environment\nBusiness performance metrics which measure the impact the ML system provides to the business.\n\n\n\n\n\n\n\nflowchart LR\n  subgraph ML[ML System]\n    direction BT\n    subgraph p0[Overall System Performance]\n    end\n    subgraph tech[Technical performance]\n      subgraph p1[Model performance metrics]\n      end\n      subgraph p2[System performance metrics]\n      end\n    end\n    subgraph p3[Business performance metrics]\n    end\n\n   p1 --&gt; p0\n   p2 --&gt; p0\n   p3 --&gt; p0\n  end\n\n\n\n\n\nFigure 2.2: Understanding an ML sytems performance requires multiple types of performance metrics: (1) model performance metrics, (2) system performance metrics, and (3) business performance metrics.\n\n\n\n\n\nBy incorporating all three, ML practitioners can ensure that the system is not only technically accurate but also capable of meeting the demands of production use and drives measurable improvements for the business. Let’s dive into each category in more detail.\n\nTechnical Performance Metrics\nTechnical performance metrics are essential for evaluating both the model’s accuracy and the system’s operational efficiency. These metrics allow teams to optimize not only the predictive power of the model but also its usability within a production environment. By considering both model and system performance metrics, teams can ensure the ML system is not only accurate but also performant, reliable, and capable of meeting operational requirements in production.\n\n\n\n\n\n\nModel Performance Metrics\n\n\n\n\n\nThese metrics evaluate the accuracy and reliability of the ML model in making predictions. These metrics include common metrics you likely have heard of before if you’ve done any ML tasks such as mean squared error, \\(R^2\\), and mean absolute error for regression problems; cross-entropy, gini index, precision, and recall for classification problems; or BLEU, BERTScore, and perplexity for large language models. The choice of metric depends on the type of ML task (e.g., classification, regression) and the consequences of different kinds of errors.\nFurther reading: Selecting the Right Metric for evaluating Machine Learning Models - Part 1, Part 2\n\n\n\n\n\n\n\n\n\nSystem Performance Metrics\n\n\n\n\n\nWhile model accuracy is critical, it’s equally important to assess how well the overall ML system functions in real-world conditions. These metrics track the system’s efficiency, responsiveness, and reliability, ensuring that it can handle production demands. Metrics we often consider to measure system performance include:\n\nLatency: Measures the time required for the system to make a prediction once data is available. Low latency is critical in real-time applications like fraud detection and autonomous vehicles. Imagine a pedestrian steps out in front of an autonomous vehicle, there needs to be nearly zero time (or nearly no latency) between the car’s ML system taking in that input and making a prediction to stop otherwise the consequence will be catostrophic! 1\nThroughput: Indicates the number of predictions the system can process per unit of time. High throughput is essential for large-scale applications, such as recommendation systems used by Netflix and Spotify. For example, Netflix has around 280 million customers that they need to produce recommendation predictions for on a daily to weekly basis. 2\nUptime: Tracks the percentage of time the system is fully operational. Uptime is especially important for mission-critical systems, where outages can disrupt service and lead to lost revenue.\nScalability: Assesses the system’s ability to maintain performance as user demand or data volume increases, especially while needing to maintain real-time inference requirements. For example, imagine how user demand for Amazon surges on Black Friday! 3\nResource Utilization: Tracks CPU, memory, and GPU usage to ensure efficient resource allocation, particularly relevant for cloud-based ML systems. Efficient resource utilization can significantly reduce costs, as discussed in Ramesh et al., 2020.\n\n\n\n\nConsidering both model and system performance metrics enables teams to build ML systems that are not only accurate but also highly responsive, efficient, and reliable in a production environment.\n\n\nBusiness Performance Metrics\nWhile technical metrics are essential for evaluating the functionality of the ML system, business metrics ensure that the ML system drives meaningful impact for the organization. These metrics translate technical outputs into measurable business outcomes, aligning the ML system with the broader organizational goals. Some examples follow.\n\n\n\n\n\n\nRevenue Impact\n\n\n\n\n\nFor systems designed to drive sales or conversions, revenue-focused metrics quantify effectiveness. For example:\n\nIncremental Revenue: measures the additional revenue generated by the ML system compared to a baseline. This can demonstrate the impact of a recommendation system on customer purchases (McKinsey, 2018).\nConversion Rate: measures the percentage of users who complete a desired action after interacting with the ML system (e.g., making a purchase after receiving a recommendation).\n\n\n\n\n\n\n\n\n\n\nOperational Efficiency\n\n\n\n\n\nFor applications focused on cost reduction or improving efficiency, business will tend to use metrics that capture the system’s ability to streamline processes such as:\n\nCost Savings: measures the reduction in operational costs achieved by using the ML system, such as fewer maintenance visits with predictive maintenance models (Deloitte, 2020).\nProcess Time Reduction: measures the time saved in completing a process using the ML system, relevant in areas such as automated customer support ticket resolution (IBM, 2019).\n\n\n\n\n\n\n\n\n\n\nCustomer Engagement\n\n\n\n\n\nFor ML systems that serve customers directly, engagement metrics help gauge relevance and user experience. For example:\n\nClick-Through Rate (CTR): measures the percentage of users who engage with personalized content or recommendations. CTR is a common metric for digital content recommendations (Covington et al., 2016).\nUser Retention Rate: measures the percentage of users who continue to engage with the platform over time, providing a longer-term view of the system’s relevance.\n\n\n\n\n\n\n\n\n\n\nAccuracy in High-Stakes Decisions\n\n\n\n\n\nFor applications in areas like healthcare or finance, additional metrics measure the system’s ability to make accurate, high-impact decisions. For example:\n\nReduction in Error Rates for High-Risk Cases: measures the reduction in error rates (false-positives or false-negatives) in high-stakes predictions (e.g., fraud detection), which can demonstrate the value of the ML system in risk mitigation (Fawcett & Provost, 1997).\nCompliance and Risk Management Impact: measures the system’s role in meeting regulatory standards and reducing compliance risks, especially critical in regulated industries (EU GDPR, 2018).\n\n\n\n\nBy defining and monitoring both technical and business metrics, ML teams can ensure their system is effective from both a technical and strategic perspective, delivering real business value.\n\n\nBest Practices for Implementing Performance Metrics\nSuccessfully implementing performance metrics requires thoughtful integration into the ML system lifecycle. Here are some best practices commonly used to make metrics more effective. Later chapters will dive into the best practices focused on monitoring and setting thresholds and alerts.\n\nSet Benchmarks and Targets: Establish target values for each metric based on baseline performance, industry standards, and stakeholder expectations. Benchmarks help set realistic goals for model and system improvement (Sculley et al., 2015).\nUse Multiple Metrics for a Holistic View: Relying on a single metric can lead to misleading conclusions. Using a combination of metrics—such as precision and latency, or recall and business impact—provides a balanced understanding of performance (Saito & Rehmsmeier, 2015).\nMonitor Metrics Continuously: Performance metrics should be monitored throughout the model lifecycle, including in production. Continuous monitoring helps detect issues like data drift or system degradation (Breck et al., 2017).\nSet Thresholds and Alerts: Define acceptable thresholds for critical metrics and set up alerts if the system’s performance falls below these levels. This allows for quick intervention to resolve potential issues before they impact users or business outcomes.\nRegularly Re-Evaluate Metrics: As business needs and data sources evolve, so should the metrics. Periodic re-evaluation ensures that metrics remain relevant and continue to align with the organization’s priorities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#understanding-the-potential-value-of-a-solution",
    "href": "02-before-we-build.html#understanding-the-potential-value-of-a-solution",
    "title": "2  Before We Build",
    "section": "2.4 Understanding the Potential Value of a Solution",
    "text": "2.4 Understanding the Potential Value of a Solution\nBefore embarking on building an ML system, it is essential to evaluate the potential value that the solution will bring to the organization. Understanding this value helps justify the investment of time, resources, and budget, and ensures that the ML system aligns with the organization’s business objectives. Evaluating potential value involves asking the right questions, considering both tangible and intangible benefits, and aligning with stakeholder priorities to secure buy-in.\n\nEvaluating Impact on Business Objectives\nThe success of an ML system is ultimately measured by its impact on business objectives. By evaluating how the system will improve key metrics — whether increasing revenue, reducing costs, enhancing customer satisfaction, or improving operational efficiency — teams can prioritize the features and use cases that will generate the greatest return on investment. The goal is to clearly articulate how the ML solution will contribute to the broader goals of the business, whether it’s through improved decision-making, optimized processes, or creating better customer experiences.\n\n\nQuestions to Estimate and Define the System’s Value\nTo estimate and define the value of an ML system, it’s useful to ask the following questions:\n\nHow Will the Solution Improve Business Outcomes? Consider the specific business outcomes that the ML system is intended to influence. For example, will a recommendation system increase sales by suggesting more relevant products to customers? And if so, what does the business reasonably expect those incremental sales to equate to? Or maybe the improved recommendation system will be able to make more time relevant predictions based on today’s trends or news, which could increase daily active users which could increase potential ad revenue. Or maybe the improved recommendation system is designed to reduce customer churn, which could lead to lower new customer acquisition costs. Answering these question helps determine just how the ML system is expected to benefit the organization and if the potential benefits justify the investment and ensures the solution is well-targeted.\nWhat Are the Tangible and Intangible Benefits? When evaluating the potential value, it’s important to look beyond the immediate financial benefits. Tangible benefits, such as cost savings, increased revenue, or reduced processing time, are easier to quantify and often form the basis for return on investment analyses. However, intangible benefits, like improved customer satisfaction, increased user engagement, enhanced employee productivity, or reduced risk, are equally important for long-term success but not as easy to quantify. For example, a fraud detection model may not only save money but also enhance customer trust—an intangible yet valuable outcome for the business. Although these are difficult to quantify, it is important to identify these potential benefits and determine if there is a way to quantify and track them.\n\n\n\nAligning Value Assessment with Stakeholder Priorities\nTo secure buy-in, it’s critical to align the value assessment with stakeholder priorities. This involves understanding the goals and pain points of various stakeholders and clearly articulating how the ML solution addresses them. Stakeholders, such as business executives, team leads, and end users, each have different perspectives on what constitutes value. By tailoring the value assessment to address these perspectives, ML practitioners can better communicate the expected benefits and ensure everyone is aligned on the importance of the project.\nInvolving stakeholders in this value assessment also provides an opportunity to identify potential risks, set realistic expectations, and agree on success criteria from the outset. When stakeholders see how the solution directly supports their goals, they are more likely to support the project and allocate the necessary resources to ensure its success.\nUnderstanding and clearly articulating the potential value of the ML solution lays a strong foundation for the project. It ensures that the system is designed to address the right problem, prioritizes outcomes that matter most to the business, and secures the support needed for successful development and deployment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#understanding-technical-requirements-to-determine-feasibility",
    "href": "02-before-we-build.html#understanding-technical-requirements-to-determine-feasibility",
    "title": "2  Before We Build",
    "section": "2.5 Understanding Technical Requirements to Determine Feasibility",
    "text": "2.5 Understanding Technical Requirements to Determine Feasibility\nBefore building an ML system, it’s crucial to assess the technical requirements to determine the feasibility of the project. Defining these requirements upfront helps ensure that the solution is practical, achievable, and that the necessary resources are available.\n\n\n\n\n\n\nIt’s important to note that you will not have complete information on the requirements that follow. However, getting as much understanding upfront can help drive key decisions to mitigate risks, and set the foundation for a successful project early in the process.\n\n\n\nTechnical requirements span several areas, including data, algorithms, infrastructure, deployment, and scalability. Understanding these requirements early allows teams to identify potential challenges and make informed decisions regarding the scope and direction of the project.\n\n\n\n\n\n\nflowchart LR\n  subgraph ML[ML System]\n    direction BT\n    subgraph p0[Overall Technical Requirements]\n    end\n    subgraph p1[Data requirements]\n    end\n    subgraph p2[Algorithm requirements]\n    end\n    subgraph p3[Infrastructure requirements]\n    end\n    subgraph p4[Deployment requirements]\n    end\n    subgraph p5[Scale requirements]\n    end\n\n   p1 --&gt; p0\n   p2 --&gt; p0\n   p3 --&gt; p0\n   p4 --&gt; p0\n   p5 --&gt; p0\n  end\n\n\n\n\nFigure 2.3: Technical requirements span several areas, including data, algorithms, infrastructure, deployment, and scalability.\n\n\n\n\n\n\n\n\n\n\n\nData Requirements\n\n\n\n\n\nData is the foundation of any ML system, and understanding data requirements is vital for project success. This involves identifying data sources (e.g., internal databases, third-party APIs), assessing data quality (e.g., completeness, accuracy, consistency), and determining preprocessing needs, such as data cleaning, transformation, or augmentation. Ensuring that sufficient, high-quality data is available early in the project is crucial, as poor data quality can significantly impact model performance.\n\n\n\n\n\n\n\n\n\nAlgorithm Requirements\n\n\n\n\n\nChoosing the appropriate algorithm involves considering the problem type, the complexity of potential solutions, and the interpretability needs of stakeholders. Some problems may require simple, interpretable models like linear regression, while others may benefit from more complex models, such as deep learning. The algorithm choice also impacts computation time, required data, and the ability to provide explanations for predictions—all of which influence feasibility.\n\n\n\n\n\n\n\n\n\nInfrastructure Requirements\n\n\n\n\n\nML systems often demand significant computational resources for training and inference. Understanding infrastructure requirements includes determining compute needs (e.g., CPUs, GPUs), selecting appropriate software tools (e.g., TensorFlow, PyTorch), and evaluating cloud vs. on-premises options for scalability and cost-effectiveness. For projects with heavy resource demands, cloud services can provide a flexible solution, while smaller projects may be better suited to local infrastructure.\n\n\n\n\n\n\n\n\n\nDeployment Requirements\n\n\n\n\n\nDeployment considerations include the delivery method and the integration of the model within existing systems. This could involve setting up an API endpoint to deliver predictions, integrating the model into an application, or deploying the model on edge devices. A well-planned deployment pipeline is key to automating processes such as validation, testing, and deployment, ensuring that the model moves from development to production efficiently.\n\n\n\n\n\n\n\n\n\nScale Requirements\n\n\n\n\n\nIt’s important to understand the anticipated demand for the ML system and whether it will operate in a batch or real-time setting. For systems that require low latency (e.g., fraud detection), scalability must be a priority, ensuring the system can handle large numbers of requests without degrading performance. For batch processing tasks, the focus may be more on data throughput and the ability to handle large datasets effectively.\n\n\n\nOnce the technical requirements have been defined, teams must assess whether they have the necessary resources and expertise to proceed. Gaps may exist in areas such as data availability, technical skills, or infrastructure capabilities. Addressing these gaps might involve acquiring additional data, seeking third-party services, or upskilling team members to meet project needs. Making feasibility adjustments—such as narrowing the project scope, choosing simpler algorithms, or relying on cloud infrastructure to meet compute demands—can also ensure that the project remains viable within existing constraints.\nEvaluating technical requirements in these areas ensures that the ML system is both feasible and aligned with available resources. By addressing these aspects early, teams can make informed decisions, mitigate risks, and set the foundation for a successful project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#recognizing-ml-system-development-as-an-iterative-process",
    "href": "02-before-we-build.html#recognizing-ml-system-development-as-an-iterative-process",
    "title": "2  Before We Build",
    "section": "2.6 Recognizing ML System Development as an Iterative Process",
    "text": "2.6 Recognizing ML System Development as an Iterative Process\nDeveloping an ML system is inherently an iterative process, involving continuous refinement, experimentation, and adaptation. Unlike traditional software development, where requirements and specifications are typically well-defined from the outset, ML systems evolve as they learn from data, integrate feedback, and adapt to changing business needs. Successful ML projects often require multiple rounds of model tuning, retraining, and adjustments to meet both technical and business objectives. Embracing this iterative nature helps ensure that the final solution is robust, scalable, and well-aligned with its intended purpose.\n\n\n\n\n\n\nflowchart LR\n  subgraph planning[Foundational Planning]\n    direction TB\n    subgraph p1[Understand the Business Problem]\n    end\n    subgraph p2[Determine ML Suitability]\n    end\n    subgraph p3[Define Performance Metrics]\n    end\n    subgraph p4[Understand the Potential Value]\n    end\n    subgraph p5[Understand Technical Requirements]\n    end\n  end\n\n  p1 --&gt; p2 --&gt; p3 --&gt; p4 --&gt; p5\n\n  planning --\"Iteration\"--&gt; building[ML System Development] --\"Iteration\"--&gt; planning\n\n\n\n\n\nFigure 2.4: Developing an ML system is inherently an iterative process, involving continuous refinement, experimentation, and adaptation as more information is learned through the planning and ML system development process.\n\n\n\n\n\nAs an ML project progresses, many of the topics discussed in this chapter—including performance metrics, the valuation of the project, technical requirements, and even the business problem itself—are likely to evolve. For instance:\n\nMetrics to Assess: Initial metrics defined to assess model performance or system efficiency may need to change if they are found to be insufficient, misleading, or if stakeholder priorities shift. For example, a metric like accuracy may become less useful if a focus on precision and recall better reflects the business goal.\nValuation of the Project: The estimated value of the project may evolve as new opportunities or limitations arise during development. The intended business impact, such as increasing customer engagement, may need revaluation if customer behaviors change or if new data sources reveal additional insights.\nTechnical Requirements: Technical needs such as data quality, infrastructure, or deployment pipelines often need adjustments based on model experiments, results, and feasibility assessments. Iterative model training may reveal gaps in data that require new sources or different preprocessing techniques, or identify infrastructure constraints that necessitate changes to compute resources.\nBusiness Problem: As the project progresses, the understanding of the business problem may change. Engaging with stakeholders and observing early results may lead to a deeper understanding of what needs to be solved, prompting refinements to the problem’s scope.\n\nThis iterative process requires finding a balance between being flexible enough to adapt to new information while avoiding uncontrolled changes, known as scope creep. Scope creep can derail a project by causing a continuous expansion of objectives, leading to wasted resources, delays, and reduced effectiveness. To manage this, teams should establish clear, well-documented milestones and revisit them periodically to assess if changes are necessary and justified. Any adjustments should be based on insights gained during iterations, with stakeholders involved in the decision-making process to ensure alignment.\nRecognizing ML system development as an iterative process allows teams to embrace change when needed while maintaining focus on the ultimate goals. It fosters continuous improvement, ensuring that the final system is effective, meets business objectives, and is resilient to the dynamic nature of data and business environments. By embracing iteration thoughtfully and managing scope effectively, ML practitioners can deliver solutions that are both impactful and sustainable.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#summary",
    "href": "02-before-we-build.html#summary",
    "title": "2  Before We Build",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nThis chapter has outlined the crucial planning steps required before diving into the development of a machine learning (ML) system. Effective ML projects start with a deep understanding of the business problem, achieved by collaborating closely with stakeholders to ensure alignment between business needs and technical efforts. It is vital to evaluate whether the problem truly requires an ML solution, as not all issues benefit from the complexity and resource demands of machine learning. By avoiding unnecessary ML implementation, teams can focus resources on areas that provide meaningful impact.\nWe also explored the importance of defining performance metrics to assess both technical and business outcomes, as well as evaluating the potential value of the solution to determine if it justifies the investment. Technical requirements, such as data availability, infrastructure, and deployment needs, must be understood early to ensure feasibility and identify gaps that could impede progress. Finally, we recognized that ML system development is inherently iterative. Changes to metrics, requirements, or even the business problem are likely as the project progresses, requiring a careful balance between flexibility and avoiding scope creep.\nThe chapters that follow will equip you with the knowledge to understand and apply these foundational concepts, identify key decisions when designing an ML system, and effectively implement the tools and best practices required to build successful ML solutions. By building on this strong foundation, you will be better prepared to create ML systems that are impactful, adaptable, and well-aligned with organizational goals.\n\n\n\n\n\n\nFurther Reading\n\n\n\n\n\n\nDesigning ML-powered Software: This reading has a great section focused on the Machine Learning Canvas, a tool to structure the ML project, help to specify the core requirements, and answer the question – what do we want to achieve for the end-users of the predictive system?\n\n\n\n\nThe Machine Learning Canvas",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#exercise",
    "href": "02-before-we-build.html#exercise",
    "title": "2  Before We Build",
    "section": "2.8 Exercise",
    "text": "2.8 Exercise\nConsider a scenario where a healthcare organization wants to build an ML system to improve patient outcomes by predicting hospital readmissions. To help guide this example, read the first three pages (sections 1-1.4) of this real-life case study of Mount Sinai Hospital in New York City. Use this example to help guide you in answering the following questions as thoroughly as the given information allows:\n\nStakeholder Engagement\n\nWho would the key stakeholders be for this project?\nWhat questions would you ask these stakeholders to ensure you understand the business problem?\nWhat assumptions might be uncovered during these discussions?\n\nEvaluating ML Suitability\n\nAssess whether ML is a suitable solution. What factors would you consider to determine if ML is appropriate for this problem?\nProvide an example of an alternative, non-ML approach that could be considered. What are the limitations of this approach compared to an ML approach?\n\nDefine Performance Metrics\n\nDefine three performance metrics for the ML system. Include at least one technical metric (e.g., accuracy), one system performance metric (e.g., latency), and one business metric (e.g., reduction in readmission rates).\nExplain why each of these metrics is important for evaluating the success of the ML system.\n\nUnderstanding Value and Feasibility\n\nWrite a paragraph that outlines the potential value of the ML system to the healthcare organization. Consider both tangible (e.g., cost savings) and intangible (e.g., improved patient satisfaction) benefits.\nList some key technical requirements that would be helpful to understand early on before developing the solution (e.g., data, infrastructure). What gaps might exist, and how would you address them?\n\nThe Iterative Process\n\nDescribe why the development of this ML system would be an iterative process. Provide an example of something that could change during development (e.g., a performance metric, a technical requirement) and how you would manage this change to avoid scope creep.\n\n\n\n\n\n\nMattson, Peter, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, et al. 2020. “Mlperf Training Benchmark.” Proceedings of Machine Learning and Systems 2: 336–49.\n\n\nTolomei, Gabriele, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas. 2017. “Interpretable Predictions of Tree-Based Ensembles via Actionable Feature Tweaking.” In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 465–74.\n\n\nVemulapalli, Gopichand. 2023. “Operationalizing Machine Learning Best Practices for Scalable Production Deployments.” International Machine Learning Journal and Computer Engineering 6 (6): 1–21.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "02-before-we-build.html#footnotes",
    "href": "02-before-we-build.html#footnotes",
    "title": "2  Before We Build",
    "section": "",
    "text": "Latency is well-covered in Tolomei et al. (2017), which discusses latency optimization for ML infrastructure.↩︎\nMattson et al. (2020) explores techniques for optimizing throughput in production ML systems.↩︎\nVemulapalli (2023) discusses some machine learning best practices for scalable production deployments.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Before We Build</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html",
    "href": "03-dataops-role.html",
    "title": "3  The Role of DataOps",
    "section": "",
    "text": "3.1 Data Ingestion\nDataOps, short for Data Operations, is a collaborative and agile approach to designing, implementing, and managing data workflows. It is an essential pillar within the MLOps lifecycle, ensuring that data — the lifeblood of any machine learning system — is efficiently, reliably, and securely delivered to support model training and inference. While MLOps focuses on the end-to-end process of deploying and maintaining machine learning systems, DataOps hones in on the data-specific aspects, addressing the challenges of data management, processing, and quality control. By embedding DataOps practices into the MLOps framework, organizations can build scalable, reliable ML systems that deliver consistent and meaningful results.\nThe primary goals of DataOps are to ensure that data pipelines are efficient, reliable, and produce high-quality data for machine learning workflows. These goals are achieved through robust processes for:\nTogether, these core components form the backbone of DataOps, enabling teams to handle growing data volumes, ensure compliance with regulations, and adapt to evolving business needs. By establishing a strong DataOps foundation, organizations can mitigate risks like data inconsistencies, inefficiencies, and errors, ultimately paving the way for successful ML systems.\nThis chapter will discuss each of these core components and then the next chapter will start giving you the tools and patterns used to implement these components.\nData ingestion involves gathering and importing data from multiple sources into a system for storage, processing, and use in machine ML workflows. The nature of the data sources, their structure, and the method of ingestion play a significant role in determining the efficiency and reliability of the ML system. In this section, we will explore the fundamental differences between data sources, when to use them, and their advantages and disadvantages. We will also compare batch and streaming ingestion methods and their suitability for different scenarios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-ingestion",
    "href": "03-dataops-role.html#data-ingestion",
    "title": "3  The Role of DataOps",
    "section": "",
    "text": "Understanding Data Sources\nData sources provide the foundation for machine learning (ML) workflows, and the choice of data source significantly impacts the design and effectiveness of the ML system. Each type of data source has unique characteristics, use cases, advantages, and limitations. Understanding these aspects is essential for building an efficient and scalable data ingestion pipeline.\n\n\n\n\n\n\nDatabases\n\n\n\n\n\nDatabases are structured systems designed to store, manage, and retrieve data efficiently. They are commonly used for transactional data, such as customer records, sales transactions, or financial ledgers.\n\nWhen to Use: Databases are ideal when data is frequently updated, needs to be queried precisely, or must maintain high consistency. For example, an e-commerce application may use a relational database to track user purchases and manage inventory levels. NoSQL databases are better suited for dynamic or semi-structured data, such as user-generated content or real-time event logs.\nAdvantages:\n\nRelational Databases (e.g., MySQL, PostgreSQL): Ensure data integrity through schema enforcement and strong consistency models.\nNoSQL Databases (e.g., MongoDB, DynamoDB): Offer flexibility for semi-structured or unstructured data and scale horizontally to handle growing data volumes.\n\nDisadvantages:\n\nRelational databases can struggle with scalability in high-throughput applications.\nNoSQL databases may not provide strong transactional guarantees, which could be problematic for certain use cases.\n\n\n\n\n\n\n\n\n\n\n\nAPIs\n\n\n\n\n\nAPIs (Application Programming Interfaces) enable programmatic access to data from external systems or services. They are often used to fetch dynamic data, such as weather updates, financial market data, or social media interactions.\n\nWhen to Use: APIs are most useful when the data is maintained externally and needs to be accessed on-demand. For example, a stock-trading platform might fetch real-time price updates through a financial market API.\nAdvantages:\n\nProvide a standardized way to access external data.\nAllow flexible querying of specific fields or data ranges.\nEnable real-time or near-real-time access to dynamic data.\n\nDisadvantages:\n\nAPI access can be rate-limited, introducing potential delays in data collection.\nDependence on external systems may lead to availability or latency issues if the API provider experiences downtime.\nOften requires robust error handling and retries to ensure reliability.\n\n\n\n\n\n\n\n\n\n\n\nData Lakes\n\n\n\n\n\nData lakes serve as centralized repositories for storing large volumes of raw, unstructured, and semi-structured data. They are designed to handle diverse datasets, such as logs, multimedia files, and IoT sensor readings.\n\nWhen to Use: Data lakes are ideal for big data, where the organization needs to store vast amounts of heterogeneous data for future exploration or processing. For instance, a media company might use a data lake to aggregate clickstream logs, user profiles, and video content metadata.\nAdvantages:\n\nEnable storage of massive datasets at a low cost.\nProvide flexibility for processing data in various ways, such as using batch or streaming pipelines.\nAllow analysts and data scientists to explore raw data without predefined schemas.\n\nDisadvantages:\n\nLack of enforced schema can lead to inconsistent data organization, often referred to as a “data swamp.”\nSlower access times compared to structured systems, especially for specific queries.\nRequire strong governance and metadata management to maintain data discoverability and usability.\n\n\n\n\n\n\n\n\n\n\n\nReal-Time Data Streams\n\n\n\n\n\nReal-time data streams consist of continuous flows of data generated by sources like IoT devices, user interactions, or event-driven systems. They are commonly used for applications requiring immediate insights, such as fraud detection, predictive maintenance, or live recommendation engines.\n\nWhen to Use: Real-time streams are essential when time sensitivity is critical. For example, an autonomous vehicle must process sensor data streams in real-time to make split-second decisions.\nAdvantages:\n\nProvide up-to-date information for time-sensitive applications.\nSupport dynamic updating of ML models and dashboards in near real-time.\nEnable responsiveness to events as they occur, improving decision-making agility.\n\nDisadvantages:\n\nRequire significant infrastructure to support continuous ingestion and processing.\nCan be complex to implement and maintain, especially for low-latency systems.\nHigher resource costs compared to batch processing due to the always-on nature of real-time systems.\n\n\n\n\n\nSelecting the right data source depends on the specific requirements of the ML system and its intended use case. For instance:\n\nUse databases when precise, structured data is needed for queries and frequent updates are expected.\nLeverage APIs when data resides in external systems and must be fetched dynamically or on demand.\nOpt for data lakes when dealing with vast amounts of heterogeneous data that may be analyzed in diverse ways over time.\nImplement real-time data streams when time-sensitive insights or rapid responses are required.\n\nBy understanding the fundamental differences and trade-offs between data sources, ML teams can design data ingestion pipelines tailored to their needs, ensuring efficient, reliable, and scalable workflows.\n\n\n\n\n\n\nYou don’t always have a choice!\n\n\n\n\n\nWhile choosing the ideal data source is important, you don’t always have a choice. In many cases, the data source is determined by external constraints—such as an organization’s existing infrastructure, third-party providers, or legacy systems. For example, if a company’s customer data is only available through a legacy database, you must work with that database, regardless of its limitations. Similarly, when pulling weather or stock market data, you may be limited to an API provided by the service provider, even if it introduces latency or rate limits. A critical part of data ingestion is recognizing these constraints early and designing the pipeline to work efficiently with the available data source.\n\n\n\n\n\nBatch vs. Streaming Ingestion\nIn the previous section, we discussed the idea of real-time data streams. Let’s discuss this a little more. The method of data ingestion — batch or streaming — determines how data flows into the ML system. Each method has distinct characteristics suited to specific needs.\nBatch ingestion collects data in chunks or intervals, such as daily, weekly, monthly, etc. This method is ideal for scenarios where real-time data is not critical, and the focus is on processing large volumes of data efficiently. For example, an e-commerce company may use batch ingestion to aggregate and analyze customer orders from the previous day. The simplicity of batch ingestion makes it easier to implement, and it is more cost-effective since it requires fewer continuous resources. However, its periodic nature introduces latency, which may be unacceptable in time-sensitive applications.\n\n\n\n\n\n\nflowchart LR\n    subgraph Monday\n      direction TB\n      ob1(observation) --&gt; id1[(Data source)]\n      ob2(observation) --&gt; id1\n      ob3(observation) --&gt; id1\n    end\n    subgraph Tuesday\n      direction TB\n      ob4(observation) --&gt; id2[(Data source)]\n      ob5(observation) --&gt; id2\n      ob6(observation) --&gt; id2\n    end\n    subgraph Wednesday\n      direction TB\n      ob7(observation) --&gt; id3[(Data source)]\n      ob8(observation) --&gt; id3\n      ob9(observation) --&gt; id3\n    end\n    processing[Batch data processing]\n    downstream[Downstream ML system processes]\n\n    Monday --&gt; processing\n    Tuesday --&gt; processing\n    Wednesday --&gt; processing\n    processing --&gt; downstream\n\n\n\n\nFigure 3.2: Batch processing collects data in chunks or intervals, such as on a daily cadence. In this example, every day a batch process is ran where data is ingested from its relevant data sources, processed and fed to downstream processes in the ML system.\n\n\n\n\n\nStreaming ingestion, by contrast, involves the continuous collection of data as it becomes available. This method is essential for use cases requiring real-time insights, such as fraud detection systems or live recommendation engines. Streaming allows systems to react immediately to new data, providing up-to-date results. However, this approach introduces higher complexity and infrastructure costs, as systems must be designed for low-latency processing and scalability. For instance, a financial institution detecting fraudulent transactions in real-time must ingest and process data streams from payment systems within milliseconds.\n\n\n\n\n\n\nflowchart LR\n    ob1(observation) --&gt; processing[Real-time data processing]\n    ob2(observation) --&gt; processing\n    ob3(observation) --&gt; processing\n    ob4(observation) --&gt; processing\n    ob5(observation) --&gt; processing\n\n    processing --&gt; downstream[Downstream ML system processes]\n    processing --&gt; id3[(Database)]\n\n\n\n\n\nFigure 3.3: Streaming, or real-time, systems process each observation as it comes in and then feeds that data to downstream ML systems and, simoultaneously, to a database for longterm storage.\n\n\n\n\n\nIn practice, organizations often adopt a hybrid approach, using batch ingestion for historical data and streaming ingestion for real-time updates. For example, a retail company may analyze historical sales trends using batch data while simultaneously processing live customer behavior data through streams. This strategy leverages the strengths of both methods, ensuring comprehensive and timely insights for ML systems.\n\n\nExamples\nBelow are three scenarios describing machine learning systems that utilize different types of data sources and require varying ingestion methods. For each scenario:\n\nIdentify the data source(s) (e.g., database, API, real-time stream, data lake) that are being used, or that you believe should be used.\nDetermine whether the ingestion process should be batch, streaming, or a hybrid approach.\nJustify your choice of ingestion method by considering the system’s requirements for latency, data volume, and frequency of updates.\n\n\n\n\n\n\n\nScenarios\n\n\n\n\n\nScenario 1: Predictive Maintenance for Industrial Machines\nA manufacturing company is building an ML system to predict when machines on the production line will require maintenance to avoid unplanned downtime. The system collects data from IoT sensors installed on the machines. These sensors continuously send information such as temperature, vibration levels, and pressure readings. The ML system needs to analyze this incoming data in real time to provide timely predictions and alerts for potential breakdowns.\nScenario 2: Customer Segmentation for a Retail Business\nA retail company wants to build an ML system to segment customers based on their purchase history, demographic data, and online behavior. The data comes from two sources: a relational database that stores historical transaction data and an API that provides weekly updates on recent marketing campaign responses. The system generates segmentation insights monthly for marketing teams to design personalized campaigns.\nScenario 3: Fraud Detection for Financial Transactions\nA bank is developing an ML system to detect fraudulent transactions. The system receives data from real-time transaction streams as customers make payments using credit cards. The ML model must analyze each transaction immediately to flag suspicious activity and trigger appropriate alerts. Historical data stored in a data lake is also used periodically to retrain the fraud detection model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-processing",
    "href": "03-dataops-role.html#data-processing",
    "title": "3  The Role of DataOps",
    "section": "3.2 Data Processing",
    "text": "3.2 Data Processing\nData processing is a cornerstone of machine learning workflows, transforming raw, messy, or unstructured data into a clean, structured, and feature-rich format that models can effectively use. The quality and reliability of this step directly impact the performance of the machine learning system, making it one of the most critical stages in the pipeline.\n\n”Poor data quality is Enemy #1 to the widespread, profitable use of machine learning, and for this reason, the growth of machine learning increases the importance of data cleansing and preparation. The quality demands of machine learning are steep, and bad data can backfire twice – first when training predictive models and second in the new data used by that model to inform future decisions.”1\n\nIn practice, data processing involves several interrelated tasks. First, data sampling ensures that the volume of data being processed is manageable while maintaining representativeness. Next, data quality issues—such as missing values, inconsistencies, and outliers—are addressed through cleaning to ensure that the data is accurate and usable. Feature engineering comes next, where meaningful features are crafted from the raw data to capture relevant patterns and insights for the model. Finally, data leakage, a subtle yet potentially devastating issue, must be carefully avoided to ensure the integrity and generalizability of the model.\nThis section will explore these topics in detail, emphasizing their role in building reliable and scalable ML systems. By the end, you will understand the importance of thoughtful data processing and its role in creating robust machine learning pipelines.\n\nData Sampling\nData sampling is a crucial step in the data processing pipeline, particularly when working with large datasets. In many real-world scenarios, it can be infeasible or inefficient to process all available data due to computational constraints, resource limitations, or time considerations. Sampling allows data practitioners to work with a manageable subset of the data that accurately represents the overall population, ensuring efficient processing without compromising model performance or insights.\n\nWhy Sampling is Necessary\nModern machine learning systems often have access to massive datasets, ranging from millions of customer transactions to terabytes of sensor data from IoT devices. While having more data is generally beneficial, processing all of it can be resource-intensive and unnecessary, especially during exploratory data analysis or initial model development. Sampling enables practitioners to:\n\nReduce computational load, making processing faster and less expensive.\nTest and prototype pipelines or algorithms on smaller, representative datasets.\nAvoid introducing bias or errors due to overfitting large, redundant data.\n\nFor example, an organization analyzing customer purchase behavior across millions of transactions might sample a subset of data to identify initial trends or test new features before scaling up to the entire dataset.\n\n\nTypes of Sampling\nTwo primary sampling methods are commonly used in data processing: nonprobability sampling and probability sampling. Each has specific use cases, advantages, and limitations.\n\n\n\n\n\n\nNonprobability Sampling\n\n\n\n\n\nNonprobability sampling involves selecting data points based on specific criteria or convenience rather than assigning each data point a known chance of inclusion. This method is often used when quick insights or targeted analysis is needed, even if the results may not generalize to the entire population. Some examples include:\n\nConvenience sampling: Samples of data are selected based on their availability. This sampling method is popular because, well, it’s convenient.\nSnowball sampling: Future samples are selected based on existing samples. For example, to scrape legitimate Twitter accounts without having access to Twitter databases, you start with a small number of accounts, then you scrape all the accounts they follow, and so on.\nJudgment sampling: Experts decide what samples to include.\nQuota sampling: You select samples based on quotas for certain slices of data without any randomization. For example, when doing a survey, you might want 100 responses from each of the age groups: under 30 years old, between 30 and 60 years old, and above 60 years old, regardless of the actual age distribution.\n\nThe samples selected by nonprobability criteria are not representative of the real-world data and therefore are riddled with selection biases.(Heckman 2013) Because of these biases, you might think that it’s a bad idea to select data to train ML models using this family of sampling methods. You’re right. Unfortunately, in many cases, the selection of data for ML models is still driven by convenience.\nOne example of these cases is language modeling. Language models are often trained not with data that is representative of all possible texts but with data that can be easily collected—Wikipedia, Common Crawl, Reddit.\nAnother example is data for training self-driving cars. Initially, data collected for self-driving cars came largely from two areas: Phoenix, Arizona (because of its lax regulations), and the Bay Area in California (because many companies that build self-driving cars are located here). Both areas have generally sunny weather. In 2016, Waymo expanded its operations to Kirkland, Washington, specially for Kirkland’s rainy weather,(Lerman February 3, 2016) but there’s still a lot more self-driving car data for sunny weather than for rainy or snowy weather.\nNonprobability sampling can be a quick and easy way to gather your initial data to get your project off the ground. However, for reliable models, you want to use probability-based sampling if possible.\n\n\n\n\n\n\n\n\n\nProbability sampling\n\n\n\n\n\nProbability sampling ensures that every data point in the population has a known and non-zero chance of being included in the sample. This method is ideal for obtaining a representative subset of data and minimizing selection bias. Several subtypes of probability sampling exist, including:\n\nSimple Random Sampling: Every data point in the population has an equal chance of being selected. Often used when the dataset is large and homogeneous, such as selecting random customer records.\nStratified Sampling: The population is divided into distinct groups (strata), and samples are drawn proportionally from each group to ensure representation. This is often used in customer churn analysis to ensure proportional representation of customer demographics (e.g., age groups, regions).\nWeighted Sampling: Data points are assigned weights based on their importance or relevance, and sampling is performed based on these weights. Commonly used in recommendation systems to oversample high-value customers or popular products.\nReservoir Sampling: A fixed-size random sample is drawn from a stream of data, ensuring every item has an equal probability of being included. Commonly used for sampling large, continuous data streams like website clickstream logs in real-time.\nImportance Sampling: Data points are sampled based on their likelihood of influencing the model or target variable, focusing on high-impact points. Used in rare-event modeling, such as fraud detection, where fraudulent transactions are more heavily sampled.\n\n\n\n\nThe choice between sampling approaches depends on the goals and context of the analysis. In some cases, a hybrid approach may be appropriate, combining probability and nonprobability sampling. For example, a regional utility company wants to predict energy demand to optimize power generation and reduce operational costs. The dataset includes hourly energy consumption data from residential, commercial, and industrial customers across several regions, along with weather data and regional economic indicators. A hybrid approach such as the following may be required:\n\nProbability Sampling (Stratified Sampling for Regional Representation): The team uses stratified sampling to ensure that data is proportionally drawn from different regions (e.g., urban, suburban, rural) and customer types (residential, commercial, industrial). This ensures that the dataset is balanced and representative of the diverse energy consumption patterns across the utility’s coverage area.\n\nReason: Stratified sampling ensures that the model generalizes well across all regions and customer categories, reflecting realistic energy usage patterns.\n\nNonprobability Sampling (Convenience Sampling of Weather Data): The utility supplements the dataset with convenience sampling of weather data, pulling information from readily available weather stations rather than ensuring even geographic coverage. The data includes temperature, humidity, and wind speed for regions with accessible sensors.\n\nReason: Weather data is crucial for understanding energy demand (e.g., high temperatures lead to increased air conditioning use), but some rural areas lack weather stations. Convenience sampling allows the utility to quickly incorporate available weather data without delaying the project.\n\n\n\n\n\nData Quality\nData quality is a foundational pillar of any successful machine learning (ML) system. High-quality data ensures that the insights derived from the system are accurate, reliable, and actionable, while poor data quality can lead to flawed models, wasted resources, and misguided decisions. In the context of machine learning, data quality is about more than just correctness—it encompasses the accuracy, completeness, consistency, and relevance of the data being used.\nRaw data is often messy and unstructured, riddled with issues such as missing values, duplicates, inconsistencies, and outliers. These imperfections, if left unaddressed, can skew results, reduce model performance, and compromise the integrity of the ML system. Cleaning and validating data is, therefore, a critical step in the pipeline, ensuring that the inputs provided to models are trustworthy and representative of real-world scenarios.\nThis section explores the most common data quality challenges faced in ML workflows, their potential impacts, and the techniques used to address them. From handling missing data to removing duplicates and managing outliers, you’ll learn how to prepare data that meets the standards required for robust and reliable ML systems. By understanding and prioritizing data quality, practitioners can lay the groundwork for models that perform well not just in development but also in production, where the stakes are higher.\n\nCommon Data Quality Concerns\nData quality is the foundation of reliable and effective machine learning systems. Poor-quality data—riddled with missing values, inconsistencies, duplicates, outliers, or irrelevant features—can lead to biased models, inaccurate predictions, and flawed insights. Addressing these challenges requires a deep understanding of their causes, impacts, and resolutions. In this section, we’ll explore these issues in detail, using examples to illustrate their real-world relevance and techniques to resolve them.\n\n\n\n\n\n\nMissing data\n\n\n\n\n\nWhat it is: Missing data occurs when some observations lack values in one or more features. This issue can arise due to incomplete data collection processes, technical issues, or privacy restrictions. Missing data is often classified into three categories:\n\nMissing Completely at Random (MCAR): The absence of data is entirely independent of other variables.\nMissing at Random (MAR): Missingness is related to other observed variables but not the missing variable itself.\nMissing Not at Random (MNAR): Missingness is directly related to the value of the missing variable.\n\nExample: A hospital dataset tracking patient health outcomes might have missing values for “blood pressure” or “heart rate” because some patients did not complete their check-ups.\nImpact:\n\nMissing data can introduce bias, especially if the missingness is not random. For example, if patients with severe health issues are more likely to skip check-ups, the dataset will underrepresent critical cases, skewing the model.\nCertain machine learning algorithms, such as logistic regression, cannot handle missing values directly, leading to errors in training.\n\nTechniques:\n\nImputation: Replace missing values with statistical estimates (e.g., mean, median) or model-based predictions. For instance, you might fill in missing blood pressure values using averages grouped by age or gender.\nRemoval: Discard rows or columns with significant missingness if they don’t impact the overall dataset integrity. For example, dropping patients with multiple missing attributes may preserve data quality.\nDomain-Specific Solutions: Work with domain experts to infer missing values or augment the dataset with external data sources. For example, use past medical records to estimate missing patient data.\n\n\n\n\n\n\n\n\n\n\nInconsistent data\n\n\n\n\n\nWhat it is: Inconsistent data refers to variations in formatting, units, or categorizations across records. These inconsistencies often occur when datasets from different sources are merged or when collection standards are not enforced.\nExample: A global sales dataset might include prices recorded in both USD and EUR without indicating the currency, leading to ambiguity.\nImpact:\n\nPreprocessing pipelines may fail if they expect consistent inputs, such as a single currency or uniform date format.\nInconsistent data can lead to incorrect feature transformations, misinterpretation of model results, and reduced accuracy.\n\nTechniques:\n\nStandardization: Convert data into consistent formats before processing. For example, ensure all prices are converted to USD using the prevailing exchange rate.\nValidation Rules: Set up automated checks to enforce consistency, such as verifying that all date fields follow a standard format (e.g., YYYY-MM-DD).\nData Cleaning Scripts: Automate the process of detecting and resolving inconsistencies. For example, scripts can identify and reconcile category mismatches like “NYC” and “New York City.”\n\n\n\n\n\n\n\n\n\n\nDuplicate data\n\n\n\n\n\nWhat it is: Duplicate data consists of redundant records that represent the same observation multiple times. This issue often arises during data integration or repeated logging.\nExample: A customer database might have two identical entries for a single customer due to an error during data migration.\nImpact:\n\nDuplicates can inflate metrics like customer counts or revenue totals, leading to incorrect conclusions.\nThey bias models by overrepresenting certain patterns, making the model less generalizable.\n\nTechniques:\n\nDeduplication: Identify and remove duplicates using unique identifiers, such as email addresses or order IDs. In the absence of such identifiers, similarity measures like fuzzy matching can help detect duplicates.\nMerge Strategies: For partially overlapping duplicates, combine the relevant details into a single record. For example, consolidate multiple entries for a customer into one record with the most complete information.\nPreventative Measures: Enforce data integrity constraints during collection to avoid duplicates. For instance, ensure that customer IDs are unique within the database.\n\n\n\n\n\n\n\n\n\n\nOutiers\n\n\n\n\n\nWhat it is: Outliers are data points that deviate significantly from the rest of the dataset. They may represent errors, rare events, or natural variability.\nExample: A dataset tracking daily transactions might include a single record for a $1,000,000 purchase, which could be an error or a legitimate bulk order.\nImpact:\n\nOutliers can distort statistical measures such as mean and variance, leading to unreliable preprocessing steps like feature scaling.\nAlgorithms like linear regression and k-means clustering are highly sensitive to outliers, which can result in poor model performance.\n\nTechniques:\n\nOutlier Detection: Use statistical methods like interquartile range (IQR) or z-scores to identify anomalous points. For example, flag transactions that exceed three standard deviations from the mean.\nCapping or Truncation: Limit extreme values to predefined thresholds, such as capping all sales amounts above the 95th percentile.\nDomain Expertise: Determine whether outliers are valid or erroneous. For instance, consult sales teams to confirm whether a high transaction value is legitimate.\n\n\n\n\n\n\n\n\n\n\nRedundant or irrelevant data\n\n\n\n\n\nWhat it is: Redundant data includes features that duplicate information, while irrelevant data consists of variables that do not contribute meaningfully to the predictive task.\nExample: A dataset might include both “total purchase amount” and “average purchase amount per transaction,” where one feature can be derived from the other.\nImpact:\n\nRedundant or irrelevant features add noise to the dataset, increasing training time and risking overfitting.\nIrrelevant features can obscure meaningful patterns, reducing model interpretability and accuracy.\n\nTechniques:\n\nFeature Selection: Use statistical tests or model-based methods to identify and retain the most relevant features. For example, mutual information scores can indicate which variables are most predictive of the target variable.\nDimensionality Reduction: Combine redundant features into simpler representations using techniques like PCA. For instance, reduce high-dimensional text features into key topics or sentiment scores.\nExpert Review: Consult domain experts to exclude features that are not relevant. For example, remove “zip code” from a churn prediction model if it does not influence customer behavior.\n\n\n\n\nHigh-quality data is essential for robust and reliable machine learning systems. By addressing common issues such as missing values, inconsistencies, duplicates, outliers, and irrelevant features, practitioners can ensure that their datasets are clean, consistent, and ready for analysis. Each of these challenges can be mitigated through careful planning, appropriate techniques, and collaboration with domain experts, resulting in models that deliver meaningful and trustworthy results.\n\n\nImportance of Domain Knowledge\nDomain knowledge is essential when addressing data quality concerns in machine learning systems. While technical tools and methodologies provide the means to process and clean data, domain knowledge ensures that these actions are contextually informed and relevant to the problem at hand. Without a thorough understanding of the domain, practitioners risk misinterpreting data issues, applying inappropriate fixes, or overlooking critical nuances that could significantly impact the performance and reliability of the ML system.\n\nContextualizing Data Issues\nEach data quality issue—whether missing data, inconsistencies, duplicates, outliers, or irrelevant features—has unique implications that are best understood with domain expertise. For example, in healthcare datasets, missing values for key features like patient blood pressure or glucose levels might indicate incomplete check-ups or testing, but the severity of these gaps can only be properly assessed by medical professionals. Similarly, in financial datasets, an unusually large transaction might initially appear as an outlier, but domain experts can determine whether it reflects a legitimate bulk order or a fraudulent activity. By contextualizing these issues, domain experts help ensure that data quality interventions address the root causes without compromising the integrity of the dataset.\n\n\nGuiding Data Quality Strategies\nDomain knowledge informs the strategies used to resolve data quality issues. For instance:\n\nWhen dealing with inconsistent data, such as variations in currency or measurement units, domain experts can recommend appropriate standardization practices, such as converting all sales figures to a base currency like USD.\nFor duplicate data, they can help identify unique identifiers, such as customer IDs or transaction numbers, to accurately merge or remove records without losing valuable information.\nIn addressing redundant or irrelevant data, domain knowledge is invaluable for determining which features are meaningful to the predictive task and which can be safely excluded.\n\nThese insights ensure that data cleaning and preprocessing efforts enhance the dataset’s utility for machine learning without introducing new biases or inaccuracies.\n\n\nEnsuring Alignment with Business Objectives\nAddressing data quality issues is not just a technical task; it is also about ensuring that the cleaned dataset aligns with the organization’s goals and priorities. Domain experts bridge the gap between raw data and business needs by defining what “quality” means in the context of the problem. For example:\n\nIn a customer churn prediction model, domain experts might emphasize the importance of including recent complaint data, even if it contains some inconsistencies, because of its strong predictive value.\nIn an e-commerce recommendation system, domain experts might prioritize accurate timestamp data for user interactions, as it is critical for understanding seasonal trends and customer behavior.\n\nBy incorporating domain knowledge, ML practitioners can tailor their data quality efforts to deliver insights and predictions that drive meaningful business outcomes.\n\n\n\n\nFeature Engineering\nFeature engineering is the process of transforming raw data into meaningful inputs that enhance a machine learning model’s performance. It involves creating, modifying, or combining features to highlight relevant patterns or relationships that the model can learn from effectively. This process is critical to the success of machine learning systems, as the quality and relevance of features often have a more significant impact on model performance than the choice of algorithm.\n\nThe Role of Feature Engineering\nAt its core, feature engineering serves to bridge the gap between raw data and a machine learning model’s input requirements. Raw data, while abundant, often contains noise, irrelevant details, or complexities that hinder model performance. Feature engineering transforms this raw data into a more structured, interpretable, and informative format, making it easier for the model to identify patterns and relationships.\n\n\n\n\n\n\nflowchart LR\n    data[(Raw Data)] --&gt; feat{Feature&lt;br&gt;Engineering}\n    feat --&gt; mldata[(Model Input&lt;br&gt;Requirements)]\n    mldata --&gt; ml[Model Building]\n\n\n\n\n\nFigure 3.4: Feature engineering serves to bridge the gap between raw data and a machine learning model’s input requirements.\n\n\n\n\n\nFor example:\n\nIn a customer churn prediction model, raw transactional data might be transformed into features like “average purchase frequency” or “days since last purchase” to better capture customer behavior.\nIn a fraud detection system, transaction logs might be engineered into features such as “average transaction size during peak hours” or “number of transactions per location.”\n\nThrough such transformations, feature engineering ensures that the model focuses on the most relevant aspects of the data, improving accuracy, interpretability, and robustness.\n\n\nHow It Varies by Data and Algorithms\nThe approach to feature engineering depends significantly on the type of data being used and the algorithms employed. Different data structures and machine learning models require tailored techniques to maximize their effectiveness.\n\n\n\n\n\n\nStructured Data\n\n\n\n\n\nIn datasets with a tabular format, common techniques include:\n\nAggregations: Summarizing data through metrics like mean, median, or count (e.g., average monthly spending).\nScaling: Normalizing numerical values to ensure features are on a comparable scale, especially for distance-based models like k-nearest neighbors.\nEncoding Categorical Variables: Converting non-numeric categories into numerical representations using methods like one-hot encoding or label encoding.\n\n\n\n\n\n\n\n\n\n\nText Data\n\n\n\n\n\nUnstructured text requires specialized transformations to capture linguistic patterns and meaning:\n\nTokenization: Splitting text into smaller units such as words or phrases.\nEmbeddings: Using techniques like word2vec or BERT to map text into dense vector representations that encode semantic meaning.\nSentiment Scoring: Assigning scores to text to quantify sentiment (e.g., positive, neutral, negative) for tasks like opinion analysis.\n\n\n\n\n\n\n\n\n\n\nTime-Series Data\n\n\n\n\n\nSequential data often benefits from transformations that capture temporal relationships:\n\nLag Features: Including past values of a variable to predict future trends.\nRolling Averages: Smoothing fluctuations by averaging values over defined time windows.\nFourier Transformations: Extracting cyclical patterns from periodic data.\n\n\n\n\n\n\n\n\n\n\nAlgorithm-Specific Considerations\n\n\n\n\n\n\nTree-Based Models (e.g., Random Forests, XGBoost): These models handle raw and categorical data well, requiring minimal preprocessing but can benefit from carefully engineered aggregations or interaction terms.\nNeural Networks: These models excel with raw data when provided in a suitable format, such as embeddings for text or image pixels. However, they require data to be vectorized, normalized, and often augmented for robust learning.\n\n\n\n\nThe specifics of feature engineering techniques vary widely across domains and are outside the detailed focus of this book. However, it is crucial to recognize that the final output of feature engineering should structure data in a format suitable for model consumption. Depending on the use case, this could mean:\n\nTabular Data: A matrix where rows represent samples and columns represent features.\nSequential Data: Structured sequences (e.g., time-series or text) prepared for models like recurrent neural networks.\nVectorized Data: Dense or sparse vectors that encode the features, suitable for algorithms like support vector machines or neural networks.\n\n\n\n\n\n\n\nFeature engineering resources\n\n\n\n\n\nTo delve deeper into specific feature engineering practices, the following books and resources are highly recommended:\n\nFeature Engineering for Machine Learning: Principles and Techniques for Data Scientists (Zheng and Casari 2018): This book provides a comprehensive overview of feature engineering techniques across different data types, with practical advice and examples.\nFeature Engineering and Selection: A Practical Approach for Predictive Models (Kuhn and Johnson 2019): This resource delves into advanced feature selection methods and preprocessing techniques, with a focus on model performance improvement.\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Géron 2022): Chapter 2 of this book covers essential data preprocessing and feature engineering steps with Python code examples.\nOnline Tutorials and Blogs:\n\nKaggle’s Feature Engineering Course: A free course offering hands-on practice with feature engineering in real-world datasets.\nTowards Data Science Feature Engineering Articles: A collection of articles covering practical and innovative feature engineering techniques.\n\nPapers with Code:\n\nFeature Engineering Papers: A repository linking research papers on feature engineering to practical code implementations.\n\n\nBy exploring these resources, practitioners can gain a deeper understanding of feature engineering techniques tailored to their specific data types and algorithms, ultimately building more effective machine learning systems.\n\n\n\n\n\n\nData Leakage\nData leakage occurs when information that would not be available during real-world inference inadvertently influences a machine learning model during training. This typically happens when features derived from the target variable or post-event information are included in the training data, leading to artificially inflated performance metrics.\n\n\n\n\n\n\nExamples of data leakage\n\n\n\n\n\n\nIncluding Future Information in Training Data\n\nScenario: A fraud detection model is designed to predict whether a transaction is fraudulent. The training dataset includes a feature indicating whether the transaction was flagged as fraud by manual review.\nWhy It’s Data Leakage: The feature directly correlates with the target variable but would not be available during real-time prediction. Including it inflates the model’s performance during training.\nImpact in MLOps: In production, the absence of this feature would cause the model’s performance to drop significantly, leading to incorrect fraud predictions and operational inefficiencies.\n\nImproper Data Splitting in Time-Series Analysis\n\nScenario: A time-series model is trained to predict stock prices based on historical data. The dataset is split randomly into training and test sets without considering the temporal sequence.\nWhy It’s Data Leakage: Future stock prices (from the test set) could influence training, as the random split allows information from the future to inform past predictions.\nImpact in MLOps: When deployed in production, the model fails to replicate its test performance because it can no longer rely on future data. This leads to inaccurate forecasting and potential financial losses.\n\nUsing Post-Event Data in Customer Churn Prediction\n\nScenario: A customer churn prediction model includes features like “number of customer support calls in the last month.” However, this feature is calculated retrospectively after the customer has already decided to leave.\nWhy It’s Data Leakage: This feature is only available after the target event (churn) has occurred and would not be accessible during inference for active customers.\nImpact in MLOps: During deployment, the model’s predictions are unreliable because it cannot use post-event data, resulting in flawed retention strategies and wasted resources.\n\n\n\n\n\n\nWhy Data Leakage is a Critical Issue in MLOps\nIn MLOps, where the goal is to create scalable, reliable, and production-ready machine learning systems, data leakage poses significant challenges:\n\nDecreased Model Generalizability: Models trained with leaked data perform exceptionally well during testing but fail to generalize to unseen production data, undermining their real-world utility. In MLOps, this can result in costly failures when the system is deployed.\nWasted Resources: The entire MLOps lifecycle—from data collection and preprocessing to model deployment and monitoring—is resource-intensive. Data leakage can render these efforts futile, as models with leakage often need to be retrained from scratch, incurring additional time and costs.\nErosion of Trust: A key aspect of MLOps is building trust in machine learning systems among stakeholders. Data leakage creates inflated performance metrics that can lead to overconfidence in the model’s capabilities, only to have those expectations shattered in production.\nCompromised Monitoring and Feedback Loops: In MLOps, monitoring is crucial to detect issues like drift and degradation. A model trained with leakage may produce unreliable predictions in production, complicating efforts to establish effective monitoring and feedback loops.\n\n\n\nHow to Prevent Data Leakage in MLOps\nTo build reliable and scalable ML systems in an MLOps workflow, rigorous practices are required to prevent data leakage:\n\nDataset Isolation: In an MLOps pipeline, ensure strict separation of training, validation, and test datasets. Automation tools should enforce this separation to prevent accidental overlap during preprocessing or feature generation.\nPipeline Validation: Validate the entire data pipeline to ensure that no future information is incorporated into the training process. This includes ensuring that scaling parameters (e.g., mean, standard deviation) or imputation values are calculated exclusively on training data.\nFeature Engineering Audits: Conduct regular audits of features to detect and remove any that are derived from the target variable or contain post-event information. Tools like feature importance scores or explainability frameworks can help identify problematic features.\nVersioning and Traceability: Use data versioning tools (e.g., DVC) to track changes in datasets and features over time. In MLOps, this ensures that any unintended leakage introduced during pipeline updates can be identified and corrected.\nContinuous Monitoring: In production, monitor model performance closely to detect signs of data leakage, such as an unexpected drop in accuracy or a sharp divergence between training and production metrics. Early detection can mitigate the impact and guide retraining efforts.\n\nIn an MLOps framework, preventing data leakage is not just a best practice but a foundational requirement for delivering reliable and maintainable machine learning systems. Leakage not only compromises the accuracy and robustness of models but also disrupts the iterative workflows that are central to MLOps, such as continuous integration, deployment, and monitoring. By prioritizing robust data handling practices and pipeline validations, MLOps teams can ensure that their models are not only performant but also trustworthy and sustainable in production environments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-validation",
    "href": "03-dataops-role.html#data-validation",
    "title": "3  The Role of DataOps",
    "section": "3.3 Data Validation",
    "text": "3.3 Data Validation\nData validation is a critical step in the DataOps process that ensures datasets used in machine learning systems are accurate, complete, and reliable. In the context of machine learning, the quality of data directly impacts the performance and generalizability of models, making validation an essential safeguard against flawed results and operational inefficiencies. By systematically checking for errors, inconsistencies, and missing values, data validation helps to identify potential issues before they propagate through the machine learning pipeline, ultimately saving time and resources. It also fosters trust in the data by ensuring that all inputs meet predefined quality standards, which is particularly important when integrating data from multiple sources.\nData validation is not a one-time task but an ongoing process that spans the entire lifecycle of a machine learning system. From data ingestion to feature engineering and model deployment, validation plays a crucial role in detecting anomalies, ensuring consistency across datasets, and maintaining the relevance of the data. By embedding validation into DataOps workflows, organizations can create a foundation for scalable and dependable machine learning systems.\n\n\n\n\n\n\nKey Objectives of Data Validation\n\n\n\nThe primary objectives of data validation are to ensure the dataset’s integrity and usability while aligning it with the needs of the machine learning system:\n\nAccuracy: Verifying that data values are correct and reflect real-world scenarios. For instance, ensuring customer birthdates are valid and consistent with age calculations avoids errors during feature engineering.\nCompleteness: Checking that no critical data is missing or incomplete. For example, ensuring that every transaction in a sales dataset includes both the product ID and quantity sold prevents downstream issues in revenue calculations.\nConsistency: Ensuring that data formats, units, and structures are uniform across the dataset. For example, standardizing currency formats (e.g., USD) in global sales data helps avoid misinterpretation during analysis.\nUniqueness: Identifying and removing duplicate records to prevent data redundancy and biased model training. For instance, deduplicating customer profiles ensures accurate customer segmentation.\nTimeliness: Ensuring data is up-to-date and relevant for the task at hand. For real-time applications, such as fraud detection, validating the timeliness of incoming transaction data is essential for reliable predictions.\n\nBy focusing on these objectives, data validation ensures that machine learning systems are built on a foundation of high-quality, dependable data, reducing the likelihood of errors and enhancing overall performance.\n\n\n\nCommon Types of Data Validation\nData validation encompasses a range of checks and processes designed to ensure the quality, consistency, and reliability of datasets used in machine learning workflows. Each type of validation addresses specific aspects of data quality, ensuring that the dataset meets the requirements for accurate analysis and modeling. Below are the most common types of data validation and their roles in maintaining data integrity.\n\n\n\n\n\n\nSchema Validation\n\n\n\n\n\nSchema validation ensures that the dataset adheres to its predefined structure, including data types, column names, and value constraints. This is often the first step in data validation, as a mismatched schema can cause downstream processes to fail.\n\nExample: A dataset intended for customer analytics might require columns like CustomerID (integer), PurchaseDate (date), and AmountSpent (float). If CustomerID contains strings or PurchaseDate has invalid date formats, schema validation would flag these issues.\nWhy It’s Important: Schema validation ensures compatibility between datasets and the systems that consume them, preventing runtime errors during data processing and analysis.\n\n\n\n\n\n\n\n\n\n\nContent Validation\n\n\n\n\n\nContent validation focuses on the correctness of individual data values within the dataset. This includes verifying that values fall within expected ranges, follow required formats, or belong to valid categories.\n\nExample: In a demographic dataset, the Age column might be expected to contain values between 0 and 120. If a record includes a value of 200, content validation would identify it as an error.\nWhy It’s Important: Incorrect values can skew model training and lead to flawed insights. Content validation ensures that the dataset reflects realistic and meaningful information.\n\n\n\n\n\n\n\n\n\n\nCross-Field Validation\n\n\n\n\n\nCross-field validation examines the relationships between fields in a dataset to ensure logical consistency. It ensures that data points align with real-world constraints and dependencies.\n\nExample: In a dataset for employee records, the HireDate field should always precede the TerminationDate. Cross-field validation would flag any records where this condition is violated.\nWhy It’s Important: Logical inconsistencies can introduce biases or errors into models and analytics, reducing the reliability of results.\n\n\n\n\n\n\n\n\n\n\nCross-Dataset Validation\n\n\n\n\n\nCross-dataset validation ensures consistency and compatibility when integrating multiple datasets. It checks for alignment in shared fields, relationships, or identifiers across datasets.\n\nExample: When joining a sales dataset with a customer dataset, cross-dataset validation might confirm that all CustomerID values in the sales data exist in the customer data. Missing CustomerID values could indicate errors in data collection or integration.\nWhy It’s Important: Discrepancies between datasets can disrupt data pipelines and lead to incomplete or erroneous analyses.\n\n\n\n\n\n\n\n\n\n\nStatistical Validation\n\n\n\n\n\nStatistical validation uses descriptive statistics and patterns to identify anomalies or unexpected distributions in the data. This type of validation is especially useful for large datasets where manual checks are impractical.\n\nExample: In a dataset of daily sales, statistical validation might flag a sudden spike in sales for a particular day as a potential anomaly. This could indicate a data entry error or an event that requires further investigation.\nWhy It’s Important: Statistical anomalies can distort machine learning models and insights. Validation ensures that outliers or shifts in distributions are identified and addressed appropriately.\n\n\n\n\nBy incorporating these types of validation into the data pipeline, organizations can detect and resolve issues early, ensuring that datasets are clean, consistent, and ready for machine learning workflows. Each type of validation contributes to a robust foundation for building reliable and scalable ML systems.\n\n\nBest Practices for Data Validation\nEffective data validation is fundamental to building reliable machine learning systems. By leveraging domain expertise, incorporating validation at multiple stages, and implementing continuous checks, teams can address data quality issues proactively and ensure the success of their pipelines. Below are some key practices for robust data validation.\n\n\n\n\n\n\nLeverage Domain Expertise\n\n\n\n\n\nDomain knowledge is vital for designing meaningful validation checks that align with the specific context and operational needs of the data. While statistical methods and automated tools can identify obvious anomalies, domain experts provide critical insights into patterns, constraints, and acceptable ranges that are unique to the field. Their input ensures that validation rules capture the intricacies of the data and its intended use.\n\nDeep Contextual Understanding: Domain experts can identify subtleties that automated tools might miss. For example, a recorded blood glucose level of 10 mg/dL in a healthcare dataset might technically fall within a predefined range but would raise a red flag for medical professionals as a near-impossible value for a living patient.\nDefining Logical Relationships: Experts help validate cross-field dependencies, such as ensuring that “Hire Date” is always earlier than “Termination Date” in HR records or that “Loan Amount” does not exceed “Annual Income” in financial datasets.\nResolving Ambiguities: Inconsistent categorizations or missing metadata can lead to misinterpretations. Domain expertise helps reconcile these discrepancies by providing the real-world context necessary for making informed decisions.\nBest Practices: To incorporate domain knowledge effectively, schedule regular reviews with subject-matter experts, document domain-specific rules, and use their feedback to design validation logic.\n\nBy leveraging domain expertise, teams can ensure that validation processes are aligned with real-world expectations, reducing the risk of subtle but impactful errors that could undermine model performance.\n\n\n\n\n\n\n\n\n\nIncorporate Validation at Multiple Stages\n\n\n\n\n\nData validation should not be confined to a single step in the pipeline. Instead, it must be integrated at critical points to catch and correct errors as early as possible, minimizing their impact on downstream processes. A layered validation approach ensures that issues are addressed before they compound, saving time and resources.\n\nDuring Data Ingestion: Validate data as it enters the system to catch schema mismatches, missing fields, or invalid formats. Early detection ensures that errors don’t propagate to later stages of the pipeline.\n\nExample: A customer database ingesting data from an API might validate that all required fields (e.g., CustomerID, Email, SignUpDate) are present and correctly formatted. Missing or improperly formatted data would trigger an alert and block the ingestion process.\nWhy It’s Critical: Catching errors at this stage prevents bad data from being stored, processed, or analyzed, reducing the risk of cascading issues.\n\nBefore and After Data Transformations: Transformation processes, such as scaling, encoding, and feature extraction, can inadvertently introduce inconsistencies or errors. Validating data before and after these steps ensures the integrity of transformations.\n\nExample: After encoding categorical variables into numeric values, validate that all categories have been properly mapped and no invalid codes remain.\nWhy It’s Critical: Errors during transformation can mislead models or invalidate assumptions about the data, compromising model performance.\n\nPrior to Model Training or Inference: Perform comprehensive checks to confirm that the final dataset is clean, complete, and aligned with model requirements. Validate feature distributions, detect outliers, and ensure proper dataset splits to avoid data leakage.\n\nExample: Verify that feature distributions in the training and test sets are consistent. Large discrepancies might indicate data drift or improper splitting.\nWhy It’s Critical: Ensuring that the dataset aligns with model expectations prevents training on flawed or unrepresentative data, improving generalizability and performance.\n\n\nIntegrating validation at multiple stages creates a layered defense against data issues, ensuring that problems are addressed as they arise and minimizing downstream disruptions.\n\n\n\nIncorporate Validation at Multiple Stages\n\n\n\n\n\n\n\n\n\n\n\nImplement Continuous Validation for Real-Time Pipelines\n\n\n\n\n\nReal-time or streaming data pipelines introduce unique challenges, as data flows continuously and must be validated on the fly. Continuous validation ensures that data quality standards are upheld in dynamic environments, enabling reliable predictions and analysis even under high-velocity conditions.\n\nAutomated Validation Checks: Implement automated rules that validate data in real time, such as ensuring fields are populated, values fall within expected ranges, and timestamps are sequential.\n\nExample: For a weather forecasting model ingesting sensor data, validate that temperature readings are within a plausible range (-50°C to 60°C) and that timestamps are consistently ordered.\nWhy It’s Critical: Automated checks prevent bad data from entering the pipeline, reducing the risk of unreliable predictions or system failures.\n\nHandling Anomalies: Set up alert mechanisms to flag anomalous data points for further investigation or correction. For critical systems, design fallback processes to handle invalid data gracefully without disrupting operations.\n\nExample: In a fraud detection system, flagging a transaction with a missing TransactionID or an implausible transaction amount (e.g., $0 or $10 million) allows operators to investigate the issue without halting the pipeline.\nWhy It’s Critical: Continuous monitoring and anomaly detection ensure that issues are addressed promptly, maintaining the integrity of real-time workflows.\n\nIntegration with Monitoring Systems: Real-time pipelines should be integrated with monitoring tools to track key data quality metrics (e.g., percentage of missing fields, frequency of anomalies) and identify trends over time.\n\nExample: In a recommendation system for an e-commerce platform, monitor the consistency of product metadata to ensure that invalid records (e.g., missing prices or categories) are flagged and corrected automatically.\nWhy It’s Critical: Ongoing monitoring helps detect patterns of degradation or drift, enabling proactive maintenance and adjustments.\n\n\nContinuous validation ensures that high-velocity data pipelines remain reliable and robust, supporting the dynamic needs of real-time machine learning systems.\n\n\n\nBy leveraging domain expertise, validating data at multiple stages, and implementing continuous checks for real-time pipelines, teams can build resilient workflows that maintain data quality throughout the lifecycle. These best practices minimize errors, enhance trust in the data, and support the development of scalable and effective machine learning systems.\n\n\nCommon Challenges in Data Validation\nDespite its importance, data validation can be a complex and resource-intensive process, especially in large-scale machine learning workflows. Organizations face several challenges when implementing effective validation practices, from evolving data structures to managing high-velocity pipelines. Addressing these challenges requires a combination of strategic planning, automation, and collaboration across teams. Below are some of the most common challenges in data validation and strategies to mitigate them.\n\n\n\n\n\n\nHandling Evolving Schemas and Data Structures\n\n\n\n\n\nAs systems grow and data sources evolve, schemas and data structures often change over time. These changes can introduce unexpected errors if validation rules and downstream processes are not updated accordingly.\n\nChallenge: Changes in schema, such as adding or removing fields, altering data types, or modifying field names, can break validation checks and disrupt downstream workflows. For example, if a new column is added to a dataset without updating validation rules, it may be ignored or processed incorrectly.\nImpact: Unmanaged schema changes can lead to inconsistent or incomplete data, causing model degradation or failures in production.\nMitigation: Implement schema versioning to track and manage changes, and automate schema validation to identify discrepancies. Regular communication between data producers and consumers ensures alignment on schema updates.\n\n\n\n\n\n\n\n\n\n\nBalancing Thoroughness with Performance in Large-Scale Datasets\n\n\n\n\n\nIn large-scale datasets, exhaustive validation can be computationally expensive and time-consuming, particularly when dealing with real-time or high-volume data.\n\nChallenge: Performing detailed checks on every record in massive datasets can slow down data pipelines, increasing latency and computational costs. For instance, validating each field in a petabyte-scale dataset may be impractical without specialized infrastructure.\nImpact: Sacrificing thoroughness to maintain performance can result in undetected data quality issues, while overly thorough checks can delay time-sensitive processes.\nMitigation: Use sampling strategies to validate subsets of data for preliminary checks, and focus on high-priority fields or anomalies. Distributed processing frameworks, such as Apache Spark, can also help scale validation processes efficiently.\n\n\n\n\n\n\n\n\n\n\nManaging Validation Failures in Dynamic or High-Velocity Pipelines\n\n\n\n\n\nIn dynamic or real-time pipelines, validation failures must be handled quickly and effectively to avoid disrupting the flow of data.\n\nChallenge: High-velocity data streams generate large volumes of data that may fail validation checks due to schema mismatches, incomplete records, or anomalies. For example, a streaming pipeline for e-commerce transactions may receive invalid records during peak traffic.\nImpact: If validation failures are not managed appropriately, they can halt data ingestion, create bottlenecks, or allow poor-quality data to pass through unchecked.\nMitigation: Implement automated alerting and fallback mechanisms to handle validation failures gracefully. For instance, redirect invalid records to a quarantine or error-handling pipeline for further investigation while allowing valid data to continue flowing. Real-time dashboards can help monitor validation metrics and identify trends.\n\n\n\n\n\n\n\n\n\n\nAddressing Inconsistencies Across Multiple Data Sources\n\n\n\n\n\nIntegrating data from multiple sources often introduces inconsistencies, such as differing formats, units, or categorizations. These discrepancies can complicate validation and lead to unreliable datasets.\n\nChallenge: Datasets from disparate sources may lack standardization. For example, one dataset may record prices in USD, while another uses EUR without indicating the currency, making comparisons or aggregations problematic.\nImpact: Inconsistent data can lead to incorrect analyses, skewed model training, or misinterpretations of results.\nMitigation: Establish and enforce data standardization rules, such as using consistent formats, units, and categorizations across all sources. Employ automated data cleaning and reconciliation processes to resolve discrepancies. Collaboration with domain experts and data owners is crucial to defining consistent standards.\n\n\n\n\nData validation is a critical but challenging component of machine learning workflows. By proactively addressing issues such as evolving schemas, performance trade-offs, validation failures, and inconsistencies across sources, teams can build resilient pipelines that support robust and reliable ML systems. Tackling these challenges requires a combination of strategic planning, automation, and collaboration to ensure that data quality remains a cornerstone of the machine learning lifecycle.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#data-versioning-and-lineage",
    "href": "03-dataops-role.html#data-versioning-and-lineage",
    "title": "3  The Role of DataOps",
    "section": "3.4 Data Versioning and Lineage",
    "text": "3.4 Data Versioning and Lineage\nIn the world of machine learning and data-driven workflows, the reliability and traceability of data are critical. Data versioning and lineage are two foundational practices in DataOps that ensure datasets are consistently managed, reproducible, and transparent throughout their lifecycle. These practices not only empower teams to track changes, debug issues, and ensure compliance but also play a crucial role in fostering trust in the data used to build machine learning systems.\n\n\n\n\n\n\nData versioning focuses on capturing and maintaining historical versions of datasets. By recording each change, teams can roll back to previous versions, compare outcomes across iterations, and confidently experiment with new approaches.\n\n\n\n\n\n\n%%{init: {'gitGraph': {'mainBranchName': 'CustomerTransactions'}} }%%\n\ngitGraph\n   commit  id: \"v1.0.0\"\n   commit  id: \"v1.0.1\"\n   commit  id: \"v1.0.2\"\n   branch new_feature\n   commit id: \"add new feature\"\n   checkout CustomerTransactions\n   merge new_feature\n   commit  id: \"v1.1.0\"\n\n\n\n\n\nFigure 3.5: Data versioning tracks historical versions of a given dataset. This makes it explicit as to which instance of the dataset was used for a given model (i.e. v1.1.0) and also allows rolling back to previous versions (i.e. v1.0.2) if necessary.\n\n\n\n\n\nData lineage, on the other hand, documents the entire journey of data—from its origin to the final output—capturing how it was processed, transformed, and consumed. This traceability is essential for debugging, auditing, and understanding the impact of upstream changes on downstream processes.\n\n\n\n\n\n\nflowchart LR\n  db1[(Data&lt;br&gt;source 1)] --&gt; p1[/processing&lt;br&gt;step 1/]\n  p1 --&gt;  p2[/processing&lt;br&gt;step 2/]\n  db2[(Data&lt;br&gt;source 2)] --&gt; p2\n  p2 --&gt;  p3[/processing&lt;br&gt;step 3/]\n  db3[(Data&lt;br&gt;source 3)] --&gt; p3\n  p3 --&gt; p4[/processing&lt;br&gt;step 4/]\n  p4 --&gt; p5[/processing&lt;br&gt;step 5/]\n  p5 --&gt; db4[(Final&lt;br&gt;processed&lt;br&gt;data)]\n\n\n\n\n\nFigure 3.6: Data lineage tracks the movement and transformations of data from its original source(s) to its destination, including how the data was changed and why.\n\n\n\n\n\n\n\n\nTogether, data versioning and lineage create a transparent, reliable foundation for machine learning systems. They enable teams to meet regulatory requirements, mitigate risks, and streamline collaboration, making these practices indispensable in modern DataOps workflows.\n\nKey Use Cases for Data Versioning and Lineage\nData versioning and lineage are indispensable practices for ensuring reliability, transparency, and traceability in machine learning workflows. They address critical needs at various stages of the data lifecycle, from dataset updates to model deployment and auditing. Below provides an expanded exploration of their key use cases, highlighting their value in real-world applications.\n\n\n\n\n\n\nDataset Updates and Historical Record-Keeping\n\n\n\n\n\nDatasets are rarely static; they evolve over time as new data is added, errors are corrected, or data sources change. Versioning provides a structured approach to managing these updates, ensuring that historical records remain accessible for analysis and comparison.\n\nExample: In a healthcare application predicting patient readmissions, new patient data is added daily. Data versioning ensures that older datasets used for model training can be preserved, allowing teams to analyze performance trends and validate the impact of new data on the model.\nImpact: Maintaining historical datasets enables teams to trace the effects of data updates, ensure reproducibility, and mitigate risks associated with data drift.\n\n\n\n\n\n\n\n\n\n\nSupporting Experimentation in Machine Learning Workflows\n\n\n\n\n\nMachine learning involves iterative experimentation, where teams test different preprocessing techniques, feature engineering approaches, or model architectures. Data versioning and lineage allow for systematic tracking of these experiments, ensuring that changes to datasets and transformations are documented.\n\nExample: A retail company experimenting with customer segmentation might preprocess transactional data in multiple ways (e.g., normalizing vs. scaling numerical features). Versioning ensures each iteration is preserved, enabling direct comparison of model performance across different dataset versions.\nImpact: Experimentation becomes more controlled and efficient, as teams can revert to previous iterations, identify successful preprocessing pipelines, and replicate results with confidence.\n\n\n\n\n\n\n\n\n\n\nDebugging and Error Resolution\n\n\n\n\n\nErrors in data pipelines, such as incorrect preprocessing or missing records, can disrupt downstream processes and compromise model performance. Data lineage provides a detailed trace of the data journey, helping teams identify and resolve issues efficiently.\n\nExample: In a real-time fraud detection system, an unexpected spike in false positives might be traced back to an error during data aggregation. Lineage tools can pinpoint the exact step where the error occurred, enabling a targeted fix.\nImpact: By tracing data through every stage of the pipeline, teams can quickly isolate and correct issues, minimizing downtime and preventing future errors.\n\n\n\n\n\n\n\n\n\n\nEnsuring Regulatory Compliance and Auditability\n\n\n\n\n\nIndustries like healthcare, finance, and government require strict adherence to data governance and regulatory standards. Data versioning and lineage provide the transparency needed to demonstrate compliance during audits and ensure accountability.\n\nExample: A financial institution subject to GDPR must document how customer data is processed and used in predictive models. Lineage tools enable the institution to show the origin, transformations, and final usage of the data, meeting regulatory requirements.\nImpact: Compliance becomes more manageable, reducing the risk of fines or legal repercussions and building trust with stakeholders.\n\n\n\n\n\n\n\n\n\n\nManaging Data Across Multiple Teams and Workflows\n\n\n\n\n\nIn collaborative environments, multiple teams often work with the same datasets for different purposes, such as training, testing, and reporting. Data versioning ensures that all teams access consistent and accurate data, while lineage provides visibility into data dependencies and transformations.\n\nExample: In a global logistics company, the analytics team uses shipment data for demand forecasting, while the operations team uses the same data for route optimization. Versioning ensures that both teams work with the same baseline dataset, while lineage reveals how each team’s transformations affect their workflows.\nImpact: Collaboration becomes smoother, with reduced risks of duplication, misalignment, or conflicting results.\n\n\n\n\n\n\n\n\n\n\nMonitoring Data Quality and Detecting Drift\n\n\n\n\n\nOver time, data distributions may change due to external factors, introducing data drift that can degrade model performance. Lineage tools can help monitor data quality and track changes in data characteristics, enabling proactive responses.\n\nExample: A recommendation system for an online retailer detects a drop in click-through rates. Lineage tracing reveals a shift in customer behavior, such as increased searches for eco-friendly products. Versioning allows the team to retrain the model using updated data reflecting the new trends.\nImpact: Data drift is detected early, allowing teams to maintain model accuracy and relevance.\n\n\n\n\n\n\n\n\n\n\nReproducing Results and Facilitating Knowledge Sharing\n\n\n\n\n\nReproducibility is critical for both research and production environments. Data versioning ensures that datasets used in experiments or deployed models are preserved, enabling other teams to replicate and validate results.\n\nExample: In a pharmaceutical study, a machine learning model identifies potential drug interactions. Preserving the exact dataset and transformations used for the analysis allows external researchers to reproduce the findings and verify their validity.\nImpact: Knowledge sharing and collaboration improve, fostering trust in the results and enabling broader contributions to the project.\n\n\n\n\nBy implementing robust data versioning and lineage practices, organizations can enhance the transparency, reliability, and efficiency of their machine learning workflows. These practices not only address critical challenges like compliance and debugging but also empower teams to innovate confidently and collaboratively in an increasingly data-driven world.\n\n\nKey Components of Data Versioning and Lineage\nData versioning and lineage provide mechanisms for tracking changes, ensuring reproducibility, and offering transparency into the lifecycle of data. The following are the key components that underpin effective data versioning and lineage practices.\n\n\n\n\n\n\nData Versioning\n\n\n\n\n\nData versioning ensures that all changes to datasets are tracked and preserved, enabling teams to maintain a historical record of their data. This practice supports reproducibility, experimentation, and error recovery.\n\nMetadata Tagging for Datasets: Each dataset version should include metadata that captures important details such as timestamps, source information, preprocessing steps, and associated models. For example, tagging a dataset with its creation date, source system, and transformation details helps identify its role in various workflows.\nStoring Snapshots of Datasets: Versioning involves creating and storing immutable snapshots of datasets at specific points in time. These snapshots act as checkpoints that teams can use for comparison, debugging, or compliance purposes. For instance, retaining snapshots of customer transaction data before and after preprocessing ensures the original dataset is preserved for future reference.\nComparison for Changes: Versioning tools should allow for easy comparison of different dataset versions. For example, identifying changes in schema, added records, or updated values between two versions enables teams to analyze their impact on downstream processes or models.\n\n\n\n\n\n\n\n\n\n\nData Lineage\n\n\n\n\n\nData lineage focuses on documenting the entire journey of a dataset, from its origin to its final use. It provides insights into how data flows through pipelines and ensures traceability for debugging, auditing, and compliance.\n\nCapturing Source Information: Lineage begins with documenting where the data originated, including details like the source system, ingestion method, and timestamp. For example, knowing that a sales dataset came from a specific API on a given date ensures transparency and traceability.\nLogging Transformations Applied to the Data: Every transformation applied to a dataset—such as cleaning, normalization, or aggregation—should be recorded in detail. For instance, lineage tracking might reveal that raw sales data was aggregated by region and quarter before being used for demand forecasting.\nTracking Data Dependencies Across Workflows: Data lineage identifies relationships between datasets and workflows, enabling teams to understand how changes in one component affect others. For example, if a change occurs in the schema of a raw dataset, lineage tracking can reveal all downstream models, reports, or dashboards that rely on it.\n\n\n\n\nIn addition to capturing these key components, a few best practices to consider for data versioning and lineage include:\n\n\n\n\n\n\nIntegration with Workflow Automation\n\n\n\n\n\nData versioning and lineage should seamlessly integrate into automated workflows to ensure continuous tracking and real-time updates.\n\nPipeline Integration: Versioning and lineage tools must be embedded within data pipelines to automatically capture changes as data flows through ingestion, transformation, and modeling stages. For instance, a pipeline processing IoT sensor data should log every step, from ingestion to normalization.\nChange Detection and Alerts: Automated systems should monitor for changes to datasets or pipelines and notify teams when discrepancies arise. For example, an alert might be triggered if a dataset’s schema changes unexpectedly, allowing for quick troubleshooting.\n\n\n\n\n\n\n\n\n\n\nCentralized Management and Visualization\n\n\n\n\n\nCentralized storage and visual tools are essential for accessing and understanding versioning and lineage information.\n\nCentralized Repositories: Datasets and lineage metadata should be stored in a centralized system, such as a data lake or a dedicated tool like DVC or LakeFS. Centralized storage ensures consistency and prevents duplication, making it easier for teams to collaborate and retrieve historical data.\nVisual Representations: Tools that provide graphical representations of lineage offer an intuitive way to understand data flows and dependencies. For example, a visual lineage graph might display how raw survey data is transformed and aggregated into a final dataset used for customer sentiment analysis, highlighting dependencies and transformation steps.\n\n\n\n\n\n\n\n\n\n\nStandardize Naming and Storage\n\n\n\n\n\nEstablishing consistent naming conventions and centralized storage for datasets ensures clarity and accessibility across teams.\n\nWhy It Matters: Inconsistent naming or scattered storage can create confusion and hinder collaboration. Standardized practices make it easier to locate and identify datasets, reducing errors and miscommunication.\nHow to Implement: Use clear, descriptive names for datasets, such as including the dataset’s purpose, version, and timestamp in its name (e.g., customer_transactions_v1_2024-12-01). Store datasets in a centralized location, like a data lake or cloud repository, to ensure all team members have access to the latest and historical versions.\n\n\n\n\n\n\n\n\n\n\nAlign with Business and Regulatory Needs\n\n\n\n\n\nTailoring versioning and lineage practices to align with business objectives and regulatory requirements ensures their relevance and value.\n\nWhy It Matters: Different industries have unique compliance standards, such as GDPR in the EU or HIPAA in healthcare. Aligning data practices with these requirements reduces legal risks and ensures that systems meet stakeholder expectations.\nHow to Implement: Identify key compliance and business goals early in the process. For example, ensure lineage tools capture data provenance details to meet audit requirements, or design versioning practices that align with business-critical milestones, such as product launches or quarterly reporting.\n\n\n\n\n\n\nChallenges in Implementing Data Versioning and Lineage\nWhile data versioning and lineage are essential for maintaining reliable and transparent machine learning workflows, their implementation comes with significant challenges. These difficulties often stem from the complexity of data ecosystems, evolving requirements, and resource constraints. Below are key challenges and their implications. You’ll notice that many of these are the same challenges identified in the Data Validation section.\n\n\n\n\n\n\nHandling Evolving Schemas and Data Structures\n\n\n\n\n\nData schemas and structures frequently change due to updates in source systems, new business requirements, or changes in data pipelines. These changes can disrupt existing workflows and complicate versioning and lineage tracking.\n\nWhy It Matters: Schema changes can break downstream processes or make older dataset versions incompatible with updated systems, hindering reproducibility and operational efficiency.\nExample: An e-commerce platform might update its customer transaction schema by adding new fields (e.g., “discount code”) or renaming existing ones, causing discrepancies in the lineage and breaking versioned workflows.\nHow to Address: Implement schema evolution strategies, such as backward compatibility checks and automated schema validation tools, to accommodate changes without disrupting existing systems.\n\n\n\n\n\n\n\n\n\n\nBalancing Thoroughness with Performance in Large-Scale Datasets\n\n\n\n\n\nMaintaining detailed versioning and lineage information for massive datasets can strain storage and computational resources. Tracking every transformation and storing multiple dataset versions may lead to high costs and performance bottlenecks.\n\nWhy It Matters: Overhead from excessive storage or processing requirements can slow down workflows, making real-time or near-real-time processing unfeasible for large-scale operations.\nExample: A streaming platform processing terabytes of user activity data daily may struggle to version every batch without incurring prohibitive storage costs or latency.\nHow to Address: Use compression techniques, snapshot deltas instead of full copies, and prioritize versioning critical datasets over ephemeral or intermediate data.\n\n\n\n\n\n\n\n\n\n\nManaging Validation Failures in Dynamic Data Pipelines\n\n\n\n\n\nIn dynamic or high-velocity data pipelines, validation failures or pipeline interruptions can lead to incomplete or inconsistent lineage and versioning records.\n\nWhy It Matters: Missing or incorrect lineage records compromise the ability to debug issues or trace the data journey, leading to a loss of confidence in the system’s reliability.\nExample: In a fraud detection system ingesting live transactional data, a sudden schema mismatch during ingestion might skip validation, resulting in incomplete lineage tracking.\nHow to Address: Integrate robust validation checks at every stage of the pipeline and implement fail-safe mechanisms, such as pausing downstream processes until issues are resolved.\n\n\n\n\n\n\n\n\n\n\nAddressing Inconsistencies Across Multiple Data Sources\n\n\n\n\n\nIntegrating data from diverse sources, each with its own standards and inconsistencies, poses challenges for maintaining unified versioning and lineage.\n\nWhy It Matters: Disparate data sources with varying levels of documentation, quality, and structure make it difficult to track dependencies and transformations comprehensively.\nExample: A global organization might source data from regional offices with inconsistent naming conventions, formats, or processing practices, complicating lineage tracking and dataset reconciliation.\nHow to Address: Standardize data ingestion processes and enforce consistent naming conventions, metadata tagging, and validation rules across all data sources.\n\n\n\n\n\n\n\n\n\n\nEnsuring Scalability with Growing Data Volumes\n\n\n\n\n\nAs data volumes increase, the complexity of managing versioning and lineage grows exponentially. Systems must scale to handle larger datasets without sacrificing performance or accuracy.\n\nWhy It Matters: Scaling challenges can result in gaps in lineage tracking or slow down pipelines, limiting the system’s ability to support evolving business needs.\nExample: A social media platform analyzing user interactions in real time may find its lineage tools unable to keep pace with the sheer volume of incoming data.\nHow to Address: Use distributed systems and cloud-based solutions to scale storage and processing capabilities dynamically. Focus lineage tracking on critical datasets to balance thoroughness with efficiency.\n\n\n\n\nImplementing data versioning and lineage requires navigating challenges like evolving schemas, large-scale datasets, dynamic pipelines, and diverse data sources. By anticipating these difficulties and adopting strategies such as schema evolution, robust validation checks, and scalable infrastructure, organizations can build resilient systems that ensure data integrity and traceability while meeting the demands of modern machine learning workflows.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#summary",
    "href": "03-dataops-role.html#summary",
    "title": "3  The Role of DataOps",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nThis chapter explored the foundational role of DataOps within the MLOps lifecycle, focusing on the key processes and principles that enable the efficient, reliable, and high-quality flow of data to support machine learning systems. From data ingestion to data validation, versioning, and lineage, we examined how these processes address common challenges in data management while ensuring the scalability and reliability of ML workflows.\nKey highlights included the importance of understanding data sources and their ingestion methods, the transformative role of data processing and feature engineering, and the critical need for robust data validation practices. We also delved into the benefits of data versioning and lineage, which enable traceability, reproducibility, and compliance in increasingly complex data ecosystems.\nWhile this chapter provided a conceptual and practical understanding of each DataOps process, the next chapter will shift focus to application. Readers will explore how these processes come together in an end-to-end DataOps pipeline, gaining hands-on insights into building and managing scalable, effective workflows. This transition from theory to practice is designed to solidify your understanding of DataOps and its critical role in delivering reliable and impactful machine learning systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#exercise",
    "href": "03-dataops-role.html#exercise",
    "title": "3  The Role of DataOps",
    "section": "3.6 Exercise",
    "text": "3.6 Exercise\nThis exercise is designed to engage you in a thought experiment that explores the practical applications of DataOps concepts. Each scenario below requires critical thinking and reflection on the principles discussed in this chapter. The scenarios are designed to be a bit vague so you can feel free to make certain assumptions regarding the scenario or even identify additional questions that need to be answered.\n\n\n\n\n\n\nPart 1: Data Ingestion Design\n\n\n\n\n\nImagine you are building an ML system for a ride-sharing company to predict rider demand in real-time.\nScenario Details:\n\nHistorical Trip Data: Stored in a relational database, this includes features like pickup/dropoff locations, trip durations, fares, and timestamps for trips over the past five years. Updates occur nightly.\nReal-Time Weather Updates: Obtained from a third-party API, the weather data includes temperature, precipitation, and wind speed, updated every 10 minutes. However, the API has a limit of 1000 requests per hour.\nDriver Availability Data: Streamed from an internal system, this data provides the location and status (available, occupied, offline) of drivers in real-time, updated every second.\n\nQuestions to Consider:\n\nFor each data source, would you choose batch, streaming, or hybrid ingestion? Justify your decision.\nWhat challenges might arise when combining these sources into a unified pipeline (e.g., mismatched data formats, latency)?\nIf the weather API experiences downtime or exceeds rate limits, how would you ensure the pipeline continues functioning without compromising predictions?\n\n\n\n\n\n\n\n\n\n\nPart 2: Data Validation Strategy\n\n\n\n\n\nYou are working on a fraud detection system for an e-commerce platform.\nScenario Details:\n\nTransactional Data: Includes payment amounts, timestamps, and location information. This data is streamed in real-time as customers complete purchases.\nCustomer Demographic Data: Stored in a central database, this data includes customer age, location, and preferred payment method. Updates are infrequent, typically occurring during customer registration or profile edits.\n\nQuestions to Consider:\n\nWhat specific validation checks would you implement for:\n\nReal-time transactional data (e.g., ensuring timestamps are sequential, payment amounts are positive)?\nCustomer demographic data (e.g., completeness of profiles, consistency between locations and timestamps)?\n\nWhere in the pipeline would you implement these checks to ensure high-quality data without introducing significant latency (i.e. during data ingestion, after data processing)?\nIn a scenario where real-time data includes unexpected values or incomplete fields, what fallback mechanisms would you put in place to avoid disruptions?\n\n\n\n\n\n\n\n\n\n\nPart 3: Data Versioning and Lineage in Practice\n\n\n\n\n\nYou are tasked with designing a churn prediction model for a subscription-based video streaming service.\nScenario Details:\n\nUser Activity Logs: Include data on session duration, content watched, and browsing patterns. Logs are collected continuously and stored in a data lake.\nSubscription Details: Include plan type, renewal dates, and payment status. This data is updated monthly and stored in a relational database.\nSupport Ticket Interactions: Contain customer issues, resolutions, and timestamps. These are logged as they occur, with potential delays in updates due to manual entry.\n\nQuestions to Consider:\n\nHow would you use data versioning to track changes in user activity logs, ensuring you can reproduce model training results even as new data is added?\nWhat specific lineage information would you prioritize capturing (e.g., transformations applied to raw activity logs, dependencies between features)?\nIf regulatory compliance (e.g., GDPR, which requires traceability of user data usage) is a factor, how might you adjust your versioning and lineage strategy?\n\n\n\n\n\n\n\n\n\n\nPart 4: Reflection on DataOps Challenges\n\n\n\n\n\nReflect on a hypothetical scenario where your ML pipeline for a financial institution fails in production.\nScenario (Failure) Details:\n\nA schema change in the credit card transaction dataset (e.g., renaming “customer_id” to “user_id”) was not communicated to the team.\nThe batch ingestion process for monthly transaction summaries included corrupted records due to network interruptions during transfer.\nReal-time streaming data from payment terminals introduced inconsistencies in timestamps due to system clock misalignments.\n\nQuestions to Consider:\n\nWhich specific DataOps practices (e.g., schema validation, data versioning, lineage tracking) could have prevented these failures?\nWhat steps could you take to proactively handle similar challenges in the future, such as designing validation workflows or automating schema change notifications?\nHow would you balance thorough validation checks with maintaining pipeline performance, especially in a high-throughput environment like financial fraud detection?\n\n\n\n\n\n\n\n\nGéron, Aurélien. 2022. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow. \" O’Reilly Media, Inc.\".\n\n\nHeckman, James. 2013. “Sample Selection Bias as a Specification Error.” Applied Econometrics 31 (3): 129–37.\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman; Hall/CRC.\n\n\nLerman, Rachel. February 3, 2016. “Google Is Testing Its Self-Driving Car in Kirkland,” February 3, 2016.\n\n\nZheng, Alice, and Amanda Casari. 2018. Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "03-dataops-role.html#footnotes",
    "href": "03-dataops-role.html#footnotes",
    "title": "3  The Role of DataOps",
    "section": "",
    "text": "tdwi blog: https://tdwi.org/articles/2019/04/16/diq-all-data-quality-problems-will-haunt-your-analytics-future.aspx↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Role of DataOps</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html",
    "href": "04-dataops-build.html",
    "title": "4  Putting DataOps into Practice",
    "section": "",
    "text": "4.1 Introduction to Data Pipelines\nIn the previous chapter, we explored the foundational concepts of DataOps, focusing on the importance of processes such as data ingestion, validation, versioning, and lineage in building reliable machine learning systems. Now, it’s time to take these concepts from theory to practice by applying them to create data pipelines. Data pipelines form the backbone of any modern ML system, serving as the structured, automated pathways through which data flows from raw sources to actionable insights.\nA data pipeline is a series of interconnected steps that collect, process, validate, and transform data into formats ready for machine learning workflows. Whether you’re ingesting data from multiple sources, cleaning and preparing it for modeling, or ensuring its integrity through validation and versioning, pipelines make it possible to handle these tasks efficiently and consistently. They are essential for ensuring scalability, reliability, and repeatability in ML systems, especially in environments where data is constantly changing or arriving in real-time.\nThis chapter will guide you through the practical implementation of DataOps principles by developing end-to-end data pipelines. We’ll begin with an introduction to the components of a data pipeline and the tools commonly used to implement them. From there, we’ll delve into the step-by-step process of building a pipeline, starting with a simple example and progressing to techniques for creating scalable workflows that can handle complex, large-scale ML applications.\nBy the end of this chapter, you’ll have a deeper understanding of how to translate DataOps practices into actionable pipelines, setting the stage for real-world machine learning deployments. Whether you’re working with batch or streaming data, structured or unstructured datasets, this chapter will equip you with the tools and strategies to design robust data pipelines that meet the demands of modern ML workflows.\nIn the world of machine learning (ML), the success of a system hinges on the quality, reliability, and timeliness of the data that powers it. Data pipelines are the operational backbone of these systems, ensuring data flows seamlessly from its source, through various transformations, and into the hands of models or end-users. A data pipeline is a series of automated steps that enable data ingestion, processing, and delivery—integrating the concepts of DataOps to ensure scalability, consistency, and efficiency.\nIn the previous chapter, we explored the fundamental principles of DataOps, including data ingestion, processing, validation, versioning, and lineage. These processes establish the foundation for building robust data pipelines. Now, we turn our attention to applying these principles in practice to design end-to-end workflows that support machine learning systems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#introduction-to-data-pipelines",
    "href": "04-dataops-build.html#introduction-to-data-pipelines",
    "title": "4  Putting DataOps into Practice",
    "section": "",
    "text": "The Importance of Data Pipelines in ML System Design\nData pipelines address critical challenges faced by ML workflows, such as managing large data volumes, handling diverse data formats, and maintaining data integrity. By automating repetitive and error-prone tasks like ingestion, cleaning, and transformation, pipelines ensure that data is prepared and delivered reliably.\nFrom the concepts introduced earlier, pipelines incorporate:\n\nData Ingestion: Automating the flow of data from various sources—whether databases, APIs, or real-time streams—into a centralized system.\nData Processing: Applying transformations, feature engineering, and cleaning steps to structure raw data into formats ready for model consumption.\nData Validation: Embedding quality checks to ensure the consistency and integrity of data at every stage of the pipeline.\nData Versioning and Lineage: Providing traceability and reproducibility by maintaining historical records of datasets and documenting their transformations.\n\nFor example, in a customer churn prediction system, a data pipeline might ingest transaction data from a database, clean and preprocess it to handle missing values and inconsistencies, and validate that the data conforms to schema standards before feeding it into an ML model. Each step would leverage the DataOps principles discussed earlier to ensure that the process is efficient, scalable, and error-resistant.\n\n\nThe Role of DataOps in Constructing Scalable and Reliable Pipelines\nAs explored in the previous chapter, DataOps is not just a set of practices but a philosophy that ensures data workflows are collaborative, automated, and aligned with business objectives. When applied to pipeline construction, DataOps enables:\n\nScalability: Modular design allows pipelines to handle growing data volumes and adapt to new sources or transformations without significant redesigns.\nReliability: Continuous validation, versioning, and monitoring ensure that pipelines consistently deliver high-quality data, even in dynamic environments.\nCollaboration: DataOps principles encourage communication between data engineers, analysts, and machine learning practitioners, ensuring pipelines meet diverse needs.\nTraceability: By tracking data lineage and applying version control, DataOps ensures every step in the pipeline is documented, facilitating debugging, compliance, and reproducibility.\n\nConsider a recommendation system that ingests real-time user behavior data from an API and combines it with historical purchase data stored in a data lake. A DataOps-driven pipeline would:\n\nIngest and Validate: Fetch data from both sources, validating schema consistency and ensuring data quality.\nProcess and Transform: Standardize features such as timestamps, handle missing values, and engineer inputs like “average time between purchases.”\nTrack Lineage: Document the transformations applied to both real-time and historical data to ensure traceability and compliance.\nVersion Datasets: Maintain snapshots of preprocessed data for future analysis, debugging, or retraining.\n\nBy applying the foundational concepts of DataOps, such pipelines are not only robust and scalable but also agile enough to adapt to evolving ML requirements.\nIn this chapter, we will build on the foundational knowledge from the previous chapter to design and implement data pipelines that embody the principles of DataOps. Through hands-on examples and practical guidance, you will learn how to construct workflows that transform raw data into reliable inputs for machine learning systems, setting the stage for scalable and effective solutions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#etl-vs.-elt-paradigms",
    "href": "04-dataops-build.html#etl-vs.-elt-paradigms",
    "title": "4  Putting DataOps into Practice",
    "section": "4.2 ETL vs. ELT Paradigms",
    "text": "4.2 ETL vs. ELT Paradigms\nWhen designing data pipelines, there are two primary paradigms: Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT). These paradigms represent distinct approaches to organizing and processing data as it flows through pipelines, each with its own strengths and trade-offs. Choosing between ETL and ELT (or a hybrid approach) requires an understanding of your organization’s technical infrastructure, data requirements, and use cases.\n\nExtract, Transform, Load (ETL)\nThe ETL paradigm embodies a traditional, structured approach to data pipelines. It follows a linear process:\n\nExtract data from various sources, such as relational databases, APIs, or CSV files.\nTransform the extracted data into a clean, structured format using techniques like cleaning, aggregation, and enrichment.\nLoad the transformed data into a target system, such as a relational database or data warehouse.\n\n\n\n\n\n\n\nflowchart LR\n    ob1(Raw data source) --&gt; processing[Data processing]\n    ob2(Raw data source) --&gt; processing\n    ob3(Raw data source) --&gt; processing\n    processing --&gt; id3[(Data storage)]\n\n\n\n\nFigure 4.1: In the ETL paradigm, the pipeline ingests the data, then processes the data prior to storing the data in their target destination.\n\n\n\n\n\nETL pipelines focus on preparing high-quality, ready-to-use data before it enters the storage system. In ETL workflows, the data processing phase discussed earlier plays a central role. Transformations are performed early in the pipeline to ensure data quality and structure before it reaches the storage layer.\n\n\n\n\n\n\nWhen to Use ETL\n\n\n\n\n\n\nWhen working with traditional data warehouses or legacy systems that prioritize structured and clean data.\nIn use cases requiring strict data governance and pre-defined schemas, such as regulatory compliance reporting.\nWhen downstream applications depend on consistent, pre-processed datasets.\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\n\n\n\nSimplifies downstream workflows by ensuring data is pre-processed and ready for use.\nReduces storage costs by retaining only clean and structured data.\nAligns well with environments that require high data integrity.\n\n\n\n\n\n\n\n\n\n\nChallenges\n\n\n\n\n\n\nUpfront transformations can delay data availability.\nScaling ETL pipelines to handle large datasets or real-time workflows can be resource-intensive.\nRigid processes may struggle to adapt to rapidly changing data needs.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nImagine a financial institution that extracts transaction data, transforms it into a consistent format by applying currency conversions and anomaly detection, and loads the cleaned data into a warehouse for fraud detection reports.\n\n\n\n\n\nExtract, Load, Transform (ELT)\nThe ELT paradigm, a more modern approach, inverts the transformation and loading steps:\n\nExtract data from various sources, often retaining its raw format.\nLoad the extracted data directly into a scalable storage system, such as a data lake or cloud-based data warehouse.\nTransform the data within the storage system using its computational resources, tailoring transformations to specific analytical needs.\n\n\n\n\n\n\n\nflowchart LR\n    ob1(Raw data source) --&gt; id3[(Data storage)]\n    ob2(Raw data source) --&gt; id3\n    ob3(Raw data source) --&gt; id3\n    id3 --&gt; processing[Data processing] --&gt; id3\n\n\n\n\nFigure 4.2: In the ELT paradigm, the pipeline ingests data directly into the destination system and transforms it in parallel or after the fact.\n\n\n\n\n\nIn ELT workflows, the data ingestion phase emphasizes rapid loading of raw data into storage, enabling flexibility for later transformations. Also, by storing the data in its raw form, ELT better handles structured, unstructured, and semi-structured data and allows downstream analytics on this data to more easily adapt.\n\n\n\n\n\n\nWhen to Use ELT\n\n\n\n\n\n\nWhen leveraging cloud-native platforms like Snowflake, BigQuery, or AWS Redshift, which support high-speed storage and in-database transformations.\nIn workflows requiring rapid ingestion and the ability to adapt transformations for different use cases.\nWhen storing raw data is essential for exploratory analysis or regulatory purposes.\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\n\n\n\n\n\nEnables faster data ingestion by deferring transformations.\nSupports diverse and flexible transformations tailored to specific analyses.\nScales well with large data volumes, leveraging modern storage and processing systems.\n\n\n\n\n\n\n\n\n\n\nChallenges\n\n\n\n\n\n\nRequires advanced infrastructure to handle and process raw data efficiently.\nHigher storage costs due to the retention of raw, unprocessed data.\nAd-hoc transformations can lead to inconsistencies if not governed properly.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn e-commerce platform ingests raw clickstream data into a cloud data warehouse. When needed, transformations such as sessionizing user behavior or aggregating purchase trends are applied directly within the warehouse for personalized recommendations or sales forecasting.\n\n\n\n\n\nComparing ETL and ELT\n\n\n\n\n\n\n\n\nAspect\nETL\nELT\n\n\n\n\nData Transformation\nBefore loading into storage\nAfter loading into storage\n\n\nStorage Requirements\nLower, as only processed data is stored\nHigher, as raw data is retained\n\n\nProcessing Time\nSlower ingestion due to upfront transformations\nFaster ingestion with deferred transformations\n\n\nFlexibility\nLimited; transformations are predefined\nHigh; transformations can be ad hoc\n\n\nInfrastructure\nSuitable for legacy systems or traditional data warehouses\nIdeal for modern, scalable systems\n\n\n\n\n\n\n\n\n\nChoosing the Right Paradigm\n\n\n\n\n\nAs highlighted in the previous chapter, the goals of DataOps — efficiency, reliability, and scalability — should guide the choice of ETL, ELT, or a hybrid approach. Consider:\n\nETL is well-suited for structured environments and use cases that demand immediate access to clean, well-processed data.\nELT shines in cloud-native or big data ecosystems where raw data flexibility and scalability are critical.\n\nBoth paradigms have their strengths, and many organizations blend elements of each. For example, a retail company might use ETL for compliance reporting while leveraging ELT for real-time inventory analysis. By understanding these paradigms within the broader DataOps framework, teams can design pipelines that meet both technical and business requirements effectively.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#tools-for-dataops",
    "href": "04-dataops-build.html#tools-for-dataops",
    "title": "4  Putting DataOps into Practice",
    "section": "4.3 Tools for DataOps",
    "text": "4.3 Tools for DataOps\nBuilding effective and scalable data pipelines requires leveraging specialized tools at each stage of the DataOps lifecycle. From data ingestion to processing, validation, and versioning, the ecosystem of tools available is vast and continually expanding. This section provides an overview of commonly used tools, highlighting their capabilities, trade-offs, and when they might be most appropriate. Additionally, this space is dynamic, with new tools regularly introduced to address evolving challenges. Therefore, the goal is not to become tied to a specific tool but to understand the broader landscape and make informed decisions based on your pipeline’s requirements.\n\nData Ingestion Tools\nEfficiently gathering data from diverse sources is the foundation of any pipeline. Ingestion tools vary from those tailored for real-time streaming to solutions focused on batch processing and ease of integration. Some examples include:\n\n\n\n\n\n\nApache Kafka\n\n\n\n\n\nA distributed event-streaming platform designed for high-throughput, real-time data ingestion. Kafka is widely used in applications like fraud detection, IoT data pipelines, and real-time analytics.\n\nWhen to Use: Ideal for high-velocity, low-latency systems.\nLimitations: Steep learning curve and resource-heavy for smaller use cases.\n\n\n\n\n\n\n\n\n\n\nApache NiFi\n\n\n\n\n\nA flow-based programming tool that automates data movement and transformation with a user-friendly interface. NiFi supports a wide range of data sources and formats.\n\nWhen to Use: Low-code scenarios requiring rapid integration of multiple data sources.\nLimitations: Less suited for complex, high-throughput pipelines.\n\n\n\n\n\n\n\n\n\n\nAirbyte\n\n\n\n\n\nAn open-source data integration platform focused on moving data into data warehouses or lakes. Its extensive library of connectors simplifies ingesting data from various APIs and databases.\n\nWhen to Use: Batch ingestion with diverse source compatibility.\nLimitations: Primarily batch-focused and may require configuration for real-time pipelines.\n\n\n\n\n\n\n\n\n\n\nFivetran\n\n\n\n\n\nA managed data integration tool that simplifies data ingestion by automating schema handling and updates. It’s widely used for moving data into analytics-ready storage solutions.\n\nWhen to Use: Enterprise scenarios requiring minimal management overhead for cloud data integration.\nLimitations: Subscription-based pricing and limited customization compared to open-source tools.\n\n\n\n\n\n\nData Processing Tools\nProcessing raw data into clean, structured, and feature-rich formats is critical to preparing it for machine learning workflows. Many tools exist but the following are a few popular tools that support various scales and complexities of data transformation.\n\n\n\n\n\n\nApache Spark\n\n\n\n\n\nA powerful engine for distributed data processing. Spark supports both batch and streaming data workflows, making it suitable for big data and ML pipelines.\n\nWhen to Use: Processing large-scale data across distributed systems.\nLimitations: Requires expertise and infrastructure, which might be overkill for smaller datasets.\n\n\n\n\n\n\n\n\n\n\nDBT (Data Build Tool)\n\n\n\n\n\nA transformation tool that applies SQL to prepare data for analytics workflows. DBT is particularly useful for pipelines that involve data warehouses.\n\nWhen to Use: SQL-centric environments where analytics-ready data is the end goal.\nLimitations: Focused on transformations within SQL databases; less versatile for non-SQL workflows.\n\n\n\n\n\n\n\n\n\n\nPolars\n\n\n\n\n\nAn open source Python & Rust library for data manipulation and analysis, offering rich functionality for handling structured data with performance in mind.\n\nWhen to Use: Small to large datasets or for prototyping workflows. A great substitute for Pandas when performance and speed are required.\nLimitations: Not designed for real-time processing.\n\n\n\n\n\n\n\n\n\n\nPrefect\n\n\n\n\n\nA workflow orchestration tool that enables robust scheduling and monitoring of data processing tasks.\n\nWhen to Use: Complex pipelines requiring orchestration of multiple processing steps.\nLimitations: Adds an orchestration layer, which might be unnecessary for simple workflows.\n\n\n\n\n\n\nData Validation Tools\nEnsuring the integrity and accuracy of data is essential to building reliable pipelines. Validation tools help detect anomalies, enforce schema compliance, and improve data quality. Some popular data validation tools include:\n\n\n\n\n\n\nGreat Expectations\n\n\n\n\n\nA framework for defining, executing, and documenting data validation checks. It integrates well with modern data stacks.\n\nWhen to Use: Custom validation rules with automated reporting needs.\nLimitations: May require significant customization for non-standard checks.\n\n\n\n\n\n\n\n\n\n\nTFDV (TensorFlow Data Validation)\n\n\n\n\n\nA library designed for detecting anomalies and validating schema in ML datasets.\n\nWhen to Use: TensorFlow-based workflows or pipelines requiring statistical validation.\nLimitations: Limited applicability outside of TensorFlow-centric environments.\n\n\n\n\n\n\n\n\n\n\nDeequ\n\n\n\n\n\nA library developed by AWS for validating large datasets using Spark. It focuses on automated quality checks and anomaly detection.\n\nWhen to Use: Spark-based pipelines requiring scalable data quality checks.\nLimitations: Limited compatibility with non-Spark environments.\n\n\n\n\n\n\nData Versioning Tools\nVersioning ensures traceability and reproducibility by tracking changes to datasets and workflows. It is vital for debugging, compliance, and collaboration. Some common data versioning tools include:\n\n\n\n\n\n\nDVC (Data Version Control)\n\n\n\n\n\nA Git-like tool for versioning data, models, and pipelines. It integrates seamlessly with Git repositories.\n\nWhen to Use: Managing medium to large datasets in collaborative environments.\nLimitations: Can be challenging to set up for teams unfamiliar with Git.\n\n\n\n\n\n\n\n\n\n\nDelta Lake\n\n\n\n\n\nA storage layer that adds versioning and ACID transactions to data lakes. It works well with distributed systems like Apache Spark.\n\nWhen to Use: Large-scale pipelines needing robust versioning and consistency.\nLimitations: Requires Spark for full functionality, adding complexity to smaller-scale workflows.\n\n\n\n\n\n\n\n\n\n\nLakeFS\n\n\n\n\n\nA Git-like version control system for data lakes. It enables branching and snapshotting of data workflows.\n\nWhen to Use: Data lakes requiring advanced version control features.\nLimitations: Best suited for teams already working with data lakes.\n\n\n\n\n\n\nNavigating a Rapidly Evolving Ecosystem\nThe DataOps tooling landscape is dynamic, with new tools and frameworks regularly introduced to address emerging challenges. This constant innovation provides opportunities to enhance workflows but also demands adaptability.\n\nFocus on Principles: Rather than mastering specific tools, prioritize understanding the core concepts of data ingestion, processing, validation, and versioning. This flexibility allows you to evaluate and adopt new tools as they emerge.\nExperiment and Iterate: Allocate time for evaluating tools that may better align with your pipeline’s evolving needs. Open-source communities and GitHub repositories are excellent resources for exploration.\nHybrid Tooling: Often, no single tool will address all requirements perfectly. Combining tools—such as using Airbyte for ingestion, DBT for processing, and Great Expectations for validation—creates tailored workflows.\nLeverage Community Support: Most tools provide extensive documentation, active forums, and integration guides. Engage with these communities to stay informed about updates and best practices.\n\nBy focusing on the principles of DataOps and staying informed about the ever-changing ecosystem, teams can design resilient and adaptable pipelines. In the following sections, we’ll explore how to combine these tools into cohesive workflows and build scalable, efficient data pipelines for machine learning systems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#hands-on-example-a-youtube-data-pipeline",
    "href": "04-dataops-build.html#hands-on-example-a-youtube-data-pipeline",
    "title": "4  Putting DataOps into Practice",
    "section": "4.4 Hands-On Example: A YouTube Data Pipeline",
    "text": "4.4 Hands-On Example: A YouTube Data Pipeline\nNow that you’ve learned a bit about data pipelines, common paradigms, and tooling involved, let’s apply some of these concepts to create a simplified data pipeline that demonstrates how to ingest, process, and prepare YouTube data for downstream machine learning (ML) workflows. We’ll focus on collecting data like video titles, transcripts, views, likes, and comments. This example ties together concepts from previous chapters, including data ingestion, processing, validation, and versioning.\n\nPipeline Design\nThe pipeline design we’ll use will resemble more of an ETL process than an ELT. This is mainly for storage simplicity since this pipeline is run locally (whether that be on my machine or your machine) rather than in a cloud environment where storage capacity constraints are less of an issue.\nWe’ll use a couple of APIs to ingest our data, perform a little bit of data processing to clean and prepare our data for future analyses, and then illustrate some basic data validation along with versioning and storing our final prepared data.\n\n\n\n\n\n\nflowchart LR\n    subgraph ingest[Data Ingestion]\n      direction LR\n      subgraph p1[Ingest Video IDs]\n      end\n      subgraph p2[Ingest Video Stats]\n      end\n      subgraph p3[Ingest Video Transcript]\n      end\n    end\n    p1 --&gt; p2\n    p1 --&gt; p3\n    ingest --&gt; process(Process Raw Data)\n    process --&gt; Validate --&gt; Version --&gt; data[(Data storage)]\n\n\n\n\nFigure 4.3: High-level architecture of our data pipeline, which leverages an ETL process to ingest, process, validate, version and then store our data.\n\n\n\n\n\n\n\n\n\n\n\nBasic Requirements\n\n\n\n\n\nIf you’d like to follow along and execute this code then there are a few requirements you’ll need on your end.\nFirst, you’ll need to make sure you have the following Python libraries installed:\n\n\n\nPython version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]\n\ngreat_expectations==1.3.1\njupyterlab==4.1.6\nmatplotlib==3.8.0\nnumpy==1.26.4\npandas&lt;=2.2\npython-dotenv==0.21.0\ntqdm==4.63.0\nyoutube_transcript_api==0.6.2\n\n\n\nNext, to simplify this example I have created helper functions to abstract away a lot of the finer code details. You can see this in the following imports where I am importing helper functions from the dataops_utils.py module. If you want to reproduce this pipeline then you can download the dataops_utils.py script here.\n\n\nimport great_expectations as gx\nimport os\nimport numpy as np\nimport pandas as pd\nimport unicodedata\n\nfrom dataops_utils import (\n    ingest_channel_video_ids,\n    ingest_video_stats,\n    ingest_video_transcript,\n)\nfrom dotenv import load_dotenv\n\n\nLastly, We’ll be using the YouTube Data API to extract metadata and video information. To follow along, ensure you have:\n\nA Google Cloud account.\nAccess to the YouTube Data API to include an API key. See here to get started.\n\nOnce you have a Youtube API key then you’re ready to go! The next code chunk sets your API key and establishes the API URL and the channel ID. Note, I’m a golf junkie so the channel ID I am using is for Grant Horvat, a Youtube golfer with nearly 1 million followers.1\n\n\n# I have my API key set as an environment variable\nload_dotenv()\nAPI_KEY = os.getenv('YOUTUBE_API_KEY')\n\n# In your case you can add your API key here\nif API_KEY is None:\n    API_KEY = \"INSERT_YOUR_YOUTUBE_API_KEY\"\n\nBASE_URL = \"https://www.googleapis.com/youtube/v3\"\nCHANNEL_ID = 'UCgUueMmSpcl-aCTt5CuCKQw'\n\n\n\n\n\n\n\n\n\n\n\nData Ingestion\n\n\n\n\n\nThe first step is to ingest the Youtube data. To do so, we need to follow three steps:\n\nIngest all the video IDs provided by a given channel.\n\n\n\n# Ingest Youtube video IDs\nvideo_ids = ingest_channel_video_ids(API_KEY, CHANNEL_ID)\n\n# Example of what the first record looks like\nvideo_ids[0]\n\n{'channel_id': 'UCgUueMmSpcl-aCTt5CuCKQw',\n 'video_id': 'fSHh01YT0-Q',\n 'datetime': '2025-01-07T18:50:32Z',\n 'title': 'Tiger Woods hits the ball off the heel.'}\n\n\n\n\nUse the ingested video IDs to ingest Youtube stats for each video such as total views, likes, and comments.\n\n\n\n# Ingest Youtube video statistics\nvideo_data = ingest_video_stats(video_ids, API_KEY)\n\n# Example of the stats collected for the first video\nvideo_data[0]\n\n{'channel_id': 'UCgUueMmSpcl-aCTt5CuCKQw',\n 'video_id': 'fSHh01YT0-Q',\n 'datetime': '2025-01-07T18:50:32Z',\n 'title': 'Tiger Woods hits the ball off the heel.',\n 'views': '59334',\n 'likes': '1881',\n 'comments': '30'}\n\n\n\n\nAnd lastly, ingest the transcript for each video.\n\n\n\n# Ingest Youtube video transcripts\nvideo_data = ingest_video_transcript(video_data)\n\n# Example of the final raw data that includes\n# video ID, title, date, stats, and transcript\nvideo_data[0]\n\n{'channel_id': 'UCgUueMmSpcl-aCTt5CuCKQw',\n 'video_id': 'fSHh01YT0-Q',\n 'datetime': '2025-01-07T18:50:32Z',\n 'title': 'Tiger Woods hits the ball off the heel.',\n 'views': '59334',\n 'likes': '1881',\n 'comments': '30',\n 'transcript': \"over the course of my career I've always hit the ball off the heel this is one of the reason why I still use my old te's O Okay so if I'm going to hit one in play that's how high you would Tee It Up if I have to hit one in play if I have to hit one out of the wind okay I like that what's wrong with that wow that's right at the pole this keeps you on top and Swinging more left and as I said I would try and hit it for me over a course of my career I've always hit ball off the heel\"}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#creating-scalable-data-pipelines",
    "href": "04-dataops-build.html#creating-scalable-data-pipelines",
    "title": "4  Putting DataOps into Practice",
    "section": "4.5 Creating Scalable Data Pipelines",
    "text": "4.5 Creating Scalable Data Pipelines\n\nHow to build data pipelines that scale to accommodate increasing data volumes.\nKey considerations for scalability, such as distributed processing and fault tolerance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#conclusion-and-key-takeaways",
    "href": "04-dataops-build.html#conclusion-and-key-takeaways",
    "title": "4  Putting DataOps into Practice",
    "section": "4.6 Conclusion and Key Takeaways",
    "text": "4.6 Conclusion and Key Takeaways\n\nSummary of the key principles of DataOps and how to apply them in building effective data pipelines for ML.\nReflection on the importance of data quality, management, and scalability in ML workflows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "04-dataops-build.html#footnotes",
    "href": "04-dataops-build.html#footnotes",
    "title": "4  Putting DataOps into Practice",
    "section": "",
    "text": "Feel free to explore a different Youtube channel. Note that the channel ID is different than the Youtube handle. For example, Grant Horvat’s Youtube handle is @GrantHorvatGolfs but his channel ID is UCgUueMmSpcl-aCTt5CuCKQw. The easiest way to find a channel’s ID is to go to the channel metadata where information for listed for “About”, “Links”, and “Channel details”. At the bottom of the pop up window is an option to “Share Channel” and then “Copy Channel ID”.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Putting DataOps into Practice</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald\nGall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas\nZimmermann. 2019. “Software Engineering for Machine Learning: A\nCase Study.” In 2019 IEEE/ACM 41st International Conference\non Software Engineering: Software Engineering in Practice\n(ICSE-SEIP), 291–300. IEEE.\n\n\nGama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and\nAbdelhamid Bouchachia. 2014. “A Survey on Concept Drift\nAdaptation.” ACM Computing Surveys (CSUR) 46 (4): 1–37.\n\n\nGéron, Aurélien. 2022. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow. \" O’Reilly Media, Inc.\".\n\n\nHeckman, James. 2013. “Sample Selection Bias as a Specification\nError.” Applied Econometrics 31 (3): 129–37.\n\n\nHuyen, Chip. 2022. Designing Machine Learning Systems. \"\nO’Reilly Media, Inc.\".\n\n\nKuhn, Max, and Kjell Johnson. 2019. Feature Engineering and\nSelection: A Practical Approach for Predictive Models. Chapman;\nHall/CRC.\n\n\nLerman, Rachel. February 3, 2016. “Google Is Testing Its\nSelf-Driving Car in Kirkland,” February 3, 2016.\n\n\nLevine, Sergey, Aviral Kumar, George Tucker, and Justin Fu. 2020.\n“Offline Reinforcement Learning: Tutorial, Review, and\nPerspectives on Open Problems.” arXiv Preprint\narXiv:2005.01643.\n\n\nMattson, Peter, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius\nMicikevicius, David Patterson, Hanlin Tang, et al. 2020. “Mlperf\nTraining Benchmark.” Proceedings of Machine Learning and\nSystems 2: 336–49.\n\n\nPapernot, Nicolas, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z\nBerkay Celik, and Ananthram Swami. 2017. “Practical Black-Box\nAttacks Against Machine Learning.” In Proceedings of the 2017\nACM on Asia Conference on Computer and Communications Security,\n506–19.\n\n\nPolyzotis, Neoklis, Martin Zinkevich, Sudip Roy, Eric Breck, and Steven\nWhang. 2019. “Data Validation for Machine Learning.”\nProceedings of Machine Learning and Systems 1: 334–47.\n\n\nSculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd\nPhillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois\nCrespo, and Dan Dennison. 2015. “Hidden Technical Debt in Machine\nLearning Systems.” Advances in Neural Information Processing\nSystems 28.\n\n\nTolomei, Gabriele, Fabrizio Silvestri, Andrew Haines, and Mounia Lalmas.\n2017. “Interpretable Predictions of Tree-Based Ensembles via\nActionable Feature Tweaking.” In Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data\nMining, 465–74.\n\n\nVemulapalli, Gopichand. 2023. “Operationalizing Machine Learning\nBest Practices for Scalable Production Deployments.”\nInternational Machine Learning Journal and Computer Engineering\n6 (6): 1–21.\n\n\nZheng, Alice, and Amanda Casari. 2018. Feature Engineering for\nMachine Learning: Principles and Techniques for Data Scientists. \"\nO’Reilly Media, Inc.\".",
    "crumbs": [
      "References"
    ]
  }
]