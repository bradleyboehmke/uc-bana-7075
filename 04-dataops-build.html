<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Putting DataOps into Practice – Machine Learning Design for Business</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-modelops-role.html" rel="next">
<link href="./03-dataops-role.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-66ab7fd5e73b7f0a764e0d49b3e29ab1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-78faf54a06e50be45dc6d83ab59297a7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="mermaid-theme" content="neutral">
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-dataops-build.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Putting DataOps into Practice</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning Design for Business</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/bradleyboehmke/uc-bana-7075" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-Design-for-Business.epub" title="Download ePub" class="quarto-navigation-tool px-1" aria-label="Download ePub"><i class="bi bi-journal"></i></a>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Laying the Foundation</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-ml-system.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">ML System Design</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-before-we-build.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Before We Build</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">DataOps</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-dataops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Role of DataOps</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dataops-build.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Putting DataOps into Practice</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">ModelOps</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-modelops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Role of ModelOps</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-modelops-experimenting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model Training and Experiment Tracking</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">DevOps</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">The Human Side of ML Systems</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-data-pipelines" id="toc-introduction-to-data-pipelines" class="nav-link active" data-scroll-target="#introduction-to-data-pipelines"><span class="header-section-number">4.1</span> Introduction to Data Pipelines</a>
  <ul class="collapse">
  <li><a href="#the-importance-of-data-pipelines-in-ml-system-design" id="toc-the-importance-of-data-pipelines-in-ml-system-design" class="nav-link" data-scroll-target="#the-importance-of-data-pipelines-in-ml-system-design">The Importance of Data Pipelines in ML System Design</a></li>
  <li><a href="#the-role-of-dataops-in-constructing-scalable-and-reliable-pipelines" id="toc-the-role-of-dataops-in-constructing-scalable-and-reliable-pipelines" class="nav-link" data-scroll-target="#the-role-of-dataops-in-constructing-scalable-and-reliable-pipelines">The Role of DataOps in Constructing Scalable and Reliable Pipelines</a></li>
  </ul></li>
  <li><a href="#etl-vs.-elt-paradigms" id="toc-etl-vs.-elt-paradigms" class="nav-link" data-scroll-target="#etl-vs.-elt-paradigms"><span class="header-section-number">4.2</span> ETL vs.&nbsp;ELT Paradigms</a>
  <ul class="collapse">
  <li><a href="#extract-transform-load-etl" id="toc-extract-transform-load-etl" class="nav-link" data-scroll-target="#extract-transform-load-etl"><strong>Extract, Transform, Load (ETL)</strong></a></li>
  <li><a href="#extract-load-transform-elt" id="toc-extract-load-transform-elt" class="nav-link" data-scroll-target="#extract-load-transform-elt"><strong>Extract, Load, Transform (ELT)</strong></a></li>
  <li><a href="#comparing-etl-and-elt" id="toc-comparing-etl-and-elt" class="nav-link" data-scroll-target="#comparing-etl-and-elt">Comparing ETL and ELT</a></li>
  </ul></li>
  <li><a href="#tools-for-dataops" id="toc-tools-for-dataops" class="nav-link" data-scroll-target="#tools-for-dataops"><span class="header-section-number">4.3</span> Tools for DataOps</a>
  <ul class="collapse">
  <li><a href="#data-ingestion-tools" id="toc-data-ingestion-tools" class="nav-link" data-scroll-target="#data-ingestion-tools">Data Ingestion Tools</a></li>
  <li><a href="#data-processing-tools" id="toc-data-processing-tools" class="nav-link" data-scroll-target="#data-processing-tools">Data Processing Tools</a></li>
  <li><a href="#data-validation-tools" id="toc-data-validation-tools" class="nav-link" data-scroll-target="#data-validation-tools">Data Validation Tools</a></li>
  <li><a href="#data-versioning-tools" id="toc-data-versioning-tools" class="nav-link" data-scroll-target="#data-versioning-tools">Data Versioning Tools</a></li>
  <li><a href="#navigating-a-rapidly-evolving-ecosystem" id="toc-navigating-a-rapidly-evolving-ecosystem" class="nav-link" data-scroll-target="#navigating-a-rapidly-evolving-ecosystem">Navigating a Rapidly Evolving Ecosystem</a></li>
  </ul></li>
  <li><a href="#hands-on-example-a-youtube-data-pipeline" id="toc-hands-on-example-a-youtube-data-pipeline" class="nav-link" data-scroll-target="#hands-on-example-a-youtube-data-pipeline"><span class="header-section-number">4.4</span> Hands-On Example: A YouTube Data Pipeline</a>
  <ul class="collapse">
  <li><a href="#pipeline-design" id="toc-pipeline-design" class="nav-link" data-scroll-target="#pipeline-design">Pipeline Design</a></li>
  <li><a href="#basic-requirements" id="toc-basic-requirements" class="nav-link" data-scroll-target="#basic-requirements">Basic Requirements</a></li>
  <li><a href="#data-ingestion" id="toc-data-ingestion" class="nav-link" data-scroll-target="#data-ingestion">Data Ingestion</a></li>
  <li><a href="#data-processing" id="toc-data-processing" class="nav-link" data-scroll-target="#data-processing">Data Processing</a></li>
  <li><a href="#data-validation" id="toc-data-validation" class="nav-link" data-scroll-target="#data-validation">Data Validation</a></li>
  <li><a href="#data-versioning" id="toc-data-versioning" class="nav-link" data-scroll-target="#data-versioning">Data Versioning</a></li>
  </ul></li>
  <li><a href="#creating-reliable-scalable-data-pipelines" id="toc-creating-reliable-scalable-data-pipelines" class="nav-link" data-scroll-target="#creating-reliable-scalable-data-pipelines"><span class="header-section-number">4.5</span> Creating Reliable &amp; Scalable Data Pipelines</a>
  <ul class="collapse">
  <li><a href="#principles-incorporated-in-the-youtube-data-pipeline" id="toc-principles-incorporated-in-the-youtube-data-pipeline" class="nav-link" data-scroll-target="#principles-incorporated-in-the-youtube-data-pipeline">Principles Incorporated in the YouTube Data Pipeline</a></li>
  <li><a href="#limitations-and-areas-for-improvement" id="toc-limitations-and-areas-for-improvement" class="nav-link" data-scroll-target="#limitations-and-areas-for-improvement">Limitations and Areas for Improvement</a></li>
  <li><a href="#balancing-design-principles" id="toc-balancing-design-principles" class="nav-link" data-scroll-target="#balancing-design-principles">Balancing Design Principles</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">4.6</span> Summary</a></li>
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise"><span class="header-section-number">4.7</span> Exercise</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/edit/main/04-dataops-build.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Putting DataOps into Practice</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In the previous chapter, we explored the foundational concepts of DataOps, focusing on the importance of processes such as data ingestion, validation, versioning, and lineage in building reliable machine learning systems. Now, it’s time to take these concepts from theory to practice by applying them to create data pipelines. Data pipelines form the backbone of any modern ML system, serving as the structured, automated pathways through which data flows from raw sources to actionable insights.</p>
<p>A <strong>data pipeline</strong> is a series of interconnected steps that collect, process, validate, and transform data into formats ready for machine learning workflows. Whether you’re ingesting data from multiple sources, cleaning and preparing it for modeling, or ensuring its integrity through validation and versioning, pipelines make it possible to handle these tasks efficiently and consistently. They are essential for ensuring scalability, reliability, and repeatability in ML systems, especially in environments where data is constantly changing or arriving in real-time.</p>
<p>This chapter will guide you through the practical implementation of DataOps principles by developing end-to-end data pipelines. We’ll begin with an introduction to the components of a data pipeline and the tools commonly used to implement them. From there, we’ll delve into the step-by-step process of building a pipeline, starting with a simple example and progressing to techniques for creating scalable workflows that can handle complex, large-scale ML applications.</p>
<p>By the end of this chapter, you’ll have a deeper understanding of how to translate DataOps practices into actionable pipelines, setting the stage for real-world machine learning deployments. Whether you’re working with batch or streaming data, structured or unstructured datasets, this chapter will equip you with the tools and strategies to design robust data pipelines that meet the demands of modern ML workflows.</p>
<section id="introduction-to-data-pipelines" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="introduction-to-data-pipelines"><span class="header-section-number">4.1</span> Introduction to Data Pipelines</h2>
<p>In the world of machine learning (ML), the success of a system hinges on the quality, reliability, and timeliness of the data that powers it. Data pipelines are the operational backbone of these systems, ensuring data flows seamlessly from its source, through various transformations, and into the hands of models or end-users. A <strong>data pipeline</strong> is a series of automated steps that enable data ingestion, processing, and delivery—integrating the concepts of DataOps to ensure scalability, consistency, and efficiency.</p>
<p>In the previous chapter, we explored the fundamental principles of DataOps, including <strong>data ingestion</strong>, <strong>processing</strong>, <strong>validation</strong>, <strong>versioning</strong>, and <strong>lineage</strong>. These processes establish the foundation for building robust data pipelines. Now, we turn our attention to applying these principles in practice to design end-to-end workflows that support machine learning systems.</p>
<section id="the-importance-of-data-pipelines-in-ml-system-design" class="level3">
<h3 class="anchored" data-anchor-id="the-importance-of-data-pipelines-in-ml-system-design">The Importance of Data Pipelines in ML System Design</h3>
<p>Data pipelines address critical challenges faced by ML workflows, such as managing large data volumes, handling diverse data formats, and maintaining data integrity. By automating repetitive and error-prone tasks like ingestion, cleaning, and transformation, pipelines ensure that data is prepared and delivered reliably.</p>
<p>From the concepts introduced earlier, pipelines incorporate:</p>
<ul>
<li><strong>Data Ingestion</strong>: Automating the flow of data from various sources—whether databases, APIs, or real-time streams—into a centralized system.</li>
<li><strong>Data Processing</strong>: Applying transformations, feature engineering, and cleaning steps to structure raw data into formats ready for model consumption.</li>
<li><strong>Data Validation</strong>: Embedding quality checks to ensure the consistency and integrity of data at every stage of the pipeline.</li>
<li><strong>Data Versioning and Lineage</strong>: Providing traceability and reproducibility by maintaining historical records of datasets and documenting their transformations.</li>
</ul>
<p>For example, in a customer churn prediction system, a data pipeline might ingest transaction data from a database, clean and preprocess it to handle missing values and inconsistencies, and validate that the data conforms to schema standards before feeding it into an ML model. Each step would leverage the DataOps principles discussed earlier to ensure that the process is efficient, scalable, and error-resistant.</p>
</section>
<section id="the-role-of-dataops-in-constructing-scalable-and-reliable-pipelines" class="level3">
<h3 class="anchored" data-anchor-id="the-role-of-dataops-in-constructing-scalable-and-reliable-pipelines">The Role of DataOps in Constructing Scalable and Reliable Pipelines</h3>
<p>As explored in the previous chapter, DataOps is not just a set of practices but a philosophy that ensures data workflows are collaborative, automated, and aligned with business objectives. When applied to pipeline construction, DataOps enables:</p>
<ul>
<li><strong>Scalability</strong>: Modular design allows pipelines to handle growing data volumes and adapt to new sources or transformations without significant redesigns.</li>
<li><strong>Reliability</strong>: Continuous validation, versioning, and monitoring ensure that pipelines consistently deliver high-quality data, even in dynamic environments.</li>
<li><strong>Collaboration</strong>: DataOps principles encourage communication between data engineers, analysts, and machine learning practitioners, ensuring pipelines meet diverse needs.</li>
<li><strong>Traceability</strong>: By tracking data lineage and applying version control, DataOps ensures every step in the pipeline is documented, facilitating debugging, compliance, and reproducibility.</li>
</ul>
<p>Consider a recommendation system that ingests real-time user behavior data from an API and combines it with historical purchase data stored in a data lake. A DataOps-driven pipeline would:</p>
<ol type="1">
<li><strong>Ingest and Validate</strong>: Fetch data from both sources, validating schema consistency and ensuring data quality.</li>
<li><strong>Process and Transform</strong>: Standardize features such as timestamps, handle missing values, and engineer inputs like “average time between purchases.”</li>
<li><strong>Track Lineage</strong>: Document the transformations applied to both real-time and historical data to ensure traceability and compliance.</li>
<li><strong>Version Datasets</strong>: Maintain snapshots of preprocessed data for future analysis, debugging, or retraining.</li>
</ol>
<p>By applying the foundational concepts of DataOps, such pipelines are not only robust and scalable but also agile enough to adapt to evolving ML requirements.</p>
<p>In this chapter, we will build on the foundational knowledge from the previous chapter to design and implement data pipelines that embody the principles of DataOps. Through hands-on examples and practical guidance, you will learn how to construct workflows that transform raw data into reliable inputs for machine learning systems, setting the stage for scalable and effective solutions.</p>
</section>
</section>
<section id="etl-vs.-elt-paradigms" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="etl-vs.-elt-paradigms"><span class="header-section-number">4.2</span> ETL vs.&nbsp;ELT Paradigms</h2>
<p>When designing data pipelines, there are two primary paradigms: <strong>Extract, Transform, Load (ETL)</strong> and <strong>Extract, Load, Transform (ELT)</strong>. These paradigms represent distinct approaches to organizing and processing data as it flows through pipelines, each with its own strengths and trade-offs. Choosing between ETL and ELT (or a hybrid approach) requires an understanding of your organization’s technical infrastructure, data requirements, and use cases.</p>
<section id="extract-transform-load-etl" class="level3">
<h3 class="anchored" data-anchor-id="extract-transform-load-etl"><strong>Extract, Transform, Load (ETL)</strong></h3>
<p>The <strong>ETL</strong> paradigm embodies a traditional, structured approach to data pipelines. It follows a linear process:</p>
<ol type="1">
<li><strong>Extract</strong> data from various sources, such as relational databases, APIs, or CSV files.</li>
<li><strong>Transform</strong> the extracted data into a clean, structured format using techniques like cleaning, aggregation, and enrichment.</li>
<li><strong>Load</strong> the transformed data into a target system, such as a relational database or data warehouse.</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-etl" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-etl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-etl">flowchart LR
    ob1(Raw data source) --&gt; processing[Data processing]
    ob2(Raw data source) --&gt; processing
    ob3(Raw data source) --&gt; processing
    processing --&gt; id3[(Data storage)]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-etl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: In the ETL paradigm, the pipeline ingests the data, then processes the data prior to storing the data in their target destination.
</figcaption>
</figure>
</div>
</div>
</div>
<p>ETL pipelines focus on preparing high-quality, ready-to-use data before it enters the storage system. In ETL workflows, the <strong>data processing</strong> phase discussed earlier plays a central role. Transformations are performed early in the pipeline to ensure data quality and structure before it reaches the storage layer.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When to Use ETL
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>When working with traditional data warehouses or legacy systems that prioritize structured and clean data.</li>
<li>In use cases requiring strict data governance and pre-defined schemas, such as regulatory compliance reporting.</li>
<li>When downstream applications depend on consistent, pre-processed datasets.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Simplifies downstream workflows by ensuring data is pre-processed and ready for use.</li>
<li>Reduces storage costs by retaining only clean and structured data.</li>
<li>Aligns well with environments that require high data integrity.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Challenges
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Upfront transformations can delay data availability.</li>
<li>Scaling ETL pipelines to handle large datasets or real-time workflows can be resource-intensive.</li>
<li>Rigid processes may struggle to adapt to rapidly changing data needs.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Imagine a financial institution that extracts transaction data, transforms it into a consistent format by applying currency conversions and anomaly detection, and loads the cleaned data into a warehouse for fraud detection reports.</p>
</div>
</div>
</div>
</section>
<section id="extract-load-transform-elt" class="level3">
<h3 class="anchored" data-anchor-id="extract-load-transform-elt"><strong>Extract, Load, Transform (ELT)</strong></h3>
<p>The <strong>ELT</strong> paradigm, a more modern approach, inverts the transformation and loading steps:</p>
<ol type="1">
<li><strong>Extract</strong> data from various sources, often retaining its raw format.</li>
<li><strong>Load</strong> the extracted data directly into a scalable storage system, such as a data lake or cloud-based data warehouse.</li>
<li><strong>Transform</strong> the data within the storage system using its computational resources, tailoring transformations to specific analytical needs.</li>
</ol>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-elt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-elt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-elt">flowchart LR
    ob1(Raw data source) --&gt; id3[(Data storage)]
    ob2(Raw data source) --&gt; id3
    ob3(Raw data source) --&gt; id3
    id3 --&gt; processing[Data processing] --&gt; id3
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-elt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: In the ELT paradigm, the pipeline ingests data directly into the destination system and transforms it in parallel or after the fact.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In ELT workflows, the <strong>data ingestion</strong> phase emphasizes rapid loading of raw data into storage, enabling flexibility for later transformations. Also, by storing the data in its raw form, ELT better handles structured, unstructured, and semi-structured data and allows downstream analytics on this data to more easily adapt.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When to Use ELT
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>When leveraging cloud-native platforms like Snowflake, BigQuery, or AWS Redshift, which support high-speed storage and in-database transformations.</li>
<li>In workflows requiring rapid ingestion and the ability to adapt transformations for different use cases.</li>
<li>When storing raw data is essential for exploratory analysis or regulatory purposes.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Enables faster data ingestion by deferring transformations.</li>
<li>Supports diverse and flexible transformations tailored to specific analyses.</li>
<li>Scales well with large data volumes, leveraging modern storage and processing systems.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Challenges
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Requires advanced infrastructure to handle and process raw data efficiently.</li>
<li>Higher storage costs due to the retention of raw, unprocessed data.</li>
<li>Ad-hoc transformations can lead to inconsistencies if not governed properly.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An e-commerce platform ingests raw clickstream data into a cloud data warehouse. When needed, transformations such as sessionizing user behavior or aggregating purchase trends are applied directly within the warehouse for personalized recommendations or sales forecasting.</p>
</div>
</div>
</div>
</section>
<section id="comparing-etl-and-elt" class="level3">
<h3 class="anchored" data-anchor-id="comparing-etl-and-elt">Comparing ETL and ELT</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 38%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>ETL</th>
<th>ELT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Data Transformation</strong></td>
<td>Before loading into storage</td>
<td>After loading into storage</td>
</tr>
<tr class="even">
<td><strong>Storage Requirements</strong></td>
<td>Lower, as only processed data is stored</td>
<td>Higher, as raw data is retained</td>
</tr>
<tr class="odd">
<td><strong>Processing Time</strong></td>
<td>Slower ingestion due to upfront transformations</td>
<td>Faster ingestion with deferred transformations</td>
</tr>
<tr class="even">
<td><strong>Flexibility</strong></td>
<td>Limited; transformations are predefined</td>
<td>High; transformations can be ad hoc</td>
</tr>
<tr class="odd">
<td><strong>Infrastructure</strong></td>
<td>Suitable for legacy systems or traditional data warehouses</td>
<td>Ideal for modern, scalable systems</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Choosing the Right Paradigm
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>As highlighted in the previous chapter, the goals of DataOps — efficiency, reliability, and scalability — should guide the choice of ETL, ELT, or a hybrid approach. Consider:</p>
<ul>
<li><strong>ETL</strong> is well-suited for structured environments and use cases that demand immediate access to clean, well-processed data.</li>
<li><strong>ELT</strong> shines in cloud-native or big data ecosystems where raw data flexibility and scalability are critical.</li>
</ul>
<p>Both paradigms have their strengths, and many organizations blend elements of each. For example, a retail company might use ETL for compliance reporting while leveraging ELT for real-time inventory analysis. By understanding these paradigms within the broader DataOps framework, teams can design pipelines that meet both technical and business requirements effectively.</p>
</div>
</div>
</div>
</section>
</section>
<section id="tools-for-dataops" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="tools-for-dataops"><span class="header-section-number">4.3</span> Tools for DataOps</h2>
<p>Building effective and scalable data pipelines requires leveraging specialized tools at each stage of the DataOps lifecycle. From data ingestion to processing, validation, and versioning, the ecosystem of tools available is vast and continually expanding. This section provides an overview of commonly used tools, highlighting their capabilities, trade-offs, and when they might be most appropriate. Additionally, this space is dynamic, with new tools regularly introduced to address evolving challenges. Therefore, the goal is not to become tied to a specific tool but to understand the broader landscape and make informed decisions based on your pipeline’s requirements.</p>
<section id="data-ingestion-tools" class="level3">
<h3 class="anchored" data-anchor-id="data-ingestion-tools">Data Ingestion Tools</h3>
<p>Efficiently gathering data from diverse sources is the foundation of any pipeline. Ingestion tools vary from those tailored for real-time streaming to solutions focused on batch processing and ease of integration. Some examples include:</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://kafka.apache.org/">Apache Kafka</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A distributed event-streaming platform designed for high-throughput, real-time data ingestion. Kafka is widely used in applications like fraud detection, IoT data pipelines, and real-time analytics.</p>
<ul>
<li><strong>When to Use</strong>: Ideal for high-velocity, low-latency systems.</li>
<li><strong>Limitations</strong>: Steep learning curve and resource-heavy for smaller use cases.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://nifi.apache.org/">Apache NiFi</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A flow-based programming tool that automates data movement and transformation with a user-friendly interface. NiFi supports a wide range of data sources and formats.</p>
<ul>
<li><strong>When to Use</strong>: Low-code scenarios requiring rapid integration of multiple data sources.</li>
<li><strong>Limitations</strong>: Less suited for complex, high-throughput pipelines.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://github.com/airbytehq/airbyte">Airbyte</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An open-source data integration platform focused on moving data into data warehouses or lakes. Its extensive library of connectors simplifies ingesting data from various APIs and databases.</p>
<ul>
<li><strong>When to Use</strong>: Batch ingestion with diverse source compatibility.</li>
<li><strong>Limitations</strong>: Primarily batch-focused and may require configuration for real-time pipelines.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://www.fivetran.com/">Fivetran</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A managed data integration tool that simplifies data ingestion by automating schema handling and updates. It’s widely used for moving data into analytics-ready storage solutions.</p>
<ul>
<li><strong>When to Use</strong>: Enterprise scenarios requiring minimal management overhead for cloud data integration.</li>
<li><strong>Limitations</strong>: Subscription-based pricing and limited customization compared to open-source tools.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="data-processing-tools" class="level3">
<h3 class="anchored" data-anchor-id="data-processing-tools">Data Processing Tools</h3>
<p>Processing raw data into clean, structured, and feature-rich formats is critical to preparing it for machine learning workflows. Many tools exist but the following are a few popular tools that support various scales and complexities of data transformation.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://spark.apache.org/">Apache Spark</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A powerful engine for distributed data processing. Spark supports both batch and streaming data workflows, making it suitable for big data and ML pipelines.</p>
<ul>
<li><strong>When to Use</strong>: Processing large-scale data across distributed systems.</li>
<li><strong>Limitations</strong>: Requires expertise and infrastructure, which might be overkill for smaller datasets.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://www.getdbt.com/">DBT (Data Build Tool)</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A transformation tool that applies SQL to prepare data for analytics workflows. DBT is particularly useful for pipelines that involve data warehouses.</p>
<ul>
<li><strong>When to Use</strong>: SQL-centric environments where analytics-ready data is the end goal.</li>
<li><strong>Limitations</strong>: Focused on transformations within SQL databases; less versatile for non-SQL workflows.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://pola.rs/">Polars</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>An open source Python &amp; Rust library for data manipulation and analysis, offering rich functionality for handling structured data with performance in mind.</p>
<ul>
<li><strong>When to Use</strong>: Small to large datasets or for prototyping workflows. A great substitute for Pandas when performance and speed are required.</li>
<li><strong>Limitations</strong>: Not designed for real-time processing.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://www.prefect.io/">Prefect</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A workflow orchestration tool that enables robust scheduling and monitoring of data processing tasks.</p>
<ul>
<li><strong>When to Use</strong>: Complex pipelines requiring orchestration of multiple processing steps.</li>
<li><strong>Limitations</strong>: Adds an orchestration layer, which might be unnecessary for simple workflows.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="data-validation-tools" class="level3">
<h3 class="anchored" data-anchor-id="data-validation-tools">Data Validation Tools</h3>
<p>Ensuring the integrity and accuracy of data is essential to building reliable pipelines. Validation tools help detect anomalies, enforce schema compliance, and improve data quality. Some popular data validation tools include:</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://greatexpectations.io/">Great Expectations</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A framework for defining, executing, and documenting data validation checks. It integrates well with modern data stacks.</p>
<ul>
<li><strong>When to Use</strong>: Custom validation rules with automated reporting needs.</li>
<li><strong>Limitations</strong>: May require significant customization for non-standard checks.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://github.com/tensorflow/data-validation">TFDV (TensorFlow Data Validation)</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A library designed for detecting anomalies and validating schema in ML datasets.</p>
<ul>
<li><strong>When to Use</strong>: TensorFlow-based workflows or pipelines requiring statistical validation.</li>
<li><strong>Limitations</strong>: Limited applicability outside of TensorFlow-centric environments.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://github.com/awslabs/deequ">Deequ</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A library developed by AWS for validating large datasets using Spark. It focuses on automated quality checks and anomaly detection.</p>
<ul>
<li><strong>When to Use</strong>: Spark-based pipelines requiring scalable data quality checks.</li>
<li><strong>Limitations</strong>: Limited compatibility with non-Spark environments.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="data-versioning-tools" class="level3">
<h3 class="anchored" data-anchor-id="data-versioning-tools">Data Versioning Tools</h3>
<p>Versioning ensures traceability and reproducibility by tracking changes to datasets and workflows. It is vital for debugging, compliance, and collaboration. Some common data versioning tools include:</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://dvc.org/">DVC (Data Version Control)</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A Git-like tool for versioning data, models, and pipelines. It integrates seamlessly with Git repositories.</p>
<ul>
<li><strong>When to Use</strong>: Managing medium to large datasets in collaborative environments.</li>
<li><strong>Limitations</strong>: Can be challenging to set up for teams unfamiliar with Git.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://delta.io/">Delta Lake</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A storage layer that adds versioning and ACID transactions to data lakes. It works well with distributed systems like Apache Spark.</p>
<ul>
<li><strong>When to Use</strong>: Large-scale pipelines needing robust versioning and consistency.</li>
<li><strong>Limitations</strong>: Requires Spark for full functionality, adding complexity to smaller-scale workflows.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://lakefs.io/">LakeFS</a>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A Git-like version control system for data lakes. It enables branching and snapshotting of data workflows.</p>
<ul>
<li><strong>When to Use</strong>: Data lakes requiring advanced version control features.</li>
<li><strong>Limitations</strong>: Best suited for teams already working with data lakes.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="navigating-a-rapidly-evolving-ecosystem" class="level3">
<h3 class="anchored" data-anchor-id="navigating-a-rapidly-evolving-ecosystem">Navigating a Rapidly Evolving Ecosystem</h3>
<p>The DataOps tooling landscape is dynamic, with new tools and frameworks regularly introduced to address emerging challenges. This constant innovation provides opportunities to enhance workflows but also demands adaptability.</p>
<ul>
<li><strong>Focus on Principles</strong>: Rather than mastering specific tools, prioritize understanding the core concepts of data ingestion, processing, validation, and versioning. This flexibility allows you to evaluate and adopt new tools as they emerge.</li>
<li><strong>Experiment and Iterate</strong>: Allocate time for evaluating tools that may better align with your pipeline’s evolving needs. Open-source communities and GitHub repositories are excellent resources for exploration.</li>
<li><strong>Hybrid Tooling</strong>: Often, no single tool will address all requirements perfectly. Combining tools—such as using Airbyte for ingestion, DBT for processing, and Great Expectations for validation—creates tailored workflows.</li>
<li><strong>Leverage Community Support</strong>: Most tools provide extensive documentation, active forums, and integration guides. Engage with these communities to stay informed about updates and best practices.</li>
</ul>
<p>By focusing on the principles of DataOps and staying informed about the ever-changing ecosystem, teams can design resilient and adaptable pipelines. In the following sections, we’ll explore how to combine these tools into cohesive workflows and build scalable, efficient data pipelines for machine learning systems.</p>
</section>
</section>
<section id="hands-on-example-a-youtube-data-pipeline" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="hands-on-example-a-youtube-data-pipeline"><span class="header-section-number">4.4</span> Hands-On Example: A YouTube Data Pipeline</h2>
<p>Now that you’ve learned a bit about data pipelines, common paradigms, and tooling involved, let’s apply some of these concepts to create a simplified data pipeline that demonstrates how to ingest, process, and prepare YouTube data for downstream machine learning (ML) workflows. We’ll focus on collecting data like video titles, transcripts, views, likes, and comments. This example ties together concepts from previous chapters, including data ingestion, processing, validation, and versioning.</p>
<section id="pipeline-design" class="level3">
<h3 class="anchored" data-anchor-id="pipeline-design">Pipeline Design</h3>
<p>The pipeline design we’ll use will resemble more of an ETL process than an ELT. This is mainly for storage simplicity since this pipeline is run locally (whether that be on my machine or your machine) rather than in a cloud environment where storage capacity constraints are less of an issue.</p>
<p>We’ll use a couple of APIs to ingest our data, perform a little bit of data processing to clean and prepare our data for future analyses, and then illustrate some basic data validation along with versioning and storing our final prepared data.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-youtube-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-youtube-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-youtube-pipeline">flowchart LR
    subgraph ingest[Data Ingestion]
      direction LR
      subgraph p1[Ingest Video IDs]
      end
      subgraph p2[Ingest Video Stats]
      end
      subgraph p3[Ingest Video Transcript]
      end
    end
    p1 --&gt; p2
    p1 --&gt; p3
    ingest --&gt; process(Process Raw Data)
    process --&gt; Validate --&gt; Version --&gt; data[(Data storage)]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-youtube-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: High-level architecture of our data pipeline, which leverages an ETL process to ingest, process, validate, version and then store our data.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="basic-requirements" class="level3">
<h3 class="anchored" data-anchor-id="basic-requirements">Basic Requirements</h3>
<p>If you’d like to follow along and execute this code then there are a few requirements you’ll need on your end.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>You can find the requirements, source code for the data pipeline, and helper functions <a href="https://github.com/bradleyboehmke/uc-bana-7075/tree/main/DataOps">here</a>.</p>
</div>
</div>
</div>
<p>First, you’ll need to make sure you have the following Python libraries installed:</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-19">
<div id="cell-19" class="cell" data-tags="[&quot;dataops-rqmts&quot;]" data-execution_count="12">
<div class="cell-output cell-output-stdout">
<pre><code>Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]

dvc==3.59.0
great_expectations==1.3.1
jupyterlab==4.1.6
matplotlib==3.8.0
numpy==1.26.4
pandas&lt;=2.2
python-dotenv==0.21.0
tqdm==4.63.0
youtube_transcript_api==0.6.2</code></pre>
</div>
</div>
</div>
<p>Next, to simplify this example I have created helper functions to <a href="http://localhost:4761/01-intro-ml-system.html#modularity-and-abstraction">abstract</a> away a lot of the finer code details. You can see this in the following imports where I am importing helper functions from the <code>dataops_utils.py</code> module. If you want to reproduce this pipeline then you can download the dataops_utils.py script <a href="">here</a>.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-2">
<div id="cell-2" class="cell" data-tags="[&quot;requirements&quot;]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> great_expectations <span class="im">as</span> gx</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unicodedata</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataops_utils <span class="im">import</span> (</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    ingest_channel_video_ids,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    ingest_video_stats,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    ingest_video_transcript,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Lastly, We’ll be using the YouTube Data API to extract metadata and video information. To follow along, ensure you have:</p>
<ul>
<li>A Google Cloud account.</li>
<li>Access to the <a href="https://developers.google.com/youtube/v3">YouTube Data API</a> to include an API key. See <a href="https://developers.google.com/youtube/v3/getting-started">here to get started</a>.</li>
</ul>
<p>Once you have a Youtube API key then you’re ready to go! The next code chunk sets your API key and establishes the API URL and the channel ID. Note, I’m a golf junkie so the channel ID I am using is for <a href="https://www.youtube.com/@GrantHorvatGolfs">Grant Horvat</a>, a Youtube golfer with nearly 1 million followers.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-4">
<div id="cell-4" class="cell" data-tags="[&quot;pipeline-envars&quot;]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># I have my API key set as an environment variable</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>load_dotenv()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>API_KEY <span class="op">=</span> os.getenv(<span class="st">'YOUTUBE_API_KEY'</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># In your case you can add your API key here</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> API_KEY <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    API_KEY <span class="op">=</span> <span class="st">"INSERT_YOUR_YOUTUBE_API_KEY"</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>BASE_URL <span class="op">=</span> <span class="st">"https://www.googleapis.com/youtube/v3"</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>CHANNEL_ID <span class="op">=</span> <span class="st">'UCgUueMmSpcl-aCTt5CuCKQw'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="data-ingestion" class="level3">
<h3 class="anchored" data-anchor-id="data-ingestion">Data Ingestion</h3>
<p>The first step is to ingest the Youtube data. To do so, we need to follow three steps:</p>
<ol type="1">
<li>Ingest all the video IDs provided by a given channel.</li>
</ol>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-6">
<div id="cell-6" class="cell" data-tags="[&quot;ingest-video-ids&quot;]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ingest Youtube video IDs</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>video_ids <span class="op">=</span> ingest_channel_video_ids(API_KEY, CHANNEL_ID)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of what the first record looks like</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>video_ids[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>{'channel_id': 'UCgUueMmSpcl-aCTt5CuCKQw',
 'video_id': 'wzrIKGcOlsU',
 'datetime': '2025-01-13T17:00:24Z',
 'title': 'Rory McIlroy has another gear.'}</code></pre>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Use the ingested video IDs to ingest Youtube stats for each video such as total views, likes, and comments.</li>
</ol>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-7">
<div id="cell-7" class="cell" data-tags="[&quot;ingest-video-stats&quot;]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ingest Youtube video statistics</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>video_data <span class="op">=</span> ingest_video_stats(video_ids, API_KEY)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of the stats collected for the first video</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>video_data[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>{'channel_id': 'UCgUueMmSpcl-aCTt5CuCKQw',
 'video_id': 'wzrIKGcOlsU',
 'datetime': '2025-01-13T17:00:24Z',
 'title': 'Rory McIlroy has another gear.',
 'views': '3142',
 'likes': '304',
 'comments': '16'}</code></pre>
</div>
</div>
</div>
<ol start="3" type="1">
<li>And lastly, ingest the transcript for each video.</li>
</ol>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-8">
<div id="cell-8" class="cell" data-tags="[&quot;final-raw-data&quot;]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ingest Youtube video transcripts</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>video_data <span class="op">=</span> ingest_video_transcript(video_data)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of the final raw data that includes</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># video ID, title, date, stats, and transcript</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>video_data[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>{'channel_id': 'UCgUueMmSpcl-aCTt5CuCKQw',
 'video_id': 'wzrIKGcOlsU',
 'datetime': '2025-01-13T17:00:24Z',
 'title': 'Rory McIlroy has another gear.',
 'views': '3142',
 'likes': '304',
 'comments': '16',
 'transcript': "I've never seen you go after the ball like this there it is wow that was it that was so good 190 190 nice that was hit well good what was that 72 as well that was good that was nice that was FL that was nice 127 A2 191 that's gone 34 there you go"}</code></pre>
</div>
</div>
</div>
</section>
<section id="data-processing" class="level3">
<h3 class="anchored" data-anchor-id="data-processing">Data Processing</h3>
<p>For data preprocessing, we’re going to first convert our data to a Pandas DataFrame</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-10">
<div id="cell-10" class="cell" data-tags="[&quot;data-preprocess-dataframe&quot;]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> pd.DataFrame(video_data)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>raw_data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">channel_id</th>
<th data-quarto-table-cell-role="th">video_id</th>
<th data-quarto-table-cell-role="th">datetime</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">views</th>
<th data-quarto-table-cell-role="th">likes</th>
<th data-quarto-table-cell-role="th">comments</th>
<th data-quarto-table-cell-role="th">transcript</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>wzrIKGcOlsU</td>
<td>2025-01-13T17:00:24Z</td>
<td>Rory McIlroy has another gear.</td>
<td>3142</td>
<td>304</td>
<td>16</td>
<td>I've never seen you go after the ball like thi...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>Pz42jFngEzM</td>
<td>2025-01-13T03:25:25Z</td>
<td>Thank you. 1 Million ❤️</td>
<td>59474</td>
<td>6757</td>
<td>428</td>
<td>[Music] so sick let's go [Music]</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>fSHh01YT0-Q</td>
<td>2025-01-07T18:50:32Z</td>
<td>Tiger Woods hits the ball off the heel.</td>
<td>64116</td>
<td>2055</td>
<td>30</td>
<td>over the course of my career I've always hit t...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>erzLT7fy2r0</td>
<td>2025-01-07T17:56:07Z</td>
<td>Tiger Woods liked my golf swing!</td>
<td>122584</td>
<td>4486</td>
<td>76</td>
<td>what's wrong with that yeah that came off you ...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>3O08SnyZ88U</td>
<td>2025-01-07T17:07:04Z</td>
<td>Tiger Woods teaches me how to hit it straight!</td>
<td>555884</td>
<td>18286</td>
<td>142</td>
<td>what did you do in your career when you had a ...</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>And then clean our data by:</p>
<ol type="1">
<li><strong>Removing missing values</strong>. There are a few videos with no transcript text because the videos have no talking in them.</li>
<li><strong>Removing duplicate observations</strong>. Just in case there is duplication in video information during the downloading process.</li>
<li><strong>Remove any inconsistent data types</strong>. This avoids errors during data processing and ensures consistency in operations applied to the data.</li>
<li><strong>Remove any observations that have invalid datetime values</strong>. This ensures chronological accuracy for any time-based analysis or trends.</li>
<li><strong>Remove any videos with minimal number of views</strong>. This filters out content that may not provide enough engagement data for meaningful insights.</li>
<li><strong>Remove any videos with very little transcript text</strong>. This ensures that the remaining data contains sufficient content for natural language processing or text-based analysis.</li>
<li><strong>Clean the title and transcript text</strong>. This removes unnecessary noise, such as non-character string values (i.e.&nbsp;unicode characters), making the text suitable for analysis.</li>
</ol>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-11">
<div id="cell-11" class="cell" data-tags="[&quot;data-preprocess-clean&quot;]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove rows with missing data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>cleaned_data <span class="op">=</span> raw_data.dropna()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove duplicate rows</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>cleaned_data <span class="op">=</span> cleaned_data.drop_duplicates()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove any inconsistent data types</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> [<span class="st">'views'</span>, <span class="st">'likes'</span>, <span class="st">'comments'</span>]:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    cleaned_data[col] <span class="op">=</span> pd.to_numeric(cleaned_data[col], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove any observations that have invalid datetime values</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>cleaned_data[<span class="st">'datetime'</span>] <span class="op">=</span> pd.to_datetime(cleaned_data[<span class="st">'datetime'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>cleaned_data <span class="op">=</span> cleaned_data.dropna(subset<span class="op">=</span>[<span class="st">'datetime'</span>])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove any observations where the views value is less than 3 standard deviations</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># from the mean</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>mean_views <span class="op">=</span> cleaned_data[<span class="st">'views'</span>].mean()</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>std_views <span class="op">=</span> cleaned_data[<span class="st">'views'</span>].std()</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>cleaned_data <span class="op">=</span> cleaned_data[cleaned_data[<span class="st">'views'</span>] <span class="op">&gt;=</span> (mean_views <span class="op">-</span> <span class="dv">3</span> <span class="op">*</span> std_views)]</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove any observations where the transcript length is less than 3 standard deviations</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># from the mean transcript length</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>cleaned_data[<span class="st">'transcript_length'</span>] <span class="op">=</span> cleaned_data[<span class="st">'transcript'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x) <span class="cf">if</span> pd.notnull(x) <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>mean_transcript_length <span class="op">=</span> cleaned_data[<span class="st">'transcript_length'</span>].mean()</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>std_transcript_length <span class="op">=</span> cleaned_data[<span class="st">'transcript_length'</span>].std()</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>cleaned_data <span class="op">=</span> cleaned_data[cleaned_data[<span class="st">'transcript_length'</span>] <span class="op">&gt;=</span> (mean_transcript_length <span class="op">-</span> <span class="dv">3</span> <span class="op">*</span> std_transcript_length)]</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove/clean the title and transcript columns for non-character string values</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co"># (i.e. unicode characters)</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text):</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(text, <span class="bu">str</span>):</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> unicodedata.normalize(<span class="st">'NFKD'</span>, text).encode(<span class="st">'ascii'</span>, <span class="st">'ignore'</span>).decode(<span class="st">'ascii'</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>cleaned_data[<span class="st">'title'</span>] <span class="op">=</span> cleaned_data[<span class="st">'title'</span>].<span class="bu">apply</span>(clean_text)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>cleaned_data[<span class="st">'transcript'</span>] <span class="op">=</span> cleaned_data[<span class="st">'transcript'</span>].<span class="bu">apply</span>(clean_text)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>cleaned_data.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">channel_id</th>
<th data-quarto-table-cell-role="th">video_id</th>
<th data-quarto-table-cell-role="th">datetime</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">views</th>
<th data-quarto-table-cell-role="th">likes</th>
<th data-quarto-table-cell-role="th">comments</th>
<th data-quarto-table-cell-role="th">transcript</th>
<th data-quarto-table-cell-role="th">transcript_length</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>wzrIKGcOlsU</td>
<td>2025-01-13 17:00:24+00:00</td>
<td>Rory McIlroy has another gear.</td>
<td>3142</td>
<td>304</td>
<td>16</td>
<td>I've never seen you go after the ball like thi...</td>
<td>246</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>Pz42jFngEzM</td>
<td>2025-01-13 03:25:25+00:00</td>
<td>Thank you. 1 Million</td>
<td>59474</td>
<td>6757</td>
<td>428</td>
<td>[Music] so sick let's go [Music]</td>
<td>32</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>fSHh01YT0-Q</td>
<td>2025-01-07 18:50:32+00:00</td>
<td>Tiger Woods hits the ball off the heel.</td>
<td>64116</td>
<td>2055</td>
<td>30</td>
<td>over the course of my career I've always hit t...</td>
<td>483</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>erzLT7fy2r0</td>
<td>2025-01-07 17:56:07+00:00</td>
<td>Tiger Woods liked my golf swing!</td>
<td>122584</td>
<td>4486</td>
<td>76</td>
<td>what's wrong with that yeah that came off you ...</td>
<td>208</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>UCgUueMmSpcl-aCTt5CuCKQw</td>
<td>3O08SnyZ88U</td>
<td>2025-01-07 17:07:04+00:00</td>
<td>Tiger Woods teaches me how to hit it straight!</td>
<td>555884</td>
<td>18286</td>
<td>142</td>
<td>what did you do in your career when you had a ...</td>
<td>646</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>While there are many advanced techniques for preprocessing text data—such as more thorough cleaning, lemmatization, or converting text into embeddings for model input—the intent of this section is to focus on building a simplified example of an end-to-end data pipeline. These additional steps can significantly enhance the quality and utility of text data, but they are outside the scope of this example, which is designed to highlight the core concepts and practical steps involved in creating a data pipeline.</p>
</div>
</div>
</div>
</section>
<section id="data-validation" class="level3">
<h3 class="anchored" data-anchor-id="data-validation">Data Validation</h3>
<p>Next, we’ll validate our data. To do so we’ll use <a href="https://greatexpectations.io/">Great Expectations</a> and, first, we need to create a data and define our data assets.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-13">
<div id="cell-13" class="cell" data-tags="[&quot;validate-setup&quot;]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Data Context.</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> gx.get_context()</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Data Source, Data Asset, Batch Definition, and Batch.</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>data_source <span class="op">=</span> context.data_sources.add_pandas(<span class="st">"pandas"</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>data_asset <span class="op">=</span> data_source.add_dataframe_asset(name<span class="op">=</span><span class="st">"Youtube video data"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>batch_definition <span class="op">=</span> data_asset.add_batch_definition_whole_dataframe(<span class="st">"batch definition"</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> batch_definition.get_batch(batch_parameters<span class="op">=</span>{<span class="st">"dataframe"</span>: cleaned_data})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Now that the infrastructure is set up, we’ll go through a process of validating that our data meets certain expectations. This code defines a data validation suite to ensure the integrity of our cleaned YouTube dataset. It consists of three primary validation steps:</p>
<ol type="1">
<li><p><strong>Column Existence Validation</strong>: It verifies that all required columns (<code>channel_id</code>, <code>video_id</code>, <code>datetime</code>, <code>title</code>, <code>views</code>, <code>likes</code>, <code>comments</code>, <code>transcript</code>, and <code>transcript_length</code>) are present in the dataset. This step ensures that the essential structure of the dataset is intact.</p></li>
<li><p><strong>Data Type Validation</strong>: It checks that each column contains values of the expected data type, such as <code>Object</code> for textual data (<code>channel_id</code>, <code>video_id</code>, <code>title</code>, and <code>transcript</code>), <code>Timestamp</code> for date-related fields, and <code>int64</code> for numerical fields (<code>views</code>, <code>likes</code>, <code>comments</code>, and <code>transcript_length</code>). This ensures that data types align with the intended use of each column.</p></li>
<li><p><strong>Null Value Validation</strong>: It confirms that no empty or null values exist in the critical columns. This guarantees that the dataset is complete and avoids errors caused by missing data in downstream processes.</p></li>
</ol>
<p>Finally, the code executes the validation suite against a data batch and prints whether all validation checks were successful. This ensures the dataset meets the defined quality standards before being used in the data pipeline.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-14">
<div id="cell-14" class="cell" data-tags="[&quot;validate-data&quot;]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an Expectation Suite</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>suite <span class="op">=</span> gx.ExpectationSuite(name<span class="op">=</span><span class="st">"Youtube video data expectations"</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the Expectation Suite to the Data Context</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>suite <span class="op">=</span> context.suites.add(suite)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Validate columns exist</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'channel_id'</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'video_id'</span>))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'datetime'</span>))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'title'</span>))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'views'</span>))</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'likes'</span>))</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'comments'</span>))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'transcript'</span>))</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnToExist(column<span class="op">=</span><span class="st">'transcript_length'</span>))</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Validate data types</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'channel_id'</span>, type_<span class="op">=</span><span class="st">"object"</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'video_id'</span>, type_<span class="op">=</span><span class="st">"object"</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'datetime'</span>, type_<span class="op">=</span><span class="st">"Timestamp"</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'title'</span>, type_<span class="op">=</span><span class="st">"object"</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'views'</span>, type_<span class="op">=</span><span class="st">"int64"</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'likes'</span>, type_<span class="op">=</span><span class="st">"int64"</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'comments'</span>, type_<span class="op">=</span><span class="st">"int64"</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'transcript'</span>, type_<span class="op">=</span><span class="st">"object"</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToBeOfType(</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>    column<span class="op">=</span><span class="st">'transcript_length'</span>, type_<span class="op">=</span><span class="st">"int64"</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Validate no empty values exist</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'channel_id'</span>))</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'video_id'</span>))</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'datetime'</span>))</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'title'</span>))</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'views'</span>))</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'likes'</span>))</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'comments'</span>))</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'transcript'</span>))</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column<span class="op">=</span><span class="st">'transcript_length'</span>))</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Validate results</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>validation_results <span class="op">=</span> batch.validate(suite)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(validation_results.success)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bdda22c8821490f90255ceae3cb659a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>True</code></pre>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>The validation checks implemented here represent just a small subset of the capabilities available with Great Expectations. This powerful tool provides a wide array of validation checks, enabling you to ensure data quality at a much deeper level. For instance, you can validate the distribution of values to check for expected statistical patterns, ensure cardinality constraints to avoid duplicate or excessive values, or verify that data adheres to specific patterns (e.g., email formats or numeric ranges). These additional validations can further enhance the robustness and reliability of your data pipelines. Explore the full range of options here: Great Expectations Documentation - Expectations.</p>
</div>
</div>
</div>
</section>
<section id="data-versioning" class="level3">
<h3 class="anchored" data-anchor-id="data-versioning">Data Versioning</h3>
<p>For our purposes, we’re going to store the final <code>cleaned_data</code> to our project directory and then we’re going to use <a href="https://dvc.org/">DVC</a> (Data Version Control) to version and track changes to our data.</p>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>DVC assumes you are working within a <strong>Git repository</strong>, as it relies on Git for tracking the metadata of your data files and pipelines.</p>
<ul>
<li><p><strong>If you’re familiar with Git</strong>: You can follow along with this example as long as you are working within a Git repository. Make sure you have initialized a Git repository (<code>git init</code>) in your project directory and have basic knowledge of Git commands.</p></li>
<li><p><strong>If Git is new to you</strong>: Don’t worry! You can still follow along to understand the process conceptually, even if you don’t execute the Git commands. Later chapters in this book will provide a deeper dive into Git, equipping you with the knowledge to implement version control effectively.</p></li>
</ul>
<p>For now, focus on understanding how DVC integrates with data pipelines to manage and version datasets systematically.</p>
</div>
</div>
</div>
<p>So, first, we’ll store our final data as a parquet file, which is just a more efficient approach than storing as a CSV file.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/DataOps/youtube-data-pipeline.ipynb" data-notebook-title="Example Youtube Data Pipeline" data-notebook-cellid="cell-16">
<div id="cell-16" class="cell" data-tags="[&quot;store-clean-data&quot;]" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the directory exists</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>os.makedirs(<span class="st">'data'</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the cleaned data to a parquet file</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>cleaned_data.to_parquet(<span class="st">'data/youtube_video_data.parquet'</span>, index<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Next, we need to initialize DVC in our project by running the following command in the project root.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">dvc</span> init</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This initializes a .dvc directory with a few internal files to track data and pipeline versions. These should be added to Git.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> status</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Changes</span> to be committed:</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="ex">new</span> file:   .dvc/.gitignore</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="ex">new</span> file:   .dvc/config</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        <span class="ex">...</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> commit <span class="at">-m</span> <span class="st">"Initialize DVC"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, you need to use <code>dvc add</code> to start tracking the dataset file. This command creates a <code>.dvc</code> file (youtube_video_data.parquet.dvc) to track the file’s metadata and location. It also adds a <code>.gitignore</code> file so that the actual .parquet data file is not committed to git; instead the .parquet.dvc metadata file is committed.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> add data/youtube_video_data.parquet.dvc data/.gitignore</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, run the following commands to track and tag the dataset changes in Git.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> commit <span class="at">-m</span> <span class="st">'Initial processed Youtube data'</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> tag <span class="at">-a</span> <span class="st">"v1.0"</span> <span class="at">-m</span> <span class="st">"Youtube data v1.0"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you have a remote storage location (e.g., AWS S3, Google Drive, Azure Blob Storage) configured for DVC, you can also push the data asset to that location to ensure it is stored offsite and sharable to the rest of your team/organization.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">dvc</span> remote add <span class="at">-d</span> myremote s3://mybucket/myproject</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="ex">dvc</span> push</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, say we re-run this pipeline next week and get additional video data. We can just follow this same procedure to save and version the updated data:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Write the cleaned data to a parquet file</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="ex">cleaned_data.to_parquet</span><span class="er">(</span><span class="st">'data/youtube_video_data.parquet'</span><span class="ex">,</span> index=False<span class="kw">)</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Track the updated file</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> add data/youtube_video_data.parquet.dvc</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Commit the changes</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> commit <span class="at">-m</span> <span class="st">'Updated Youtube data'</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> tag <span class="at">-a</span> <span class="st">"v2.0"</span> <span class="at">-m</span> <span class="st">"Youtube data v2.0"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Our example demonstrates only the <strong>basic functionality</strong> of DVC, showcasing how it can be used to version datasets within a pipeline. However, DVC offers a wide range of additional features, including:</p>
<ul>
<li>Tracking and managing <strong>entire pipelines</strong>.</li>
<li>Handling <strong>large datasets</strong> efficiently with external storage integrations.</li>
<li>Automating <strong>experiment tracking</strong> and comparisons.</li>
</ul>
<p>To explore the full potential of DVC and learn how to leverage its advanced capabilities, check out the official documentation at <a href="https://dvc.org/doc">https://dvc.org/doc</a>. This resource provides comprehensive guidance and examples for incorporating DVC into robust, end-to-end workflows.</p>
</div>
</div>
</div>
</section>
</section>
<section id="creating-reliable-scalable-data-pipelines" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="creating-reliable-scalable-data-pipelines"><span class="header-section-number">4.5</span> Creating Reliable &amp; Scalable Data Pipelines</h2>
<p>Designing reliable and scalable data pipelines requires adherence to the <a href="https://bradleyboehmke.github.io/uc-bana-7075/01-intro-ml-system.html#design-principles-for-good-ml-system-design">foundational principles</a> that we discussed that promote maintainability, reproducibility, and efficiency. The hands-on example of the YouTube data pipeline demonstrates some of these design principles; however, there are areas where improvements could further align the pipeline with these principles.</p>
<section id="principles-incorporated-in-the-youtube-data-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="principles-incorporated-in-the-youtube-data-pipeline">Principles Incorporated in the YouTube Data Pipeline</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Modularity and Abstraction
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>What We Did</strong>: The pipeline uses a modular design by encapsulating reusable functions within the <code>dataops_utils</code> module. This abstraction reduces complexity and makes the codebase more manageable, enabling easier updates and debugging.</li>
<li><strong>Why It Matters</strong>: Modularity ensures that individual components, such as data cleaning, validation, and versioning, are independently testable and maintainable. Abstraction allows users to focus on higher-level logic without worrying about low-level details.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reproducibility
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>What We Did</strong>: By incorporating Git for version control and DVC for dataset tracking, the pipeline ensures that every step—from data ingestion to cleaning—can be reproduced with consistent results.</li>
<li><strong>Why It Matters</strong>: Reproducibility is essential for debugging, auditing, and collaboration, especially when multiple team members work on the same project or when retraining models on historical data.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data Validation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>What We Did</strong>: The pipeline integrates <strong>Great Expectations</strong> to validate the structure and quality of the dataset. This ensures that the data meets predefined criteria before it is processed further.</li>
<li><strong>Why It Matters</strong>: Validation prevents poor-quality data from contaminating downstream workflows, thereby enhancing reliability and trust in the pipeline.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="limitations-and-areas-for-improvement" class="level3">
<h3 class="anchored" data-anchor-id="limitations-and-areas-for-improvement">Limitations and Areas for Improvement</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Scalability
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Current Challenge</strong>: The YouTube API imposes daily quota limits on data retrieval, constraining the volume of data that can be ingested. This makes the pipeline less scalable for large-scale applications.</li>
<li><strong>Potential Solution</strong>: To overcome this limitation, consider implementing data caching strategies, batching requests over multiple days, or leveraging additional API keys across multiple accounts to distribute the load.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-28-contents" aria-controls="callout-28" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Automation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-28" class="callout-28-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Current Challenge</strong>: The pipeline is not automated and requires manual execution for each step. This limits its reliability and scalability in production environments.</li>
<li><strong>Potential Solution</strong>: Incorporate workflow automation tools such as Apache Airflow or Prefect to schedule and monitor the pipeline. Automating these tasks would enhance reliability and reduce manual intervention.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Fault Tolerance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Current Challenge</strong>: The pipeline lacks mechanisms to handle failures gracefully, such as retrying failed API calls or dealing with incomplete datasets.</li>
<li><strong>Potential Solution</strong>: Introduce error-handling routines and logging mechanisms to ensure that the pipeline can recover from failures without disrupting downstream processes.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Scalable Storage and Compute
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Current Challenge</strong>: The pipeline is designed for small-scale use and does not leverage distributed computing or scalable storage solutions.</li>
<li><strong>Potential Solution</strong>: Transition to cloud-based storage systems like AWS S3 or Google Cloud Storage and integrate distributed computing frameworks such as Apache Spark for handling large datasets.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="balancing-design-principles" class="level3">
<h3 class="anchored" data-anchor-id="balancing-design-principles">Balancing Design Principles</h3>
<p>While the YouTube data pipeline incorporates key design principles such as modularity, abstraction, and reproducibility, there are trade-offs due to its simplicity. The primary objective of this example was to illustrate the foundational steps in building a data pipeline, rather than creating a production-ready system. For real-world applications, enhancing scalability, automation, and fault tolerance would be critical to align fully with the design principles of reliable and scalable systems.</p>
<p>By continuously iterating on these principles, teams can evolve simple pipelines into robust, production-ready systems capable of handling complex, large-scale workflows.</p>
</section>
</section>
<section id="summary" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">4.6</span> Summary</h2>
<p>In this chapter, we explored the practical aspects of building data pipelines to support robust machine learning workflows. By combining the principles and concepts introduced in the previous chapter, we demonstrated how to design, implement, and validate a data pipeline using tools like Pandas, Great Expectations, and DVC. The YouTube data pipeline provided a hands-on example of how data ingestion, processing, validation, and versioning can come together to create a cohesive system.</p>
<p>We also compared the ETL and ELT paradigms, highlighting their applications and trade-offs, and discussed how the design principles for good ML systems—such as modularity, abstraction, and reproducibility—can guide the creation of reliable and scalable pipelines. While the example illustrated foundational techniques, it also highlighted areas for improvement, such as addressing scalability and automation challenges.</p>
<p>This chapter aimed to provide you with the foundational knowledge and tools to build data pipelines tailored to your needs. As you advance, you’ll encounter more complex requirements and additional tools to refine and scale your workflows. In the next chapter</p>
</section>
<section id="exercise" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="exercise"><span class="header-section-number">4.7</span> Exercise</h2>
<p>This exercise will help you apply the concepts covered in this chapter to design and evaluate a data pipeline. The focus is on understanding the design principles, tools, and processes involved in building reliable and scalable data pipelines.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-31-contents" aria-controls="callout-31" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 1: Conceptual Design
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-31" class="callout-31-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Scenario</strong>: Imagine you are tasked with building a data pipeline for an online retail store. The pipeline should:</p>
<ul>
<li>Ingest daily sales data from a transactional database and inventory updates from an API.</li>
<li>Process the data to calculate daily revenue and identify low-stock items.</li>
<li>Validate the data for missing or inconsistent records.</li>
<li>Version the processed data for auditing and historical tracking.</li>
</ul>
<p><strong>Task</strong>:</p>
<ul>
<li>Sketch a conceptual design for this pipeline. Include the steps for ingestion, processing, validation, and versioning.</li>
<li>Identify which tools from this chapter (e.g., Pandas, Great Expectations, DVC) you would use for each step and justify your choice.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 2: Hands-On Experimentation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using the YouTube pipeline example as a reference, answer the following:</p>
<ol type="1">
<li><strong>Modify the Pipeline</strong>:
<ul>
<li>Extend the provided YouTube pipeline to include additional validation checks (e.g., ensure <code>views</code> is greater than <code>likes</code> or that <code>transcript_length</code> is within a realistic range).</li>
<li>What changes did you make to the validation suite, and why?</li>
</ul></li>
<li><strong>Experiment with Versioning</strong>:
<ul>
<li>Create a new version of the <code>cleaned_data</code> DataFrame by simulating a change in the input data (e.g., remove videos with fewer than 10,000 views).</li>
<li>Use DVC to version the new dataset and compare it to the original version. What do the differences reveal?</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-33-contents" aria-controls="callout-33" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 3: Reflecting on Design Principles
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-33" class="callout-33-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Evaluate the Pipeline</strong>:
<ul>
<li>Review the YouTube pipeline example and your modified pipeline. How well do they incorporate the design principles from Chapter 1 (e.g., modularity, scalability, reproducibility)?</li>
<li>Identify one principle that could be improved in your pipeline design. How would you address this in a future iteration?</li>
</ul></li>
<li><strong>Scenario-Based Discussion</strong>:
<ul>
<li>Imagine that the YouTube API introduces stricter <a href="https://developers.google.com/youtube/v3/guides/quota_and_compliance_audits">quota limits</a>. How would this impact the pipeline? Propose a solution to handle such constraints while ensuring the pipeline remains functional and reliable.</li>
</ul></li>
</ol>
</div>
</div>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Feel free to explore a different Youtube channel. Note that the channel ID is different than the Youtube handle. For example, Grant Horvat’s Youtube handle is <code>@GrantHorvatGolfs</code> but his channel ID is UCgUueMmSpcl-aCTt5CuCKQw. The easiest way to find a channel’s ID is to go to the channel metadata where information for listed for “About”, “Links”, and “Channel details”. At the bottom of the pop up window is an option to “Share Channel” and then “Copy Channel ID”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-dataops-role.html" class="pagination-link" aria-label="The Role of DataOps">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Role of DataOps</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-modelops-role.html" class="pagination-link" aria-label="The Role of ModelOps">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Role of ModelOps</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/edit/main/04-dataops-build.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>