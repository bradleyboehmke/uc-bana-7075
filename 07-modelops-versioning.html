<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; Model Versioning and Reproducibility – Machine Learning Design for Business</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./08-modelops-deployment.html" rel="next">
<link href="./06-modelops-experimenting.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-66ab7fd5e73b7f0a764e0d49b3e29ab1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-72c93205fb41c15e2e398b4f9a505ee2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="mermaid-theme" content="neutral">
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./07-modelops-versioning.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Model Versioning and Reproducibility</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning Design for Business</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/bradleyboehmke/uc-bana-7075" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-Design-for-Business.epub" title="Download ePub" class="quarto-navigation-tool px-1" aria-label="Download ePub"><i class="bi bi-journal"></i></a>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Laying the Foundation</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-ml-system.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">ML System Design</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-before-we-build.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Before We Build</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">DataOps</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-dataops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Role of DataOps</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dataops-build.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Putting DataOps into Practice</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">ModelOps</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-modelops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Role of ModelOps</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-modelops-experimenting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model Training and Experiment Tracking</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-modelops-versioning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Model Versioning and Reproducibility</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-modelops-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">DevOps</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">The Human Side of ML Systems</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-need-for-model-versioning" id="toc-the-need-for-model-versioning" class="nav-link active" data-scroll-target="#the-need-for-model-versioning"><span class="header-section-number">7.1</span> The Need for Model Versioning</a>
  <ul class="collapse">
  <li><a href="#challenges-of-managing-multiple-models" id="toc-challenges-of-managing-multiple-models" class="nav-link" data-scroll-target="#challenges-of-managing-multiple-models">Challenges of Managing Multiple Models</a></li>
  <li><a href="#versioning-for-traceability-and-consistency" id="toc-versioning-for-traceability-and-consistency" class="nav-link" data-scroll-target="#versioning-for-traceability-and-consistency">Versioning for Traceability and Consistency</a></li>
  <li><a href="#examples-of-when-model-versioning-is-essential" id="toc-examples-of-when-model-versioning-is-essential" class="nav-link" data-scroll-target="#examples-of-when-model-versioning-is-essential">Examples of When Model Versioning is Essential</a></li>
  </ul></li>
  <li><a href="#key-components-of-model-versioning" id="toc-key-components-of-model-versioning" class="nav-link" data-scroll-target="#key-components-of-model-versioning"><span class="header-section-number">7.2</span> Key Components of Model Versioning</a></li>
  <li><a href="#tools-for-model-versioning-and-reproducibility" id="toc-tools-for-model-versioning-and-reproducibility" class="nav-link" data-scroll-target="#tools-for-model-versioning-and-reproducibility"><span class="header-section-number">7.3</span> Tools for Model Versioning and Reproducibility</a></li>
  <li><a href="#sec-model-version-example" id="toc-sec-model-version-example" class="nav-link" data-scroll-target="#sec-model-version-example"><span class="header-section-number">7.4</span> Hands-On Example: Implementing Model Versioning</a>
  <ul class="collapse">
  <li><a href="#setting-the-stage" id="toc-setting-the-stage" class="nav-link" data-scroll-target="#setting-the-stage">Setting the Stage</a></li>
  <li><a href="#registering-models" id="toc-registering-models" class="nav-link" data-scroll-target="#registering-models">Registering Models</a></li>
  <li><a href="#querying-registered-models" id="toc-querying-registered-models" class="nav-link" data-scroll-target="#querying-registered-models">Querying Registered Models</a></li>
  <li><a href="#reproducibility-of-registered-model" id="toc-reproducibility-of-registered-model" class="nav-link" data-scroll-target="#reproducibility-of-registered-model">Reproducibility of Registered Model</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">7.5</span> Summary</a></li>
  <li><a href="#sec-model-version-exercise" id="toc-sec-model-version-exercise" class="nav-link" data-scroll-target="#sec-model-version-exercise"><span class="header-section-number">7.6</span> Exercise</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/edit/main/07-modelops-versioning.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-model-versioning-chapter" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Model Versioning and Reproducibility</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>As machine learning systems become more integral to business operations, the need for reliable, traceable, and maintainable workflows grows exponentially. Model versioning and reproducibility lie at the heart of achieving this reliability, forming the foundation for robust ModelOps practices. These concepts ensure that teams can iterate on models, track their evolution, and reproduce results across different environments and timelines, even in the face of complex pipelines and evolving data.</p>
<p>In the previous chapter, we explored the importance of <strong>experiment tracking</strong>, which enables teams to document the evolution of models by logging hyperparameters, datasets, metrics, and results for each experiment. While experiment tracking provides a granular view of individual runs and their outcomes, <strong>model versioning</strong> builds on this by offering a systematic way to manage and track the lifecycle of fully trained models. Together, these practices ensure that the best-performing models identified during experimentation can be efficiently deployed, monitored, and revisited when necessary.</p>
<p>Model versioning involves systematically managing multiple iterations of machine learning models, capturing changes in architectures, hyperparameters, training data, and evaluation metrics. By maintaining clear records of model versions, teams can track progress, compare performance, and ensure seamless handoffs between development and deployment stages. For example, versioning allows data scientists to confidently deploy the best-performing models identified during the experimentation phase while maintaining the ability to revert to earlier versions if needed.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-ml-versioning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-versioning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-ml-versioning">flowchart TB
    subgraph Experiment 3
    direction BT
    m1[Model Run 3.1]
    m2[Model Run 3.2]
    m3[Model Run 3.3]
    end
    subgraph Experiment 2
    direction BT
    m4[Model Run 2.1]
    m5[Model Run 2.2]
    m6[Model Run 2.3]
    end
    subgraph Experiment 1
    direction BT
    m7[Model Run 1.1]
    m8[Model Run 1.2]
    m9[Model Run 1.3]
    end
    m8 -- 'Best model' --&gt; v1[Model v1.0]
    m4 -- 'Best model' --&gt; v2[Model v1.1]
    m3 -- 'Best model' --&gt; v3[Model v1.2]
    subgraph Versioned Models
    direction BT
    v1
    v2
    v3
    end
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-versioning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.1: Model versioning builds on model experiment tracking by offering a systematic way to manage, track, and version models that we move through the lifecycle (typically models that will move from development into production).
</figcaption>
</figure>
</div>
</div>
</div>
<p>Reproducibility ensures that experiments and results can be reliably recreated, which complements model versioning by providing consistency across environments. This requires capturing all the components that influence model training, including data versions, preprocessing steps, random seeds, and dependencies. Together, versioning and reproducibility are critical for debugging, auditing, and building trust in machine learning systems. Without them, even minor discrepancies in results can lead to significant challenges in troubleshooting and compliance.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-ml-reproducibility" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-reproducibility-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-ml-reproducibility">flowchart BT
    subgraph r3[Requirements]
    direction RL
    m1[data]
    m2[features]
    m3[hyperparameters]
    m4[random seeds]
    m5[dependencies]
    end
    subgraph r2[Requirements]
    direction LR
    m6[data]
    m7[features]
    m8[hyperparameters]
    m9[random seeds]
    m10[dependencies]
    end
    subgraph r1[Requirements]
    direction LR
    m11[data]
    m12[features]
    m13[hyperparameters]
    m14[random seeds]
    m15[dependencies]
    end
    r1 ---&gt; v1[Model v1.0]
    r2 ---&gt; v2[Model v1.1]
    r3 ---&gt; v3[Model v1.2]
    subgraph Versioned Models
    direction BT
    v1
    v2
    v3
    end
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-reproducibility-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7.2: Reproducibility complements model versioning by capturing all the components that the model depends on such as data versions, preprocessing steps, random seeds, environment dependencies, etc.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This chapter builds on the previous chapter to demonstrate how model versioning and reproducibility come to life in a real-world scenario. By integrating the principles of experiment tracking with robust versioning practices, we will illustrate how to maintain a transparent and scalable workflow. By the end of this chapter, you will understand how model versioning and reproducibility complement experiment tracking and learn to implement these practices effectively in your machine learning projects.</p>
<section id="the-need-for-model-versioning" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="the-need-for-model-versioning"><span class="header-section-number">7.1</span> The Need for Model Versioning</h2>
<p>In the rapidly evolving field of machine learning, managing models is no small feat. The lifecycle of a machine learning model is iterative, often requiring multiple rounds of experimentation, updates, and deployments. Without a structured approach to versioning, this complexity can lead to challenges in maintaining consistency, ensuring reproducibility, and managing multiple iterations simultaneously deployed across various business processes.</p>
<section id="challenges-of-managing-multiple-models" class="level3">
<h3 class="anchored" data-anchor-id="challenges-of-managing-multiple-models">Challenges of Managing Multiple Models</h3>
<p>Machine learning systems rarely rely on a single model. Instead, organizations frequently manage multiple models, each tailored to specific use cases, business processes, or datasets. For example, an e-commerce platform may deploy separate models for product recommendations, dynamic pricing, and customer churn prediction. Additionally, even within a single use case, data scientists often iterate on models, creating slightly different versions to test new features, hyperparameters, or datasets.</p>
<p>This proliferation of models introduces several challenges:</p>
<div class="callout callout-style-simple callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Inconsistency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>It becomes difficult to ensure consistency in results if the lineage and differences between model versions are unclear.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Operational Complexity
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Managing multiple models across environments (e.g., testing, staging, production) can lead to confusion about which model is live or should be retrained.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deployment Confusion
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Organizations may inadvertently deploy outdated or suboptimal models, undermining the business objectives they aim to support.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Real-world examples highlight these challenges. For instance, Booking.com reported in their paper <a href="https://blog.kevinhu.me/2021/04/25/25-Paper-Reading-Booking.com-Experiences/bernardi2019.pdf"><em>150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com</em></a> that the lack of clear versioning early on hindered their scalability and led to inefficiencies in updating and deploying models.</p>
</div>
</div>
</div>
</section>
<section id="versioning-for-traceability-and-consistency" class="level3">
<h3 class="anchored" data-anchor-id="versioning-for-traceability-and-consistency">Versioning for Traceability and Consistency</h3>
<p>Model versioning is a critical practice that provides traceability throughout the lifecycle of a machine learning model. By versioning models, organizations can:</p>
<ul>
<li><strong>Link Experiments to Deployment</strong>: Each model deployed to production can be traced back to the specific experiment or set of experiments that informed its development, including the dataset, preprocessing steps, and hyperparameters used.</li>
<li><strong>Audit Performance</strong>: Versioning allows organizations to compare historical performance metrics against current results to determine if changes have introduced regressions or improvements.</li>
<li><strong>Facilitate Rollbacks</strong>: If a newly deployed model underperforms or introduces unintended biases, versioning ensures that the organization can quickly revert to a previous, more reliable version.</li>
</ul>
</section>
<section id="examples-of-when-model-versioning-is-essential" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-when-model-versioning-is-essential">Examples of When Model Versioning is Essential</h3>
<p>Versioning models is particularly crucial in the following scenarios:</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Retraining with Updated Data
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>When new data becomes available, retraining models often results in improved accuracy. Versioning ensures that stakeholders can differentiate between the original and updated models and evaluate performance gains.</p>
<p><strong>Example</strong>: A weather forecasting system regularly retrains models with the latest meteorological data. Versioning ensures traceability between the datasets and the updated predictions.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hyperparameter Tuning
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Small tweaks in hyperparameters can lead to significant performance differences. Versioning ensures that the optimal configuration is clearly documented and reproducible.</p>
<p><strong>Example</strong>: A fraud detection system adjusts sensitivity thresholds to minimize false positives. By versioning each iteration, teams can identify the version with the best balance of precision and recall.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Multiple Models for the Same Business Process
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Often, organizations deploy multiple models to serve the same process but target different subgroups or geographic regions. Versioning helps manage this complexity.</p>
<p><strong>Example</strong>: A recommendation engine deploys region-specific models for North America, Europe, and Asia. Versioning ensures clarity in model assignments and simplifies future updates.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Regulatory and Compliance Requirements
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For industries with stringent regulatory requirements, such as finance and healthcare, versioning is essential for audits and compliance.</p>
<p><strong>Example</strong>: A credit scoring model must demonstrate consistency with historical performance when audited. Versioning ensures all artifacts, from datasets to hyperparameters, are well-documented and reproducible.</p>
</div>
</div>
</div>
<p>Model versioning builds directly on the principles introduced in the previous chapter on experiment tracking. While experiment tracking captures the journey of a model during development, versioning ensures that this journey remains accessible and reproducible as the model evolves. Together, these practices provide a comprehensive framework for managing machine learning systems.</p>
</section>
</section>
<section id="key-components-of-model-versioning" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="key-components-of-model-versioning"><span class="header-section-number">7.2</span> Key Components of Model Versioning</h2>
<p>Model versioning builds upon the principles and practices of experiment tracking, serving as a bridge between experimentation and production. While experiment tracking documents the various iterations, configurations, and metrics of machine learning experiments, model versioning focuses on systematically capturing, storing, and managing these components to ensure traceability and reproducibility in production environments.</p>
<p>This continuity between experiment tracking and model versioning is essential for creating scalable, maintainable, and reliable machine learning workflows. Below, we revisit some of the key components from experiment tracking and highlight how their roles evolve in the context of model versioning.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Datasets
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Experiment Tracking Context</strong>: During experimentation, training datasets are logged to ensure that each experiment run is linked to the specific data used for training and validation. We saw this in <a href="06-modelops-experimenting.html#sec-experiment-tracking" class="quarto-xref">Section&nbsp;<span>6.4</span></a> where the training datasets for each experiment run was logged. This helps data scientists analyze the impact of data and feature engineering variations on model performance.</li>
<li><strong>In Model Versioning</strong>: Datasets are versioned alongside models to maintain traceability through deployment and future use. Consequently, if we were to version the best-performing model in the apple demand forecasting example (<a href="06-modelops-experimenting.html#sec-experiment-tracking" class="quarto-xref">Section&nbsp;<span>6.4</span></a>), the training datasets used for this model would also be stored/linked to ensure that retraining or debugging efforts can recreate the exact conditions under which a versioned/deployed model was developed.</li>
<li><strong>Why It Matters</strong>: This linkage ensures that models can be reproduced and validated under the same data conditions, making it easier to address drift, audit decisions, or meet regulatory requirements.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hyperparameters
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Experiment Tracking Context</strong>: Hyperparameters like learning rates, batch sizes, and optimization strategies are logged to document how they influence model performance.</li>
<li><strong>In Model Versioning</strong>: The hyperparameters for the best-performing model are saved as part of the model metadata, ensuring they can be reused for retraining or deployment. For instance, if a random forest model performs best then its tree depth, number of estimators, splitting criteria, and any other hyperparameters should be preserved as a model is versioned and deployed.</li>
<li><strong>Why It Matters</strong>: Consistent hyperparameter tracking enables reproducibility and helps optimize deployment configurations without re-running unnecessary experiments.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metrics
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Experiment Tracking Context</strong>: Metrics such as RMSE, accuracy, or precision are logged to evaluate and compare model performance across experiments.</li>
<li><strong>In Model Versioning</strong>: Metrics become part of the versioned model’s metadata, serving as critical benchmarks for deployment decisions. For example, if a model is versioned and deployed, then this model is often referred to as a champion model. Future model modifications are often referred to as challengers. In order to compare champion and challenger models, we must continue to have their evaluation metrics stored alongside them so that they can be compared to determine if a challenger model should become the new champion model.</li>
<li><strong>Why It Matters</strong>: Storing metrics alongside model versions ensures that deployment decisions are informed by objective performance comparisons.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Model Artifacts
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Experiment Tracking Context</strong>: During experimentation, model artifacts (e.g., weights, configuration files, saved model – .pkl, .pb) are usually logged as we demonstrated in <a href="06-modelops-experimenting.html#sec-experiment-tracking" class="quarto-xref">Section&nbsp;<span>6.4</span></a>.</li>
<li><strong>In Model Versioning</strong>: The trained model artifacts are also preserved as part of the versioning process. These artifacts represent the operational core of the machine learning workflow and must be reliably stored for deployment and retraining.</li>
<li><strong>Why It Matters</strong>: By versioning artifacts, teams ensure that deployed models can be reloaded, audited, and updated without ambiguity and ensures the exact prediction logic can be reproduced when needed.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deployment Metadata
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><strong>Experiment Tracking Context</strong>: During experiment tracking, environment configurations, runtime dependencies, and scaling parameters can be logged but are not always done so.</li>
<li><strong>In Model Versioning</strong>: However, for models that are versioned and intended to be deployed, this information becomes critical to facilitate seamless transitions of the model from development to production. This information often become referred to as deployment metadata or runtime infrastructure.</li>
<li><strong>Why It Matters</strong>: Incorporating deployment metadata ensures that models function as expected in production, regardless of the environment. For example, specifying dependency versions prevents compatibility issues when deploying a forecasting model that relies on Python 3.12 or newer.</li>
</ul>
</div>
</div>
</div>
<p>Model versioning builds directly on experiment tracking by formalizing the transition from experimentation to operationalization. In our apple demand forecasting example, experiment tracking allowed us to document how different preprocessing techniques and hyperparameter configurations influenced model performance. Model versioning took these records and added structured management of the selected model artifacts, datasets, and deployment settings, ensuring that the model could be deployed reliably and reproduced when needed.</p>
<p>This continuity not only simplifies workflows but also ensures adherence to critical design principles like <strong>reproducibility</strong>, <strong>scalability</strong>, and <strong>traceability</strong>, which are essential for robust machine learning systems.</p>
</section>
<section id="tools-for-model-versioning-and-reproducibility" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="tools-for-model-versioning-and-reproducibility"><span class="header-section-number">7.3</span> Tools for Model Versioning and Reproducibility</h2>
<p>In the previous chapter on experiment tracking (<a href="06-modelops-experimenting.html" class="quarto-xref">Chapter&nbsp;<span>6</span></a>), we explored tools like <a href="https://mlflow.org/">MLflow</a>, <a href="https://wandb.ai/">Weights &amp; Biases</a>, and <a href="https://www.comet.com/">Comet.ml</a>, which are pivotal for documenting and tracking experimentation. Building on that foundation, model versioning tools extend these capabilities to ensure the systematic management of trained models, artifacts, datasets, and deployment configurations. Below, we’ll quickly explore tools specifically designed to address the challenges of versioning and reproducibility.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLflow Model Registry
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://mlflow.org/docs/latest/model-registry.html">MLflow’s Model Registry</a> provides a robust solution for managing the lifecycle of machine learning models as part of its broader tracking ecosystem.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Central repository for storing, annotating, and managing model versions.</li>
<li>Track model stages such as <em>staging</em>, <em>production</em>, and <em>archived</em>.</li>
<li>Compare versions and collaborate using built-in commentary features.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: After identifying the best-performing apple demand forecasting model, the MLflow Model Registry enables seamless tracking of version updates, fostering reproducibility and transparency in deployment.</li>
<li><strong>Why It Matters</strong>: Centralizing model versioning and metadata enhances collaboration, traceability, and governance across teams.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
DVC (Data Version Control)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://dvc.org/">DVC</a> was introduced in <a href="04-dataops-build.html#sec-dataops-tools" class="quarto-xref">Section&nbsp;<span>4.3</span></a> where we demonstrated its capability to version datasets in a data pipeline. Beyond data, DVC is also a powerful tool for <a href="https://dvc.org/doc/use-cases/model-registry">managing machine learning models and their associated artifacts</a>.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Version control for datasets, preprocessing scripts, and trained models, enabling end-to-end reproducibility.</li>
<li>Integrates seamlessly with Git to provide structured versioning for machine learning workflows.</li>
<li>Supports storage of large files (e.g., models and datasets) in cloud platforms like AWS S3, Google Cloud Storage, or Azure Blob.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: DVC can be used to version the trained apple forecasting models, ensuring that each iteration is traceable alongside the datasets and preprocessing scripts used in training.</li>
<li><strong>Why It Matters</strong>: By providing a unified approach to versioning data, code, and models, DVC ensures that every component of the ML workflow is reproducible and accessible.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Weights &amp; Biases Model Management
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://wandb.ai/">Weights &amp; Biases</a> (W&amp;B), known for its experiment tracking capabilities, also includes features for <a href="https://wandb.ai/site/registry/">managing models and artifacts</a>.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Integrate model artifact tracking seamlessly with experiment logs.</li>
<li>Store and compare multiple versions of models alongside datasets and metrics.</li>
<li>Manage deployment-ready models and track their history across environments.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: W&amp;B allows us to log and compare various versions of the apple forecasting model, ensuring we retain all metadata and performance metrics for evaluation.</li>
<li><strong>Why It Matters</strong>: Combining experiment tracking and model versioning in one platform simplifies workflows and avoids fragmented systems.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Git-LFS and Git for Models
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://git-scm.com/">Git</a>, paired with <a href="https://git-lfs.github.com/">Git Large File Storage (LFS)</a>, is a lightweight option for tracking machine learning artifacts.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Tracks changes in code, datasets, and configurations.</li>
<li>Git-LFS efficiently stores large binary files, such as trained model weights.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: By leveraging Git-LFS, the apple forecasting model files and associated configurations can be tracked alongside the version-controlled codebase.</li>
<li><strong>Why It Matters</strong>: Using Git for model versioning integrates naturally into existing software development workflows, minimizing the need for additional tools.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TensorFlow Serving
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> is a specialized system for serving machine learning models in production environments.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Native support for versioning TensorFlow models, ensuring smooth updates without interrupting existing services.</li>
<li>Optimized for high-performance serving of real-time and batch predictions.</li>
<li>Includes automatic model version management and rollback capabilities.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: TensorFlow Serving can manage the deployment of multiple apple forecasting model versions, ensuring users seamlessly receive predictions from the best-performing model while enabling rapid updates.</li>
<li><strong>Why It Matters</strong>: TensorFlow Serving excels at operationalizing machine learning models, with built-in versioning and deployment features designed for scalability and reliability.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Neptune.ai
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://neptune.ai/">Neptune.ai</a> is a specialized tool designed for tracking, organizing, and managing machine learning models and experiments.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Centralized dashboard for tracking model artifacts and metrics.</li>
<li>Easy-to-use interface for comparing model versions and their configurations.</li>
<li>Strong integration capabilities with popular ML frameworks like TensorFlow and PyTorch.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: Neptune.ai can be used to track all iterations of the apple forecasting model and visualize performance metrics over time.</li>
<li><strong>Why It Matters</strong>: Neptune.ai provides advanced comparison and collaboration features, ideal for teams working on diverse ML projects.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
BentoML
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><a href="https://www.bentoml.com/">BentoML</a> is a specialized framework designed to simplify packaging, versioning, and deploying machine learning models.</p>
<ul>
<li><strong>Key Features</strong>:
<ul>
<li>Lightweight framework for packaging and serving models.</li>
<li>Built-in versioning for models, ensuring that all deployment-ready versions are tracked and reproducible.</li>
<li>Flexible deployment options for cloud, edge, or on-premise environments.</li>
</ul></li>
<li><strong>Use Case in Our Pipeline</strong>: BentoML could be used to package and deploy different versions of the apple forecasting model while ensuring that deployment configurations are version-controlled.</li>
<li><strong>Why It Matters</strong>: BentoML’s focus on versioning deployment-ready models makes it a strong complement to tools like MLflow and DVC.</li>
</ul>
</div>
</div>
</div>
<p>The choice of tools depends on the specific needs of your team and infrastructure:</p>
<ul>
<li>Teams already using <a href="https://mlflow.org/">MLflow</a> for tracking experiments will benefit from its integrated Model Registry for managing versions.</li>
<li>For large-scale datasets and models, <a href="https://dvc.org/">DVC</a> offers a comprehensive solution that pairs well with Git workflows.</li>
<li>Organizations looking for deployment-specific tools may prefer <a href="https://www.bentoml.com/">BentoML</a> or <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a>.</li>
<li>For advanced dashboarding and visualization, <a href="https://neptune.ai/">Neptune.ai</a> provides powerful comparison capabilities.</li>
</ul>
<p>By aligning tools with workflow requirements, teams can establish robust model versioning practices that seamlessly extend the experiment tracking processes outlined in the previous chapter.</p>
</section>
<section id="sec-model-version-example" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="sec-model-version-example"><span class="header-section-number">7.4</span> Hands-On Example: Implementing Model Versioning</h2>
<p>In the previous chapter, we explored how MLflow can be used to track experiments, documenting essential components like datasets, hyperparameters, and metrics for apple demand forecasting models (see <a href="06-modelops-experimenting.html#sec-experiment-tracking" class="quarto-xref">Section&nbsp;<span>6.4</span></a>). This experiment tracking provided a solid foundation for organizing and comparing various model iterations. However, tracking experiments is only part of the journey. Once a model is ready to transition from experimentation to production, effective management and versioning become critical.</p>
<p>This section builds on the previous hands-on example by demonstrating how to leverage the <a href="https://mlflow.org/docs/latest/model-registry.html">MLflow Model Registry</a> to implement model versioning. Model versioning ensures that every model iteration—along with its configuration, artifacts, and performance metrics—is systematically managed. It also facilitates reproducibility, traceability, and seamless lifecycle transitions.</p>
<p>In this hands-on example, we’ll revisit the apple demand forecasting problem and illustrate how to register, manage, and compare multiple model versions using the MLflow Model Registry. By the end of this section, you will understand how to:</p>
<ol type="1">
<li>Register models and document their key attributes.</li>
<li>Manage multiple versions of a model and promote them across lifecycle stages.</li>
<li>Compare model versions to identify the best-performing iteration.</li>
</ol>
<p>Through this process, you’ll see how model versioning not only enhances organization and collaboration but also lays the groundwork for deploying reliable models in production. Let’s dive in!</p>
<section id="setting-the-stage" class="level3">
<h3 class="anchored" data-anchor-id="setting-the-stage">Setting the Stage</h3>
<p>Before diving into model versioning, let’s recap where we left off in the previous chapter. In the apple demand forecasting project, we trained and tracked multiple models using MLflow’s experiment tracking capabilities. Each experiment logged essential details, including:</p>
<ul>
<li>The dataset used for training.</li>
<li>Feature engineering steps applied.</li>
<li>Hyperparameters for the model.</li>
<li>Evaluation metrics such as mean absolute error (MAE) and root mean squared error (RMSE).</li>
<li>Artifacts, including the trained model and any visualizations.</li>
</ul>
<p>Now, we’ll build on that foundation by focusing on versioning these models. The MLflow Model Registry provides a structured way to:</p>
<ol type="1">
<li><strong>Register Models</strong>: Each trained model can be registered and stored with its corresponding metadata.</li>
<li><strong>Version Models</strong>: Multiple versions of a model can be tracked, allowing you to maintain a history of iterations.</li>
<li><strong>Facilitate Collaboration</strong>: Teams can add notes, tags, aliases, and approval statuses to provide transparency and coordination during the model lifecycle.</li>
</ol>
<p>If you want to follow along with this example, ensure that you have the following prerequisites set up:</p>
<ul>
<li>A working MLflow environment with the MLflow Tracking Server enabled. If you’re using the notebook from the last chapter, it already tracks experiments to a local MLflow server.</li>
<li>The code and dataset from the apple demand forecasting example. This includes the data preprocessing and modeling pipeline we previously implemented.</li>
<li>Access to the MLflow Model Registry via the MLflow UI.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>If you followed along in <a href="06-modelops-experimenting.html" class="quarto-xref">Section&nbsp;<span>6</span></a> then you will be all set to follow along in this section!</p>
</div>
</div>
</div>
<p>With these components in place, we’ll demonstrate how to extend the workflow by registering and managing models in the MLflow Model Registry. By the end of this section, you’ll see how model versioning creates a structured and traceable process, ensuring that each model iteration is well-documented and ready for production deployment.</p>
</section>
<section id="registering-models" class="level3">
<h3 class="anchored" data-anchor-id="registering-models">Registering Models</h3>
<p>Model registration is a critical step in model versioning and reproducibility. It allows you to systematically manage, track, and deploy models in a structured manner. In this section, we demonstrate two approaches to registering models in MLflow:</p>
<ol type="1">
<li>Using the <strong>MLflow UI</strong> to register a model from a previously logged experiment.</li>
<li>Using <strong>code</strong> to programmatically register a model during training.</li>
</ol>
<section id="method-1-registering-a-model-using-the-mlflow-ui" class="level4">
<h4 class="anchored" data-anchor-id="method-1-registering-a-model-using-the-mlflow-ui">Method 1: Registering a Model Using the MLflow UI</h4>
<p>This method leverages the MLflow user interface to manually register a model from a previously logged experiment.</p>
<ol type="1">
<li><strong>Open the MLflow UI</strong>:
<ul>
<li>Start the MLflow server by running <code>mlflow ui</code> in your terminal.</li>
<li>Navigate to <code>http://localhost:5000</code> (or the local URL provided when you run <code>mlflow ui</code>).</li>
</ul></li>
<li><strong>Locate the Experiment</strong>:
<ul>
<li>In the “Experiments” section, find the experiment you created in the last chapter (e.g., “Apple Demand Forecasting”).</li>
<li>Select an experiment run, and review the logged parameters, metrics, and artifacts. <img src="images/mlflow-experiments.png" class="img-fluid" alt="MLflow Empty UI"></li>
</ul></li>
<li><strong>Register the Model</strong>:
<ul>
<li>Under the “Artifacts” section, click on the <strong>model</strong> artifact to open its details.</li>
<li>Click the <strong>Register Model</strong> button. <img src="images/mlflow-artifact-register.png" class="img-fluid" alt="MLflow Artifact UI"></li>
<li>Provide a name for the model (e.g., <code>apple_demand</code>), and confirm. <img src="images/mlflow-artifact-register2.png" class="img-fluid" alt="Register Model"></li>
<li>Now, if you navigate to the “Models” tab in the MLflow UI, you’ll see the newly registered model. <img src="images/mlflow-registered-models.png" class="img-fluid" alt="Registered Model"></li>
</ul></li>
<li><strong>Adding Metadata:</strong>:
<ul>
<li>Click on the registered model name to see its details.</li>
<li>We can use the <strong>Tags</strong> and <strong>Aliases</strong> features to add metadata about the model. This can be helpful to annotate model versions with their status. For example,
<ul>
<li>You could apply a tag <code>validation_status</code> with a value <code>pending</code> to a model version while it is being validated or going through the final reviews prior to deploying. You can then update the tag value to <code>passed</code> when it has passed any additional validation requirements (i.e.&nbsp;smoke tests, performance tests) and is ready for deployment.</li>
<li>And you can use <strong>Aliases</strong> to provide a flexible way to create named references for particular model versions - such as <strong><em>champion</em></strong> and <strong><em>challenger</em></strong>. <img src="images/mlflow-registered-model-metadata.png" class="img-fluid" alt="Registered Model Metadata"></li>
</ul></li>
</ul>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Why It Matters</strong>: Tags and aliases improve model lifecycle management by providing quick, meaningful insights into the status and usage of models. This metadata helps teams identify which versions are in production, under review, or awaiting deployment, ensuring better coordination and traceability.</p>
</div>
</div>
</div></li>
</ol>
</section>
<section id="method-2-registering-a-model-using-code" class="level4">
<h4 class="anchored" data-anchor-id="method-2-registering-a-model-using-code">Method 2: Registering a Model Using Code</h4>
<p>For workflows requiring automation, you can register models programmatically within your code. There’s actually a few different ways you can do this.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>See <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/model-registration.ipynb">this notebook</a> for the entire source code.</p>
</div>
</div>
</div>
<p>The first is to search existing runs in our experiment:</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-registration.ipynb" data-notebook-title="Example Model Registration" data-notebook-cellid="cell-7">
<div id="cell-7" class="cell" data-tags="[&quot;existing-runs&quot;]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check out the existing model runs</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>all_runs <span class="op">=</span> mlflow.search_runs(search_all_experiments<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(all_runs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                             run_id       experiment_id    status  \
0  1d4f349e3edc481685c691ef773dfb7e  186119791991456899  FINISHED   
1  9729fd97ce074ee2862cc32e1089c513  186119791991456899  FINISHED   
2  3a460bec71464493bdc4da5ec9537796  186119791991456899  FINISHED   
3  dfe70656a2f64c11b35078e288f962ab  186119791991456899  FINISHED   

                                        artifact_uri  \
0  file:///Users/b294776/Desktop/workspace/traini...   
1  file:///Users/b294776/Desktop/workspace/traini...   
2  file:///Users/b294776/Desktop/workspace/traini...   
3  file:///Users/b294776/Desktop/workspace/traini...   

                        start_time                         end_time  \
0 2025-01-26 18:21:05.944000+00:00 2025-01-26 18:21:11.317000+00:00   
1 2025-01-26 18:21:00.330000+00:00 2025-01-26 18:21:05.873000+00:00   
2 2025-01-26 18:20:01.595000+00:00 2025-01-26 18:20:04.736000+00:00   
3 2025-01-26 18:19:56.389000+00:00 2025-01-26 18:20:01.247000+00:00   

   metrics.rmse  metrics.r2  metrics.mae  metrics.mse  ... params.bootstrap  \
0     61.285576    0.900071    47.628141  3755.921850  ...             None   
1     64.496897    0.889324    50.436906  4159.849675  ...             True   
2     64.662326    0.888755    50.439003  4181.216379  ...             True   
3     61.262270    0.900147    47.611615  3753.065770  ...             None   

  params.min_samples_split params.n_estimators params.oob_score  \
0                     None                None             None   
1                       15                 200             None   
2                       10                 100            False   
3                     None                None             None   

  params.random_state tags.mlflow.source.type tags.mlflow.user  \
0                None                   LOCAL          b294776   
1                None                   LOCAL          b294776   
2                 888                   LOCAL          b294776   
3                None                   LOCAL          b294776   

                            tags.mlflow.runName  \
0  Regularized Regression Hyperparameter Tuning   
1           Random Forest Hyperparameter Tuning   
2                                 Random Forest   
3                        Regularized Regression   

                       tags.mlflow.log-model.history  \
0  [{"run_id": "1d4f349e3edc481685c691ef773dfb7e"...   
1  [{"run_id": "9729fd97ce074ee2862cc32e1089c513"...   
2  [{"run_id": "3a460bec71464493bdc4da5ec9537796"...   
3  [{"run_id": "dfe70656a2f64c11b35078e288f962ab"...   

                             tags.mlflow.source.name  
0  /opt/anaconda3/lib/python3.12/site-packages/ip...  
1  /opt/anaconda3/lib/python3.12/site-packages/ip...  
2  /opt/anaconda3/lib/python3.12/site-packages/ip...  
3  /opt/anaconda3/lib/python3.12/site-packages/ip...  

[4 rows x 24 columns]</code></pre>
</div>
</div>
</div>
<p>We can then use this info to extract the run we care about and then use <code>mlflow.register_model()</code> to register the model.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-registration.ipynb" data-notebook-title="Example Model Registration" data-notebook-cellid="cell-8">
<div id="cell-8" class="cell" data-tags="[&quot;get-rf-run-id&quot;]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract model run ID for tuned random forest model</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>run <span class="op">=</span> all_runs[<span class="st">'tags.mlflow.runName'</span>] <span class="op">==</span> <span class="st">'Random Forest Hyperparameter Tuning'</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>run_id <span class="op">=</span> all_runs[run][<span class="st">'run_id'</span>].iloc[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-registration.ipynb" data-notebook-title="Example Model Registration" data-notebook-cellid="cell-9">
<div id="cell-9" class="cell" data-tags="[&quot;register-rf-model&quot;]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># register this model</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> mlflow.register_model(<span class="ss">f'runs:/</span><span class="sc">{</span>run_id<span class="sc">}</span><span class="ss">'</span>, <span class="st">'apple_demand'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Now, if we go back to our UI, we’ll see that we have a new registered model</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mlflow-registered-model-code1.png" class="img-fluid figure-img"></p>
<figcaption>Registered RF Model via Code</figcaption>
</figure>
</div>
<p>An alternative, is to register the model during the training run, which helps allow automation within your code. Below is an example of registering an XGBoost model during training:</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-registration.ipynb" data-notebook-title="Example Model Registration" data-notebook-cellid="cell-11">
<div id="cell-11" class="cell" data-tags="[&quot;register-xgboost-model&quot;]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> generate_apple_sales_data_with_promo_adjustment(base_demand<span class="op">=</span><span class="dv">1_000</span>, n_rows<span class="op">=</span><span class="dv">1_000</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.drop(columns<span class="op">=</span>[<span class="st">"date"</span>, <span class="st">"demand"</span>])</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">"demand"</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and validation sets</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Train an XGBoost model</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>xgb_model <span class="op">=</span> XGBRegressor(n_estimators<span class="op">=</span><span class="dv">100</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_depth<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>xgb_model.fit(X_train, y_train)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> xgb_model.predict(X_val)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_val, y_pred))</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Log experiment details</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> mlflow.start_run(run_name<span class="op">=</span><span class="st">"XGBoost Model"</span>):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param(<span class="st">"model_type"</span>, <span class="st">"XGBoost"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param(<span class="st">"n_estimators"</span>, <span class="dv">100</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param(<span class="st">"learning_rate"</span>, <span class="fl">0.1</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    mlflow.log_param(<span class="st">"max_depth"</span>, <span class="dv">5</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    mlflow.log_metric(<span class="st">"rmse"</span>, rmse)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    mlflow.xgboost.log_model(xgb_model, artifact_path<span class="op">=</span><span class="st">"artifacts"</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Register model programmatically</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    active_run <span class="op">=</span> mlflow.active_run().info.run_id</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    model_uri <span class="op">=</span> <span class="ss">f"runs:/</span><span class="sc">{</span>active_run<span class="sc">}</span><span class="ss">/model"</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    registered_model <span class="op">=</span> mlflow.register_model(model_uri<span class="op">=</span>model_uri, name<span class="op">=</span><span class="st">"apple_demand"</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add metadata: Tags and Aliases</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    client <span class="op">=</span> mlflow.tracking.MlflowClient()</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    client.set_registered_model_tag(</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        registered_model.name, <span class="st">"validation_status"</span>, <span class="st">"pending"</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    client.set_registered_model_alias(</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        registered_model.name, <span class="st">"challenger"</span>, version<span class="op">=</span>registered_model.version</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>In the above code snippet, if a registered model with the name doesn’t exist, the method registers a new model and creates Version 1. If a registered model with the name exists, the method creates a new model version.</p>
<p>Also, note how you can even add tags and aliases programmatically!</p>
</div>
</div>
</div>
<p>Now, if we check out the MLFlow UI we’ll see our latest versioned model!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mlflow-all-registered-models.png" class="img-fluid figure-img"></p>
<figcaption>All Registered Models</figcaption>
</figure>
</div>
</section>
<section id="why-use-both-methods" class="level4">
<h4 class="anchored" data-anchor-id="why-use-both-methods">Why Use Both Methods?</h4>
<p>Both methods of registering models offer unique benefits, and understanding when to use each can enhance your workflow:</p>
<ul>
<li><strong>UI Registration</strong>:
<ul>
<li><strong>When to Use</strong>: This method is ideal for manual workflows or when revisiting past experiments. It allows you to visually explore experiment details, select a specific run, and easily register its model.</li>
<li><strong>Why It’s Useful</strong>: It simplifies navigation and is particularly helpful for users unfamiliar with coding or automation, making it accessible to a broader team of stakeholders.</li>
</ul></li>
<li><strong>Code-Based Registration</strong>:
<ul>
<li><strong>When to Use</strong>: Best suited for automated pipelines where models are frequently trained and registered. It ensures consistency and reduces manual intervention, which is critical for scaling operations.</li>
<li><strong>Why It’s Useful</strong>: This method integrates directly into the experimentation workflow, enabling seamless and error-free registration as part of your end-to-end process.</li>
</ul></li>
</ul>
<p><strong>Combining Both</strong>: Teams can benefit from leveraging both approaches. For example: - Use code-based registration for ongoing experiments in an automated workflow. - Use the MLflow UI to manage, review, or reassign stages for registered models, ensuring they meet lifecycle and deployment requirements.</p>
</section>
</section>
<section id="querying-registered-models" class="level3">
<h3 class="anchored" data-anchor-id="querying-registered-models">Querying Registered Models</h3>
<p>You can even query registered models:</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-registration.ipynb" data-notebook-title="Example Model Registration" data-notebook-cellid="cell-13">
<div id="cell-13" class="cell" data-tags="[&quot;search-registered-models&quot;]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>mlflow.search_registered_models()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>[&lt;RegisteredModel: aliases={'challenger': '3', 'champion': '1'}, creation_timestamp=1737916057537, description='', last_updated_timestamp=1737916154049, latest_versions=[&lt;ModelVersion: aliases=['challenger'], creation_timestamp=1737916154031, current_stage='None', description=None, last_updated_timestamp=1737916154031, name='apple_demand', run_id='021eea67d5ea4be5967ca90efe8866db', run_link=None, source='file:///Users/b294776/Desktop/workspace/training/UC/uc-bana-7075/ModelOps/mlruns/186119791991456899/021eea67d5ea4be5967ca90efe8866db/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=3&gt;], name='apple_demand', tags={'validation_status': 'pending'}&gt;]</code></pre>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>There’s a lot more functionality available with MLFlow Model Registery. To dig more into advanced functionality, check out the MLFlow docs: https://mlflow.org/docs/latest/model-registry.html</p>
</div>
</div>
</div>
</section>
<section id="reproducibility-of-registered-model" class="level3">
<h3 class="anchored" data-anchor-id="reproducibility-of-registered-model">Reproducibility of Registered Model</h3>
<p>When exploring a specific versioned model in MLflow, you’ll notice a “Source Run” link associated with each registered model version. This link serves as a direct connection to the logged details of the specific run that produced the model, including its hyperparameters, datasets, evaluation metrics, and any other artifacts logged during experimentation. By providing this seamless traceability, the “Source Run” link ensures a clear lineage between the registered model and its training information. This feature is invaluable for understanding the context of a model, diagnosing issues, reproducing results, or auditing the training process, making it an essential aspect of robust model management.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mlflow-registered-model-lineage.png" class="img-fluid figure-img"></p>
<figcaption>Source Run</figcaption>
</figure>
</div>
</section>
</section>
<section id="summary" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.5</span> Summary</h2>
<p>In this chapter, we explored the critical role of model versioning and reproducibility within the ModelOps framework, emphasizing their importance for managing the lifecycle of machine learning models. Building on the experiment tracking concepts introduced in the previous chapter, we discussed how model versioning ensures traceability, facilitates collaboration, and supports robust deployment strategies. By versioning models alongside their associated metadata, configurations, and evaluation metrics, organizations can maintain a clear lineage for every model iteration, ensuring consistency and reproducibility across workflows.</p>
<p>We highlighted key components of model versioning, showing how it acts as an extension of experiment tracking, and introduced tools like MLflow, DVC, and TensorFlow Serving, which streamline versioning and foster reliable model management. Through a hands-on example, we demonstrated how to register models using MLflow, both through the user interface and programmatically, showcasing the value of tagging and aliasing for managing model stages and deployment readiness.</p>
<p>Although this chapter focused on the fundamentals of versioning and reproducibility, we laid the foundation for the next chapter, which will delve into deploying and serving machine learning models. With deployment strategies, model lifecycle monitoring, and real-world applications ahead, the concepts explored here will serve as an essential bridge to operationalizing your models in production environments.</p>
</section>
<section id="sec-model-version-exercise" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="sec-model-version-exercise"><span class="header-section-number">7.6</span> Exercise</h2>
<p>In this exercise, you’ll extend your work from the previous chapter’s exercise (see <a href="06-modelops-experimenting.html#sec-model-experimentation-exercise" class="quarto-xref">Exercise&nbsp;<span>6.7</span></a>) to include model versioning. Building on the California home prediction task, you will use MLflow to register and version your models, enabling clear traceability and management of model iterations.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 1: Conceptual Design
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Understanding the Need for Versioning</strong>:
<ul>
<li>Reflect on the experiment tracking exercise from <a href="06-modelops-experimenting.html#sec-model-experimentation-exercise" class="quarto-xref">Section&nbsp;<span>6.7</span></a>. Why do you think Zillow would find it important to version the model produced for this task, especially when experimenting with different configurations or when preparing for deployment?</li>
<li>List scenarios in your California home prediction task where having model versioning would provide benefits, such as:
<ul>
<li>Keeping track of retrained models using updated datasets.</li>
<li>Testing new algorithms or hyperparameters.</li>
<li>Identifying the best-performing model for deployment.</li>
</ul></li>
</ul></li>
<li><strong>Define the Metadata to Track</strong>:
<ul>
<li>Consider what metadata you would tag your models with (e.g., “validation_status”).</li>
<li>Think about how aliases could help you manage and reference different model versions, such as labeling one as the “champion” model.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 2: Hands-On Experimentation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Register a Model via the MLflow UI</strong>:
<ul>
<li>Select the best-performing model from the previous chapter’s exercise in the MLflow UI.</li>
<li>Register it to the MLflow Model Registry, providing an appropriate name and description.</li>
<li>Add metadata to the registered model by tagging it with a status like “pending final validation.”</li>
<li>Assign an alias, such as “champion,” to signify its potential readiness for deployment.</li>
</ul></li>
<li><strong>Programmatically Register a Model</strong>:
<ul>
<li>Train a new model, such as an XGBoost regressor, using the same California housing dataset.</li>
<li>Log this new model and register it programmatically in MLflow by following the example provided in this chapter.</li>
<li>Add metadata and aliases to this model programmatically.</li>
</ul></li>
<li><strong>Explore the Source Run</strong>:
<ul>
<li>Navigate to the “Source Run” link for one of the registered models in the MLflow UI.</li>
<li>Observe how it links back to the original run, providing detailed training information.</li>
<li>Reflect on the importance of this feature for reproducibility and traceability.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 3: Reflection on Design Principles
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Versioning and Reproducibility</strong>:
<ul>
<li>How does the MLflow Model Registry help ensure reproducibility and maintain a clear lineage between model versions and their training runs?</li>
<li>How could this setup scale to support a team working on the same project?</li>
</ul></li>
<li><strong>Metadata and Management</strong>:
<ul>
<li>Reflect on how metadata (tags and aliases) simplifies managing multiple model versions in a production workflow.</li>
<li>Think about potential pitfalls if metadata or aliases are inconsistently applied or not updated.</li>
</ul></li>
<li><strong>Scalability and Collaboration</strong>:
<ul>
<li>Imagine how this model versioning process could be extended for a team working collaboratively on multiple models. What practices would you recommend for effective collaboration?</li>
</ul></li>
</ol>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./06-modelops-experimenting.html" class="pagination-link" aria-label="Model Training and Experiment Tracking">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model Training and Experiment Tracking</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./08-modelops-deployment.html" class="pagination-link" aria-label="Model Deployment">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Deployment</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/edit/main/07-modelops-versioning.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>