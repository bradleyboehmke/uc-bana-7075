---
title: Putting DataOps into Practice
notebook-links: true
notebook-view:
  - notebook: ../DataOps/youtube-data-pipeline.ipynb
    title: "Example Youtube Data Pipeline"
    url: https://github.com/bradleyboehmke/uc-bana-7085/blob/main/DataOps/youtube-data-pipeline.ipynb
---

In the previous chapter, we explored the foundational concepts of DataOps, focusing on the importance of processes such as data ingestion, validation, versioning, and lineage in building reliable machine learning systems. Now, it’s time to take these concepts from theory to practice by applying them to create data pipelines. Data pipelines form the backbone of any modern ML system, serving as the structured, automated pathways through which data flows from raw sources to actionable insights.

A **data pipeline** is a series of interconnected steps that collect, process, validate, and transform data into formats ready for machine learning workflows. Whether you’re ingesting data from multiple sources, cleaning and preparing it for modeling, or ensuring its integrity through validation and versioning, pipelines make it possible to handle these tasks efficiently and consistently. They are essential for ensuring scalability, reliability, and repeatability in ML systems, especially in environments where data is constantly changing or arriving in real-time.

This chapter will guide you through the practical implementation of DataOps principles by developing end-to-end data pipelines. We’ll begin with an introduction to the components of a data pipeline and the tools commonly used to implement them. From there, we’ll delve into the step-by-step process of building a pipeline, starting with a simple example and progressing to techniques for creating scalable workflows that can handle complex, large-scale ML applications.

By the end of this chapter, you’ll have a deeper understanding of how to translate DataOps practices into actionable pipelines, setting the stage for real-world machine learning deployments. Whether you’re working with batch or streaming data, structured or unstructured datasets, this chapter will equip you with the tools and strategies to design robust data pipelines that meet the demands of modern ML workflows.

## Introduction to Data Pipelines

In the world of machine learning (ML), the success of a system hinges on the quality, reliability, and timeliness of the data that powers it. Data pipelines are the operational backbone of these systems, ensuring data flows seamlessly from its source, through various transformations, and into the hands of models or end-users. A **data pipeline** is a series of automated steps that enable data ingestion, processing, and delivery—integrating the concepts of DataOps to ensure scalability, consistency, and efficiency.

In the previous chapter, we explored the fundamental principles of DataOps, including **data ingestion**, **processing**, **validation**, **versioning**, and **lineage**. These processes establish the foundation for building robust data pipelines. Now, we turn our attention to applying these principles in practice to design end-to-end workflows that support machine learning systems.

### The Importance of Data Pipelines in ML System Design

Data pipelines address critical challenges faced by ML workflows, such as managing large data volumes, handling diverse data formats, and maintaining data integrity. By automating repetitive and error-prone tasks like ingestion, cleaning, and transformation, pipelines ensure that data is prepared and delivered reliably.

From the concepts introduced earlier, pipelines incorporate:

- **Data Ingestion**: Automating the flow of data from various sources—whether databases, APIs, or real-time streams—into a centralized system.
- **Data Processing**: Applying transformations, feature engineering, and cleaning steps to structure raw data into formats ready for model consumption.
- **Data Validation**: Embedding quality checks to ensure the consistency and integrity of data at every stage of the pipeline.
- **Data Versioning and Lineage**: Providing traceability and reproducibility by maintaining historical records of datasets and documenting their transformations.

For example, in a customer churn prediction system, a data pipeline might ingest transaction data from a database, clean and preprocess it to handle missing values and inconsistencies, and validate that the data conforms to schema standards before feeding it into an ML model. Each step would leverage the DataOps principles discussed earlier to ensure that the process is efficient, scalable, and error-resistant.

### The Role of DataOps in Constructing Scalable and Reliable Pipelines

As explored in the previous chapter, DataOps is not just a set of practices but a philosophy that ensures data workflows are collaborative, automated, and aligned with business objectives. When applied to pipeline construction, DataOps enables:

- **Scalability**: Modular design allows pipelines to handle growing data volumes and adapt to new sources or transformations without significant redesigns.
- **Reliability**: Continuous validation, versioning, and monitoring ensure that pipelines consistently deliver high-quality data, even in dynamic environments.
- **Collaboration**: DataOps principles encourage communication between data engineers, analysts, and machine learning practitioners, ensuring pipelines meet diverse needs.
- **Traceability**: By tracking data lineage and applying version control, DataOps ensures every step in the pipeline is documented, facilitating debugging, compliance, and reproducibility.

Consider a recommendation system that ingests real-time user behavior data from an API and combines it with historical purchase data stored in a data lake. A DataOps-driven pipeline would:

1. **Ingest and Validate**: Fetch data from both sources, validating schema consistency and ensuring data quality.
2. **Process and Transform**: Standardize features such as timestamps, handle missing values, and engineer inputs like "average time between purchases."
3. **Track Lineage**: Document the transformations applied to both real-time and historical data to ensure traceability and compliance.
4. **Version Datasets**: Maintain snapshots of preprocessed data for future analysis, debugging, or retraining.

By applying the foundational concepts of DataOps, such pipelines are not only robust and scalable but also agile enough to adapt to evolving ML requirements.

In this chapter, we will build on the foundational knowledge from the previous chapter to design and implement data pipelines that embody the principles of DataOps. Through hands-on examples and practical guidance, you will learn how to construct workflows that transform raw data into reliable inputs for machine learning systems, setting the stage for scalable and effective solutions.

## ETL vs. ELT Paradigms

When designing data pipelines, there are two primary paradigms: **Extract, Transform, Load (ETL)** and **Extract, Load, Transform (ELT)**. These paradigms represent distinct approaches to organizing and processing data as it flows through pipelines, each with its own strengths and trade-offs. Choosing between ETL and ELT (or a hybrid approach) requires an understanding of your organization’s technical infrastructure, data requirements, and use cases.

### **Extract, Transform, Load (ETL)**

The **ETL** paradigm embodies a traditional, structured approach to data pipelines. It follows a linear process:

1. **Extract** data from various sources, such as relational databases, APIs, or CSV files.
2. **Transform** the extracted data into a clean, structured format using techniques like cleaning, aggregation, and enrichment.
3. **Load** the transformed data into a target system, such as a relational database or data warehouse.

```{mermaid}
%%| fig-cap: "In the ETL paradigm, the pipeline ingests the data, then processes the data prior to storing the data in their target destination."
%%| label: fig-etl

flowchart LR
    ob1(Raw data source) --> processing[Data processing]
    ob2(Raw data source) --> processing
    ob3(Raw data source) --> processing
    processing --> id3[(Data storage)]
```

ETL pipelines focus on preparing high-quality, ready-to-use data before it enters the storage system. In ETL workflows, the **data processing** phase discussed earlier plays a central role. Transformations are performed early in the pipeline to ensure data quality and structure before it reaches the storage layer.

::: {.callout-note collapse="true"}
## When to Use ETL

- When working with traditional data warehouses or legacy systems that prioritize structured and clean data.
- In use cases requiring strict data governance and pre-defined schemas, such as regulatory compliance reporting.
- When downstream applications depend on consistent, pre-processed datasets.
:::

::: {.callout-note collapse="true"}
## Advantages

- Simplifies downstream workflows by ensuring data is pre-processed and ready for use.
- Reduces storage costs by retaining only clean and structured data.
- Aligns well with environments that require high data integrity.
:::

::: {.callout-note collapse="true"}
##  Challenges

- Upfront transformations can delay data availability.
- Scaling ETL pipelines to handle large datasets or real-time workflows can be resource-intensive.
- Rigid processes may struggle to adapt to rapidly changing data needs.
:::

::: {.callout-note collapse="true"}
## Example

Imagine a financial institution that extracts transaction data, transforms it into a consistent format by applying currency conversions and anomaly detection, and loads the cleaned data into a warehouse for fraud detection reports.
:::

### **Extract, Load, Transform (ELT)**

The **ELT** paradigm, a more modern approach, inverts the transformation and loading steps:

1. **Extract** data from various sources, often retaining its raw format.
2. **Load** the extracted data directly into a scalable storage system, such as a data lake or cloud-based data warehouse.
3. **Transform** the data within the storage system using its computational resources, tailoring transformations to specific analytical needs.

```{mermaid}
%%| fig-cap: "In the ELT paradigm, the pipeline ingests data directly into the destination system and transforms it in parallel or after the fact."
%%| label: fig-elt

flowchart LR
    ob1(Raw data source) --> id3[(Data storage)]
    ob2(Raw data source) --> id3
    ob3(Raw data source) --> id3
    id3 --> processing[Data processing] --> id3
```

In ELT workflows, the **data ingestion** phase emphasizes rapid loading of raw data into storage, enabling flexibility for later transformations. Also, by storing the data in its raw form, ELT better handles structured, unstructured, and semi-structured data and allows downstream analytics on this data to more easily adapt.

::: {.callout-note collapse="true"}
## When to Use ELT

- When leveraging cloud-native platforms like Snowflake, BigQuery, or AWS Redshift, which support high-speed storage and in-database transformations.
- In workflows requiring rapid ingestion and the ability to adapt transformations for different use cases.
- When storing raw data is essential for exploratory analysis or regulatory purposes.
:::

::: {.callout-note collapse="true"}
## Advantages

- Enables faster data ingestion by deferring transformations.
- Supports diverse and flexible transformations tailored to specific analyses.
- Scales well with large data volumes, leveraging modern storage and processing systems.
:::

::: {.callout-note collapse="true"}
## Challenges

- Requires advanced infrastructure to handle and process raw data efficiently.
- Higher storage costs due to the retention of raw, unprocessed data.
- Ad-hoc transformations can lead to inconsistencies if not governed properly.
:::

::: {.callout-note collapse="true"}
## Example

An e-commerce platform ingests raw clickstream data into a cloud data warehouse. When needed, transformations such as sessionizing user behavior or aggregating purchase trends are applied directly within the warehouse for personalized recommendations or sales forecasting.
:::

### Comparing ETL and ELT

| Aspect                | ETL                                     | ELT                                      |
|-----------------------|-----------------------------------------|------------------------------------------|
| **Data Transformation** | Before loading into storage             | After loading into storage               |
| **Storage Requirements** | Lower, as only processed data is stored | Higher, as raw data is retained          |
| **Processing Time**    | Slower ingestion due to upfront transformations | Faster ingestion with deferred transformations |
| **Flexibility**       | Limited; transformations are predefined | High; transformations can be ad hoc      |
| **Infrastructure**    | Suitable for legacy systems or traditional data warehouses | Ideal for modern, scalable systems       |

::: {.callout-tip collapse="false"}
## Choosing the Right Paradigm

As highlighted in the previous chapter, the goals of DataOps — efficiency, reliability, and scalability — should guide the choice of ETL, ELT, or a hybrid approach. Consider:

- **ETL** is well-suited for structured environments and use cases that demand immediate access to clean, well-processed data.
- **ELT** shines in cloud-native or big data ecosystems where raw data flexibility and scalability are critical.

Both paradigms have their strengths, and many organizations blend elements of each. For example, a retail company might use ETL for compliance reporting while leveraging ELT for real-time inventory analysis. By understanding these paradigms within the broader DataOps framework, teams can design pipelines that meet both technical and business requirements effectively.
:::

## Tools for DataOps

Building effective and scalable data pipelines requires leveraging specialized tools at each stage of the DataOps lifecycle. From data ingestion to processing, validation, and versioning, the ecosystem of tools available is vast and continually expanding. This section provides an overview of commonly used tools, highlighting their capabilities, trade-offs, and when they might be most appropriate. Additionally, this space is dynamic, with new tools regularly introduced to address evolving challenges. Therefore, the goal is not to become tied to a specific tool but to understand the broader landscape and make informed decisions based on your pipeline's requirements.

### Data Ingestion Tools

Efficiently gathering data from diverse sources is the foundation of any pipeline. Ingestion tools vary from those tailored for real-time streaming to solutions focused on batch processing and ease of integration. Some examples include:

::: {.callout-note collapse="true"}
## [Apache Kafka](https://kafka.apache.org/)

A distributed event-streaming platform designed for high-throughput, real-time data ingestion. Kafka is widely used in applications like fraud detection, IoT data pipelines, and real-time analytics.

  - **When to Use**: Ideal for high-velocity, low-latency systems.
  - **Limitations**: Steep learning curve and resource-heavy for smaller use cases.
:::

::: {.callout-note collapse="true"}
## [Apache NiFi](https://nifi.apache.org/)

A flow-based programming tool that automates data movement and transformation with a user-friendly interface. NiFi supports a wide range of data sources and formats.

  - **When to Use**: Low-code scenarios requiring rapid integration of multiple data sources.
  - **Limitations**: Less suited for complex, high-throughput pipelines.
:::

::: {.callout-note collapse="true"}
## [Airbyte](https://github.com/airbytehq/airbyte)

An open-source data integration platform focused on moving data into data warehouses or lakes. Its extensive library of connectors simplifies ingesting data from various APIs and databases.

  - **When to Use**: Batch ingestion with diverse source compatibility.
  - **Limitations**: Primarily batch-focused and may require configuration for real-time pipelines.
:::

::: {.callout-note collapse="true"}
## [Fivetran](https://www.fivetran.com/)

A managed data integration tool that simplifies data ingestion by automating schema handling and updates. It's widely used for moving data into analytics-ready storage solutions.

  - **When to Use**: Enterprise scenarios requiring minimal management overhead for cloud data integration.
  - **Limitations**: Subscription-based pricing and limited customization compared to open-source tools.
:::

### Data Processing Tools

Processing raw data into clean, structured, and feature-rich formats is critical to preparing it for machine learning workflows. Many tools exist but the following are a few popular tools that support various scales and complexities of data transformation.


::: {.callout-note collapse="true"}
## [Apache Spark](https://spark.apache.org/)

A powerful engine for distributed data processing. Spark supports both batch and streaming data workflows, making it suitable for big data and ML pipelines.

  - **When to Use**: Processing large-scale data across distributed systems.
  - **Limitations**: Requires expertise and infrastructure, which might be overkill for smaller datasets.
:::

::: {.callout-note collapse="true"}
## [DBT (Data Build Tool)](https://www.getdbt.com/)

A transformation tool that applies SQL to prepare data for analytics workflows. DBT is particularly useful for pipelines that involve data warehouses.

  - **When to Use**: SQL-centric environments where analytics-ready data is the end goal.
  - **Limitations**: Focused on transformations within SQL databases; less versatile for non-SQL workflows.
:::

::: {.callout-note collapse="true"}
## [Polars](https://pola.rs/)

An open source Python & Rust library for data manipulation and analysis, offering rich functionality for handling structured data with performance in mind.

  - **When to Use**: Small to large datasets or for prototyping workflows. A great substitute for Pandas when performance and speed are required.
  - **Limitations**: Not designed for real-time processing.
:::

::: {.callout-note collapse="true"}
## [Prefect](https://www.prefect.io/)

A workflow orchestration tool that enables robust scheduling and monitoring of data processing tasks.

  - **When to Use**: Complex pipelines requiring orchestration of multiple processing steps.
  - **Limitations**: Adds an orchestration layer, which might be unnecessary for simple workflows.
:::

### Data Validation Tools

Ensuring the integrity and accuracy of data is essential to building reliable pipelines. Validation tools help detect anomalies, enforce schema compliance, and improve data quality. Some popular data validation tools include:

::: {.callout-note collapse="true"}
## [Great Expectations](https://greatexpectations.io/)

A framework for defining, executing, and documenting data validation checks. It integrates well with modern data stacks.

  - **When to Use**: Custom validation rules with automated reporting needs.
  - **Limitations**: May require significant customization for non-standard checks.
:::

::: {.callout-note collapse="true"}
## [TFDV (TensorFlow Data Validation)](https://github.com/tensorflow/data-validation)

A library designed for detecting anomalies and validating schema in ML datasets.

  - **When to Use**: TensorFlow-based workflows or pipelines requiring statistical validation.
  - **Limitations**: Limited applicability outside of TensorFlow-centric environments.
:::

::: {.callout-note collapse="true"}
## [Deequ](https://github.com/awslabs/deequ)

A library developed by AWS for validating large datasets using Spark. It focuses on automated quality checks and anomaly detection.

  - **When to Use**: Spark-based pipelines requiring scalable data quality checks.
  - **Limitations**: Limited compatibility with non-Spark environments.
:::

### Data Versioning Tools

Versioning ensures traceability and reproducibility by tracking changes to datasets and workflows. It is vital for debugging, compliance, and collaboration. Some common data versioning tools include:

::: {.callout-note collapse="true"}
## [DVC (Data Version Control)](https://dvc.org/)

A Git-like tool for versioning data, models, and pipelines. It integrates seamlessly with Git repositories.

  - **When to Use**: Managing medium to large datasets in collaborative environments.
  - **Limitations**: Can be challenging to set up for teams unfamiliar with Git.
:::

::: {.callout-note collapse="true"}
## [Delta Lake](https://delta.io/)

A storage layer that adds versioning and ACID transactions to data lakes. It works well with distributed systems like Apache Spark.

  - **When to Use**: Large-scale pipelines needing robust versioning and consistency.
  - **Limitations**: Requires Spark for full functionality, adding complexity to smaller-scale workflows.
:::

::: {.callout-note collapse="true"}
## [LakeFS](https://lakefs.io/)

 A Git-like version control system for data lakes. It enables branching and snapshotting of data workflows.

  - **When to Use**: Data lakes requiring advanced version control features.
  - **Limitations**: Best suited for teams already working with data lakes.
:::

### Navigating a Rapidly Evolving Ecosystem

The DataOps tooling landscape is dynamic, with new tools and frameworks regularly introduced to address emerging challenges. This constant innovation provides opportunities to enhance workflows but also demands adaptability.

- **Focus on Principles**: Rather than mastering specific tools, prioritize understanding the core concepts of data ingestion, processing, validation, and versioning. This flexibility allows you to evaluate and adopt new tools as they emerge.
- **Experiment and Iterate**: Allocate time for evaluating tools that may better align with your pipeline's evolving needs. Open-source communities and GitHub repositories are excellent resources for exploration.
- **Hybrid Tooling**: Often, no single tool will address all requirements perfectly. Combining tools—such as using Airbyte for ingestion, DBT for processing, and Great Expectations for validation—creates tailored workflows.
- **Leverage Community Support**: Most tools provide extensive documentation, active forums, and integration guides. Engage with these communities to stay informed about updates and best practices.

By focusing on the principles of DataOps and staying informed about the ever-changing ecosystem, teams can design resilient and adaptable pipelines. In the following sections, we’ll explore how to combine these tools into cohesive workflows and build scalable, efficient data pipelines for machine learning systems.

## Hands-On Example: A YouTube Data Pipeline

Now that you've learned a bit about data pipelines, common paradigms, and tooling involved, let's apply some of these concepts to create a simplified data pipeline that demonstrates how to ingest, process, and prepare YouTube data for downstream machine learning (ML) workflows. We’ll focus on collecting data like video titles, transcripts, views, likes, and comments. This example ties together concepts from previous chapters, including data ingestion, processing, validation, and versioning.

### Pipeline Design

The pipeline design we'll use will resemble more of an ETL process than an ELT.  This is mainly for storage simplicity since this pipeline is run locally (whether that be on my machine or your machine) rather than in a cloud environment where storage capacity constraints are less of an issue.

We'll use a couple of APIs to ingest our data, perform a little bit of data processing to clean and prepare our data for future analyses, and then illustrate some basic data validation along with versioning and storing our final prepared data.

```{mermaid}
%%| fig-cap: "High-level architecture of our data pipeline, which leverages an ETL process to ingest, process, validate, version and then store our data."
%%| label: fig-youtube-pipeline

flowchart LR
    subgraph ingest[Data Ingestion]
      direction LR
      subgraph p1[Ingest Video IDs]
      end
      subgraph p2[Ingest Video Stats]
      end
      subgraph p3[Ingest Video Transcript]
      end
    end
    p1 --> p2
    p1 --> p3
    ingest --> process(Process Raw Data)
    process --> Validate --> Version --> data[(Data storage)]
```

### Basic Requirements

If you'd like to follow along and execute this code then there are a few requirements you'll need on your end.

First, you'll need to make sure you have the following Python libraries installed:

{{< embed ../DataOps/youtube-data-pipeline.ipynb#dataops-rqmts >}}

Next, to simplify this example I have created helper functions to [abstract](http://localhost:4761/01-intro-ml-system.html#modularity-and-abstraction) away a lot of the finer code details.  You can see this in the following imports where I am importing helper functions from the `dataops_utils.py` module. If you want to reproduce this pipeline then you can download the dataops_utils.py script [here]().

{{< embed ../DataOps/youtube-data-pipeline.ipynb#requirements echo=true >}}

Lastly, We'll be using the YouTube Data API to extract metadata and video information. To follow along, ensure you have:

- A Google Cloud account.
- Access to the [YouTube Data API](https://developers.google.com/youtube/v3) to include an API key.  See [here to get started](https://developers.google.com/youtube/v3/getting-started).

Once you have a Youtube API key then you're ready to go!  The next code chunk sets your API key and establishes the API URL and the channel ID.  Note, I'm a golf junkie so the channel ID I am using is for [Grant Horvat](https://www.youtube.com/@GrantHorvatGolfs), a Youtube golfer with nearly 1 million followers.^[Feel free to explore a different Youtube channel. Note that the channel ID is different than the Youtube handle. For example, Grant Horvat's Youtube handle is `@GrantHorvatGolfs` but his channel ID is UCgUueMmSpcl-aCTt5CuCKQw. The easiest way to find a channel's ID is to go to the channel metadata where information for listed for "About", "Links", and "Channel details". At the bottom of the pop up window is an option to "Share Channel" and then "Copy Channel ID".]

{{< embed ../DataOps/youtube-data-pipeline.ipynb#pipeline-envars echo=true >}}

### Data Ingestion

The first step is to ingest the Youtube data.  To do so, we need to follow three steps:

1. Ingest all the video IDs provided by a given channel.
     {{< embed ../DataOps/youtube-data-pipeline.ipynb#ingest-video-ids echo=true >}}
2. Use the ingested video IDs to ingest Youtube stats for each video such as total views, likes, and comments.
     {{< embed ../DataOps/youtube-data-pipeline.ipynb#ingest-video-stats echo=true >}}
3. And lastly, ingest the transcript for each video.
     {{< embed ../DataOps/youtube-data-pipeline.ipynb#final-raw-data echo=true >}}

### Data Processing

For data preprocessing, we're going to first convert our data to a Pandas DataFrame

{{< embed ../DataOps/youtube-data-pipeline.ipynb#data-preprocess-dataframe echo=true >}}

And then clean our data by:

1. **Removing missing values**. There are a few videos with no transcript text because the videos have no talking in them.
2. **Removing duplicate observations**. Just in case there is duplication in video information during the downloading process.
3. **Remove any inconsistent data types**. This avoids errors during data processing and ensures consistency in operations applied to the data.
4. **Remove any observations that have invalid datetime values**. This ensures chronological accuracy for any time-based analysis or trends.
5. **Remove any videos with minimal number of views**. This filters out content that may not provide enough engagement data for meaningful insights.
6. **Remove any videos with very little transcript text**. This ensures that the remaining data contains sufficient content for natural language processing or text-based analysis.
7. **Clean the title and transcript text**. This removes unnecessary noise, such as non-character string values (i.e. unicode characters), making the text suitable for analysis.

{{< embed ../DataOps/youtube-data-pipeline.ipynb#data-preprocess-clean echo=true >}}

::: {.callout-warning}
While there are many advanced techniques for preprocessing text data—such as more thorough cleaning, lemmatization, or converting text into embeddings for model input—the intent of this section is to focus on building a simplified example of an end-to-end data pipeline. These additional steps can significantly enhance the quality and utility of text data, but they are outside the scope of this example, which is designed to highlight the core concepts and practical steps involved in creating a data pipeline.
:::

### Data Validation



### Data Versioning


## Creating Scalable Data Pipelines

- How to build data pipelines that scale to accommodate increasing data volumes.
- Key considerations for scalability, such as distributed processing and fault tolerance.

## Conclusion and Key Takeaways

- Summary of the key principles of DataOps and how to apply them in building effective data pipelines for ML.
- Reflection on the importance of data quality, management, and scalability in ML workflows.
