<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.13">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>8&nbsp; Model Deployment – Machine Learning Design for Business</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./09-modelops-monitoring.html" rel="next">
<link href="./07-modelops-versioning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-66ab7fd5e73b7f0a764e0d49b3e29ab1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1c2c0871e7950fd6be698d1a8ab01243.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-modelops-role.html">ModelOps</a></li><li class="breadcrumb-item"><a href="./08-modelops-deployment.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Deployment</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning Design for Business</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/bradleyboehmke/uc-bana-7075" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-Design-for-Business.epub" title="Download ePub" class="quarto-navigation-tool px-1" aria-label="Download ePub"><i class="bi bi-journal"></i></a>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Laying the Foundation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-intro-ml-system.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">ML System Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-before-we-build.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Before We Build</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">DataOps</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-dataops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Role of DataOps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-dataops-build.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Putting DataOps into Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">ModelOps</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-modelops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Role of ModelOps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-modelops-experimenting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Model Training and Experiment Tracking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-modelops-versioning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Model Versioning and Reproducibility</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-modelops-deployment.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-modelops-monitoring.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Monitoring</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">DevOps</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-devops-role.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Role of DevOps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-devops-git.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to Git and GitHub</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Human Elements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-human-side-of-ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">The Human Side of ML Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-continued-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Continuing Your Growth as an ML Practitioner</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#key-considerations" id="toc-key-considerations" class="nav-link active" data-scroll-target="#key-considerations"><span class="header-section-number">8.1</span> Key Considerations</a></li>
  <li><a href="#initial-model-deployment-strategies" id="toc-initial-model-deployment-strategies" class="nav-link" data-scroll-target="#initial-model-deployment-strategies"><span class="header-section-number">8.2</span> Initial Model Deployment Strategies</a>
  <ul class="collapse">
  <li><a href="#modes-of-inference" id="toc-modes-of-inference" class="nav-link" data-scroll-target="#modes-of-inference">Modes of Inference</a></li>
  <li><a href="#deployment-environments" id="toc-deployment-environments" class="nav-link" data-scroll-target="#deployment-environments">Deployment Environments</a></li>
  </ul></li>
  <li><a href="#tools-and-frameworks-for-model-deployment" id="toc-tools-and-frameworks-for-model-deployment" class="nav-link" data-scroll-target="#tools-and-frameworks-for-model-deployment"><span class="header-section-number">8.3</span> Tools and Frameworks for Model Deployment</a></li>
  <li><a href="#sec-model-deploy-example" id="toc-sec-model-deploy-example" class="nav-link" data-scroll-target="#sec-model-deploy-example"><span class="header-section-number">8.4</span> Hands-On Example: Deploying a Machine Learning Model</a>
  <ul class="collapse">
  <li><a href="#prerequisites" id="toc-prerequisites" class="nav-link" data-scroll-target="#prerequisites">Prerequisites</a></li>
  <li><a href="#retrieving-the-trained-model-from-mlflow" id="toc-retrieving-the-trained-model-from-mlflow" class="nav-link" data-scroll-target="#retrieving-the-trained-model-from-mlflow">Retrieving the Trained Model from MLflow</a></li>
  <li><a href="#deploy-a-fastapi-based-rest-api-for-batch-predictions" id="toc-deploy-a-fastapi-based-rest-api-for-batch-predictions" class="nav-link" data-scroll-target="#deploy-a-fastapi-based-rest-api-for-batch-predictions">Deploy a FastAPI-based REST API for batch predictions</a></li>
  <li><a href="#adding-a-streamlit-ui-for-interactive-predictions" id="toc-adding-a-streamlit-ui-for-interactive-predictions" class="nav-link" data-scroll-target="#adding-a-streamlit-ui-for-interactive-predictions">Adding a Streamlit UI for Interactive Predictions</a></li>
  <li><a href="#containerizing-the-application" id="toc-containerizing-the-application" class="nav-link" data-scroll-target="#containerizing-the-application">Containerizing the Application</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final Thoughts</a></li>
  </ul></li>
  <li><a href="#strategies-for-updating-models-in-production" id="toc-strategies-for-updating-models-in-production" class="nav-link" data-scroll-target="#strategies-for-updating-models-in-production"><span class="header-section-number">8.5</span> Strategies for <strong>Updating</strong> Models in Production</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">8.6</span> Summary</a></li>
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise"><span class="header-section-number">8.7</span> Exercise</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/edit/main/08-modelops-deployment.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-modelops-role.html">ModelOps</a></li><li class="breadcrumb-item"><a href="./08-modelops-deployment.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Deployment</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Model Deployment</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Reading Time: 45-60 minutes</p>
</div>
</div>
</div>
<p>Model deployment is the pivotal stage in the machine learning lifecycle where a trained and validated model transitions from development to production. While the earlier stages of the process—like model training, experimentation tracking, and versioning—focus on optimizing model performance and ensuring reproducibility, deployment brings these efforts to life by delivering actionable insights in real-world environments. A deployed model is not just a static artifact but an operational component that interacts with live data to provide predictions, influence decisions, and generate value for businesses.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>For example, consider our apple demand forecasting model that we’ve discussed in the previous two chapters (see <a href="06-modelops-experimenting.html#sec-experiment-tracking" class="quarto-xref">Section&nbsp;<span>6.4</span></a> and <a href="07-modelops-versioning.html#sec-model-version-example" class="quarto-xref">Section&nbsp;<span>7.4</span></a>). During development, the model was trained on historical sales data, hyperparameter tuning was documented, and the best-performing model was versioned. In deployment, this model would be integrated into the retailer’s inventory management system, where it would predict the demand for apples based on real-time needs and inputs.</p>
</div>
</div>
</div>
<p>This chapter builds upon the foundational work covered in the previous chapters. In <a href="06-modelops-experimenting.html" class="quarto-xref">Chapter&nbsp;<span>6</span></a>, we explored <strong>experiment tracking</strong>, emphasizing the importance of documenting hyperparameters, datasets, and performance metrics. <a href="07-modelops-versioning.html" class="quarto-xref">Chapter&nbsp;<span>7</span></a> introduced <strong>model versioning</strong>, ensuring traceability and reproducibility of model iterations. Now, we turn our attention to <strong>deploying models</strong>, bridging the gap between experimentation and real-world application. Effective deployment not only ensures that the best-performing models are operationalized but also lays the groundwork for scalability, reliability, and business impact.</p>
<p>Model deployment involves a diverse range of considerations, from choosing the right infrastructure to implementing strategies that ensure minimal disruption during updates. This chapter will provide an introductory understanding of deployment principles, explore popular tools and frameworks, and offer a hands-on example that builds on the apple forecasting problem from previous chapters. By the end of this chapter, you’ll have a roadmap for turning your well-tracked and versioned models into deployed solutions that deliver consistent and reliable results.</p>
<section id="key-considerations" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="key-considerations"><span class="header-section-number">8.1</span> Key Considerations</h2>
<p>Deploying machine learning models effectively requires a strategic approach to address a variety of technical and operational factors. A successful deployment ensures that models operate efficiently, scale to meet demand, and integrate seamlessly with existing systems. Below, we outline the key considerations for model deployment and their implications in real-world scenarios.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Scalability
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>What It Means</strong>: Scalability is the ability of a system to handle increased demand without performance degradation. For ML models, this means ensuring that the model can process growing volumes of data or user requests efficiently.</p></li>
<li><p><strong>Why It Matters</strong>: Scalability ensures the system remains functional and responsive during high-demand periods, such as peak usage times for an e-commerce site or when serving real-time predictions for millions of IoT devices.</p></li>
<li><p><strong>Example</strong>: During a Black Friday sale, an e-commerce platform’s recommendation engine may need to serve 2-5 times more shoppers than normal. This ability to scale due to increased traffic allows the system to generate personalized product recommendations quickly and accurately regardless of surges in demand.</p></li>
<li><p><strong>How to Address</strong>:</p>
<ul>
<li><strong>Cloud Scalability</strong>: Use platforms like AWS Lambda or Google Cloud Functions to automatically scale resources based on traffic. For example, serverless architecture can allocate compute resources dynamically during peak demand.</li>
<li><strong>Kubernetes</strong>: Deploy models in containers managed by Kubernetes to ensure automatic scaling, load balancing, and fault tolerance across distributed systems.</li>
<li><strong>Distributed Inference Frameworks</strong>: Utilize frameworks like TensorFlow Serving or Triton Inference Server to handle high-throughput predictions by distributing workload efficiently across machines.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Latency
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>What It Means</strong>: Latency is the time between sending an input to the model and receiving the prediction. Low latency is critical for applications requiring real-time decision-making.</p></li>
<li><p><strong>Why It Matters</strong>: High latency can make systems impractical or even dangerous in time-sensitive scenarios, such as fraud detection, autonomous driving, or live video analytics.</p></li>
<li><p><strong>Example</strong>: A credit card fraud detection system must deliver predictions within milliseconds to either approve or deny transactions, preventing fraudulent activities while minimizing user inconvenience.</p></li>
<li><p><strong>How to Address</strong>:</p>
<ul>
<li><strong>Model Optimization</strong>: Techniques like quantization and pruning can reduce the size and complexity of a model.
<ul>
<li><em>Quantization</em>: Converts model weights from high-precision (e.g., 32-bit) to lower-precision formats (e.g., 8-bit), reducing computation time without significant loss of accuracy. Read more about quantization <a href="https://huggingface.co/docs/optimum/en/concept_guides/quantization">here</a>.</li>
<li><em>Pruning</em>: Removes redundant weights or neurons from the model to make it smaller and faster. Read more about pruning <a href="https://wandb.ai/authors/pruning/reports/Diving-Into-Model-Pruning-in-Deep-Learning--VmlldzoxMzcyMDg">here</a>.</li>
</ul></li>
<li><strong>Edge Computing</strong>: Deploy the model closer to where data is generated (e.g., on IoT devices or edge servers). For instance, deploying a vision model directly on a camera for anomaly detection avoids the latency of sending data to a central server.</li>
<li><strong>Hardware Acceleration</strong>: Use GPUs, TPUs, or dedicated inference accelerators to speed up computations, particularly for complex models like deep neural networks.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reliability
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>What It Means</strong>: Reliability ensures that a deployed model consistently delivers accurate predictions under varying conditions, such as hardware failures, software bugs, or unexpected inputs.</p></li>
<li><p><strong>Why It Matters</strong>: Unreliable models can undermine trust, disrupt workflows, and result in financial or reputational losses. For example, a malfunctioning model in a supply chain system could lead to delayed shipments or stockouts.</p></li>
<li><p><strong>Example</strong>: A healthcare diagnostic model must provide consistent and dependable predictions even when faced with missing or incomplete patient data.</p></li>
<li><p><strong>How to Address</strong>:</p>
<ul>
<li><strong>Redundancy</strong>: Use multiple instances of the model in a failover setup, ensuring that if one instance fails, another can take over seamlessly.</li>
<li><strong>Testing and Validation</strong>: Conduct rigorous testing, including stress tests, edge-case analysis, and integration testing, to identify and fix potential vulnerabilities.</li>
<li><strong>Error Handling</strong>: Build robust error-handling mechanisms to gracefully manage anomalies, such as input data errors or system outages, without crashing the application.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deployment Environment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>What It Means</strong>: The deployment environment refers to the infrastructure where the model operates, such as cloud platforms, on-premises servers, or edge devices.</p></li>
<li><p><strong>Why It Matters</strong>: The choice of environment impacts cost, latency, scalability, and integration with other systems.</p></li>
<li><p><strong>Example</strong>: An agriculture company deploys a model for soil health prediction on edge devices in remote fields, allowing real-time analysis without requiring constant internet connectivity.</p></li>
<li><p><strong>How to Address</strong>:</p>
<ul>
<li><strong>Cloud Deployment</strong>: Ideal for scalable and flexible applications, cloud platforms like AWS SageMaker or Azure ML provide extensive tooling for deployment and monitoring.</li>
<li><strong>On-Premises Deployment</strong>: Necessary for industries like finance or healthcare, where data privacy regulations require keeping data within a secure local environment.</li>
<li><strong>Edge Deployment</strong>: Use lightweight frameworks like TensorFlow Lite or ONNX Runtime to deploy models on devices with limited compute power, such as IoT sensors or mobile phones.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Integration with Existing Systems
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>What It Means</strong>: Models often need to interact with other systems, including databases, APIs, and business applications.</p></li>
<li><p><strong>Why It Matters</strong>: Poor integration can create bottlenecks and limit the usability of the deployed model, reducing its value to stakeholders.</p></li>
<li><p><strong>Example</strong>: A customer churn prediction model integrated with a CRM system should allow sales teams to view at-risk customers directly in their dashboard and take action to retain them.</p></li>
<li><p><strong>How to Address</strong>:</p>
<ul>
<li><strong>APIs</strong>: Serve models as RESTful APIs using tools like Flask, FastAPI, or TensorFlow Serving, making them accessible to other applications.</li>
<li><strong>Middleware</strong>: Use middleware to bridge the model with existing systems, translating predictions into actionable insights for end users.</li>
<li><strong>Collaboration with Engineers</strong>: Work closely with software engineers to ensure seamless integration into the organization’s ecosystem, including compliance with existing infrastructure standards.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<p>Model deployment is a highly collaborative effort that relies on the expertise of engineering teams to help address many of the technical complexities of integrating machine learning models into production. Although data scientists are becoming more involved in this process, you should not expect to go it alone! Rather, look for opportunities to collaborate with engineering teams as they can simplify your life. Some key areas that engineering support can help include:</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Integration with Legacy Systems
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Many organizations rely on outdated systems that require engineering expertise to adapt model outputs to existing formats.</p>
<p><strong>Example</strong>: A fraud detection model producing probabilities may need middleware to translate results into binary outputs like “fraud” or “not fraud” for compatibility.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Infrastructure Compatibility
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Engineers can help optimize deployment environments, whether models run on cloud platforms, on-premises servers, or edge devices.</p>
<p><strong>Example</strong>: Deploying a recommendation model on a cloud platform with auto-scaling ensures responsiveness during traffic surges.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Security and Compliance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Engineers can implement secure data pipelines and access controls to ensure models comply with standards like GDPR or HIPAA.</p>
<p><strong>Example</strong>: Encrypting communication channels for a healthcare model ensures patient data is protected.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Performance Optimization
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Engineers can help apply hardware-specific optimizations, such as leveraging GPUs or edge accelerators, to enhance model efficiency.</p>
<p><strong>Example</strong>: Optimizing a drone-based object detection model with TensorRT reduces inference latency for real-time decision-making.</p>
</div>
</div>
</div>
<p>By leveraging engineering expertise, you’ll be able to deploy models that are not only functional but also scalable, secure, and seamlessly integrated into production systems.</p>
</section>
<section id="initial-model-deployment-strategies" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="initial-model-deployment-strategies"><span class="header-section-number">8.2</span> Initial Model Deployment Strategies</h2>
<p>Deploying a machine learning model is not just about writing code—it involves critical decisions about how and where the model will operate. These choices influence latency, scalability, cost, and user experience. This section focuses on strategies for deploying models for inference, including modes of inference and deployment environments, and the factors that drive these decisions.</p>
<section id="modes-of-inference" class="level3">
<h3 class="anchored" data-anchor-id="modes-of-inference">Modes of Inference</h3>
<p>Choosing the right inference mode is crucial for deploying machine learning models effectively. The inference mode determines how predictions are generated, influencing the model’s responsiveness, computational requirements, and overall system design. Below, we explore the three primary modes of inference—batch, real-time, and hybrid—providing detailed descriptions, examples, and implications for deployment strategies.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Batch Inference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Batch inference involves generating predictions for a large set of data at scheduled intervals or in response to a specific request. This approach works well for scenarios where predictions do not need to be instantaneous and can be processed in bulk. Batch inference typically utilizes batch processing systems like Apache Spark, Hadoop, or cloud services like AWS Batch.</p>
<section id="examples" class="level5">
<h5 class="anchored" data-anchor-id="examples"><strong>Examples</strong></h5>
<ul>
<li><strong>Retail Forecasting</strong>: A grocery retailer generates weekly demand forecasts for apples to ensure proper inventory management. Using historical sales data, the system runs batch inference every Sunday night, delivering predictions in time for Monday’s produce order.</li>
<li><strong>Fraud Detection Reports</strong>: A bank processes transactional data overnight to flag potentially fraudulent activities. While immediate detection might be handled by real-time inference, deeper risk analysis is done in batches to provide a comprehensive view.</li>
<li><strong>Customer Segmentation</strong>: A marketing team processes customer data monthly to identify new segments for targeted campaigns.</li>
</ul>
</section>
<section id="impact-on-deployment-strategy" class="level5">
<h5 class="anchored" data-anchor-id="impact-on-deployment-strategy"><strong>Impact on Deployment Strategy</strong></h5>
<ul>
<li><strong>Infrastructure</strong>: Batch inference systems typically require powerful computing resources but only for scheduled durations. This makes it cost-effective for use cases with predictable demand.</li>
<li><strong>Latency</strong>: Predictions are generated with a delay, so batch inference is unsuitable for time-sensitive applications.</li>
<li><strong>Scaling</strong>: Scaling is easier because the system doesn’t need to maintain always-on infrastructure; it processes large volumes of data during specific windows.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Real-Time Inference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Real-time inference generates predictions as new data becomes available, providing immediate responses. This mode is essential for applications where decisions must be made within milliseconds or seconds. Real-time systems often leverage REST APIs, gRPC, or WebSocket-based endpoints to deliver low-latency predictions.</p>
<section id="examples-1" class="level5">
<h5 class="anchored" data-anchor-id="examples-1"><strong>Examples</strong></h5>
<ul>
<li><strong>Autonomous Vehicles</strong>: Vehicles process sensor data in real time to detect obstacles, predict traffic patterns, and make driving decisions within milliseconds.</li>
<li><strong>E-Commerce Recommendations</strong>: An online shopping platform displays personalized product recommendations as users browse, enhancing the shopping experience and boosting sales.</li>
<li><strong>Fraud Detection</strong>: A payment processor analyzes transactions in real time to block potentially fraudulent activities before they are completed.</li>
</ul>
</section>
<section id="impact-on-deployment-strategy-1" class="level5">
<h5 class="anchored" data-anchor-id="impact-on-deployment-strategy-1"><strong>Impact on Deployment Strategy</strong></h5>
<ul>
<li><strong>Infrastructure</strong>: Real-time inference requires always-on systems with low-latency processing capabilities. This often involves deploying models on scalable platforms like Kubernetes or cloud-native services such as AWS SageMaker.</li>
<li><strong>Latency and Reliability</strong>: The system must meet strict latency requirements while maintaining high availability. Failures can have immediate and significant business impact.</li>
<li><strong>Scaling</strong>: Real-time systems need to scale dynamically to handle variable loads, such as during peak shopping hours or flash sales.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hybrid Inference
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Hybrid inference combines batch and real-time inference to meet diverse requirements within a single system. This approach allows organizations to optimize costs and performance by aligning inference strategies with specific use cases.</p>
<section id="examples-2" class="level5">
<h5 class="anchored" data-anchor-id="examples-2"><strong>Examples</strong></h5>
<ul>
<li><strong>News Recommendation System</strong>: A media platform precomputes general recommendations overnight using batch inference while delivering real-time personalized suggestions based on user activity.</li>
<li><strong>Energy Management</strong>: A utility company predicts daily power demand using batch inference but adjusts its predictions in real time to account for sudden changes in weather or energy usage.</li>
<li><strong>Dynamic Pricing</strong>: An airline uses batch inference to update base ticket prices daily while employing real-time inference to adjust prices dynamically based on seat availability and user demand.</li>
</ul>
</section>
<section id="impact-on-deployment-strategy-2" class="level5">
<h5 class="anchored" data-anchor-id="impact-on-deployment-strategy-2"><strong>Impact on Deployment Strategy</strong></h5>
<ul>
<li><strong>Infrastructure</strong>: Hybrid systems require flexible infrastructure capable of supporting both batch and real-time processing. This often involves integrating batch processing frameworks with real-time serving platforms.</li>
<li><strong>Cost Management</strong>: By leveraging batch inference for non-urgent tasks and real-time inference for critical processes, hybrid systems can optimize resource usage and reduce operational costs.</li>
<li><strong>Complexity</strong>: Managing a hybrid system adds complexity, as teams must ensure smooth integration and data consistency between batch and real-time workflows.</li>
</ul>
</section>
</div>
</div>
</div>
<p>The choice of inference mode is driven by the application’s business requirements, technical constraints, and cost considerations. Batch inference is suitable for predictable, non-urgent tasks, while real-time inference excels in time-sensitive scenarios. Hybrid inference offers the flexibility to address both needs but requires careful planning and infrastructure design. By understanding the trade-offs and aligning them with deployment goals, organizations can build systems that balance efficiency, responsiveness, and scalability.</p>
</section>
<section id="deployment-environments" class="level3">
<h3 class="anchored" data-anchor-id="deployment-environments">Deployment Environments</h3>
<p>The deployment environment for machine learning models is a critical decision that influences performance, scalability, cost, and maintenance. Deployment environments can be broadly categorized into <strong>cloud-based</strong>, <strong>on-premises</strong>, and <strong>edge</strong> setups. The choice of environment depends on the application’s requirements, infrastructure constraints, and mode of inference (batch, real-time, or hybrid). Below, we explore each environment in detail.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cloud-Based Deployment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Cloud-based deployment involves hosting machine learning models on platforms provided by cloud service providers such as AWS, Google Cloud Platform (GCP), or Microsoft Azure. These environments offer a wide range of managed services for storage, compute, and networking, reducing the operational burden on organizations. Cloud platforms support serverless deployments, containerization, and scalable architectures to meet diverse needs.</p>
<section id="use-cases" class="level5">
<h5 class="anchored" data-anchor-id="use-cases"><strong>Use Cases</strong></h5>
<ul>
<li><strong>Batch Inference</strong>: Models deployed in cloud environments can leverage large-scale compute resources for scheduled tasks like generating daily sales forecasts or processing customer sentiment analysis.</li>
<li><strong>Real-Time Inference</strong>: Cloud environments are ideal for deploying APIs that deliver low-latency predictions, such as fraud detection in payment systems or personalized e-commerce recommendations.</li>
<li><strong>Hybrid Inference</strong>: Combining batch processing with real-time APIs is seamless in the cloud, enabling use cases like dynamic pricing systems that precompute baseline prices and adjust them in real time.</li>
</ul>
</section>
<section id="advantages" class="level5">
<h5 class="anchored" data-anchor-id="advantages"><strong>Advantages</strong></h5>
<ul>
<li><strong>Scalability</strong>: Cloud platforms provide elastic scaling to handle variable workloads, whether processing massive datasets in batch mode or handling high traffic for real-time APIs.</li>
<li><strong>Integration</strong>: Cloud services often integrate with data lakes, pipelines, and other tools, making it easier to manage end-to-end ML workflows.</li>
<li><strong>Managed Services</strong>: Features like automatic failover, logging, and monitoring simplify operations and improve reliability.</li>
</ul>
</section>
<section id="considerations" class="level5">
<h5 class="anchored" data-anchor-id="considerations"><strong>Considerations</strong></h5>
<ul>
<li><strong>Cost</strong>: While cloud solutions are flexible, costs can escalate with high data transfer or prolonged resource usage.</li>
<li><strong>Latency</strong>: For real-time applications requiring ultra-low latency, the physical location of the cloud data center relative to the end user is a critical factor.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
On-Premises Deployment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>On-premises deployment involves hosting machine learning models within an organization’s own data centers or servers. This setup gives organizations complete control over their infrastructure and is often used when regulatory, security, or cost concerns limit cloud adoption.</p>
<section id="use-cases-1" class="level5">
<h5 class="anchored" data-anchor-id="use-cases-1"><strong>Use Cases</strong></h5>
<ul>
<li><strong>Batch Inference</strong>: On-premises setups are well-suited for organizations with robust internal infrastructure that periodically runs resource-intensive jobs, such as credit scoring or generating compliance reports.</li>
<li><strong>Real-Time Inference</strong>: Industries like banking or healthcare, where data privacy is paramount, often deploy real-time models on-premises to ensure sensitive information does not leave controlled environments.</li>
<li><strong>Hybrid Inference</strong>: On-premises systems can combine batch and real-time processing, such as precomputing analytics reports while using real-time models for user-facing applications.</li>
</ul>
</section>
<section id="advantages-1" class="level5">
<h5 class="anchored" data-anchor-id="advantages-1"><strong>Advantages</strong></h5>
<ul>
<li><strong>Data Security</strong>: On-premises deployments offer complete control over data, reducing exposure to potential breaches or unauthorized access.</li>
<li><strong>Regulatory Compliance</strong>: Many industries, such as healthcare (HIPAA) and finance (PCI DSS), require data to be stored and processed within specific geographic or organizational boundaries, making on-premises deployment a necessity.</li>
<li><strong>Cost Management</strong>: For organizations with existing infrastructure, on-premises deployment can reduce long-term operational costs.</li>
</ul>
</section>
<section id="considerations-1" class="level5">
<h5 class="anchored" data-anchor-id="considerations-1"><strong>Considerations</strong></h5>
<ul>
<li><strong>Scalability</strong>: Scaling on-premises systems requires significant investment in hardware and infrastructure, making it less flexible than cloud solutions.</li>
<li><strong>Maintenance</strong>: Organizations are responsible for hardware upkeep, system updates, and overall reliability, increasing operational complexity.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Edge Deployment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Edge deployment involves deploying machine learning models directly on devices close to where the data is generated. These devices include IoT sensors, mobile devices, or specialized hardware like NVIDIA Jetson or Raspberry Pi. Edge deployment minimizes latency and reduces the need for continuous internet connectivity.</p>
<section id="use-cases-2" class="level5">
<h5 class="anchored" data-anchor-id="use-cases-2"><strong>Use Cases</strong></h5>
<ul>
<li><strong>Real-Time Inference</strong>: Edge environments are critical for time-sensitive applications such as autonomous vehicles, where models must process sensor data and make decisions instantaneously.</li>
<li><strong>Batch Inference</strong>: Though less common, edge devices can periodically compute predictions in bulk for local use, such as environmental monitoring stations summarizing daily sensor readings.</li>
<li><strong>Hybrid Inference</strong>: Edge devices can preprocess data locally and send aggregated results to the cloud for deeper analysis or batch processing.</li>
</ul>
</section>
<section id="advantages-2" class="level5">
<h5 class="anchored" data-anchor-id="advantages-2"><strong>Advantages</strong></h5>
<ul>
<li><strong>Low Latency</strong>: By processing data locally, edge deployments eliminate the delay introduced by sending data to a central server.</li>
<li><strong>Reduced Bandwidth</strong>: Edge deployments reduce the amount of data transmitted over networks, lowering operational costs and improving efficiency in bandwidth-constrained environments.</li>
<li><strong>Autonomy</strong>: Edge devices can operate in environments with intermittent or no internet connectivity, such as remote oil rigs or disaster response zones.</li>
</ul>
</section>
<section id="considerations-2" class="level5">
<h5 class="anchored" data-anchor-id="considerations-2"><strong>Considerations</strong></h5>
<ul>
<li><strong>Resource Constraints</strong>: Edge devices have limited computational power, storage, and memory, requiring optimized models through techniques like quantization, pruning, or distillation.</li>
<li><strong>Complexity</strong>: Deploying and maintaining models across a distributed network of edge devices adds significant operational challenges.</li>
</ul>
</section>
</div>
</div>
</div>
<p>The choice of inference mode significantly influences the selection of a deployment environment:</p>
<ol type="1">
<li><strong>Batch Inference</strong>:
<ul>
<li>Best suited for <strong>cloud-based</strong> or <strong>on-premises</strong> environments where ample resources can be allocated during specific timeframes.</li>
<li>Edge deployments for batch inference are rare but can be effective in localized systems.</li>
</ul></li>
<li><strong>Real-Time Inference</strong>:
<ul>
<li><strong>Cloud-based</strong> environments are preferred for applications needing dynamic scalability and global accessibility.</li>
<li><strong>On-premises</strong> environments are necessary for scenarios with strict data privacy or compliance requirements.</li>
<li><strong>Edge deployments</strong> excel when ultra-low latency and autonomy are critical, such as in healthcare wearables or autonomous drones.</li>
</ul></li>
<li><strong>Hybrid Inference</strong>:
<ul>
<li>Combines the strengths of different environments. For example, edge devices preprocess data for real-time use, while the cloud handles batch analytics for broader insights.</li>
</ul></li>
</ol>
<p>Selecting a deployment environment involves balancing business needs, technical requirements, and cost considerations. Cloud platforms offer flexibility and scalability for most applications, while on-premises setups ensure control and compliance. Edge deployment stands out for latency-critical and bandwidth-sensitive scenarios. Understanding these environments and their alignment with inference modes empowers teams to make informed decisions, ensuring efficient and effective model deployments.</p>
</section>
</section>
<section id="tools-and-frameworks-for-model-deployment" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="tools-and-frameworks-for-model-deployment"><span class="header-section-number">8.3</span> Tools and Frameworks for Model Deployment</h2>
<p>The model deployment landscape is vast, with numerous tools and frameworks available to address diverse deployment needs. This space is also constantly evolving, with new technologies and innovations emerging regularly. While you may never use the majority of the tools listed here, understanding their strengths and use cases will enable you to make informed decisions about which tools to adopt for a given deployment scenario. Below, we explore some of the most popular tools, discussing their features, examples of real-world use, and when they are preferred.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Docker
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview" class="level5">
<h5 class="anchored" data-anchor-id="overview"><strong>Overview</strong></h5>
<p><a href="https://www.docker.com/">Docker</a> is a containerization platform that packages applications and their dependencies into isolated, portable containers. For machine learning models, Docker ensures consistent behavior across various environments, from development to production.</p>
</section>
<section id="features" class="level5">
<h5 class="anchored" data-anchor-id="features"><strong>Features</strong></h5>
<ul>
<li>Encapsulates models, dependencies, and runtime environments.</li>
<li>Enables seamless portability across operating systems and infrastructures.</li>
<li>Simplifies scaling and orchestration when combined with platforms like Kubernetes.</li>
</ul>
</section>
<section id="example" class="level5">
<h5 class="anchored" data-anchor-id="example"><strong>Example</strong></h5>
<p>An insurance company deploys a fraud detection model using Docker. By containerizing the model and its dependencies, the company ensures that the model behaves the same in local testing as in the cloud production environment. This consistency prevents environment-specific bugs that could affect predictions.</p>
</section>
<section id="when-preferred" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred"><strong>When Preferred</strong></h5>
<ul>
<li>Ideal for teams working in environments with diverse infrastructure, such as a mix of on-premises and cloud systems.</li>
<li>Preferred when reproducibility and portability are key requirements.</li>
<li>Useful for applications that may later need to integrate into a microservices architecture.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Kubernetes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview-1" class="level5">
<h5 class="anchored" data-anchor-id="overview-1"><strong>Overview</strong></h5>
<p><a href="https://kubernetes.io/">Kubernetes</a> is an open-source orchestration system designed to automate the deployment, scaling, and management of containerized applications. It provides advanced capabilities for running machine learning models in distributed environments.</p>
</section>
<section id="features-1" class="level5">
<h5 class="anchored" data-anchor-id="features-1"><strong>Features</strong></h5>
<ul>
<li>Offers auto-scaling, load balancing, and fault tolerance.</li>
<li>Supports rolling updates and rollbacks for seamless model version transitions.</li>
<li>Integrates with ML-specific tools like <a href="https://www.kubeflow.org/">Kubeflow</a> for managing end-to-end pipelines.</li>
</ul>
</section>
<section id="example-1" class="level5">
<h5 class="anchored" data-anchor-id="example-1"><strong>Example</strong></h5>
<p>A global e-commerce company deploys its recommendation system using Kubernetes. The system dynamically scales to handle increased traffic during holiday sales, ensuring low latency for real-time predictions, even as user demand spikes.</p>
</section>
<section id="when-preferred-1" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred-1"><strong>When Preferred</strong></h5>
<ul>
<li>Best for organizations needing to scale machine learning models across large user bases or high-traffic applications.</li>
<li>Ideal for managing multiple models simultaneously in production, particularly in microservices environments.</li>
<li>Preferred when reliability and high availability are critical, as Kubernetes ensures that containerized applications recover automatically from failures.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TensorFlow Serving
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview-2" class="level5">
<h5 class="anchored" data-anchor-id="overview-2"><strong>Overview</strong></h5>
<p><a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> is a flexible, high-performance serving system designed specifically for deploying TensorFlow models. It is optimized for production environments requiring low-latency predictions.</p>
</section>
<section id="features-2" class="level5">
<h5 class="anchored" data-anchor-id="features-2"><strong>Features</strong></h5>
<ul>
<li>Provides built-in support for gRPC and REST APIs for serving models.</li>
<li>Allows dynamic loading and versioning of models without restarting the service.</li>
<li>Supports TensorFlow-specific optimizations for inference performance.</li>
</ul>
</section>
<section id="example-2" class="level5">
<h5 class="anchored" data-anchor-id="example-2"><strong>Example</strong></h5>
<p>A healthcare startup deploys a TensorFlow-based medical imaging model using TensorFlow Serving. The system handles real-time X-ray image inputs and delivers diagnostic predictions to doctors within seconds, ensuring rapid decision-making in critical care situations.</p>
</section>
<section id="when-preferred-2" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred-2"><strong>When Preferred</strong></h5>
<ul>
<li>Ideal for teams already using TensorFlow in their workflows and seeking seamless integration.</li>
<li>Best for real-time inference applications where low latency is critical.</li>
<li>Suitable for environments that require robust model version management and frequent updates.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
FastAPI
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview-3" class="level5">
<h5 class="anchored" data-anchor-id="overview-3"><strong>Overview</strong></h5>
<p><a href="https://fastapi.tiangolo.com/">FastAPI</a> is a modern Python web framework for building APIs. It is lightweight and easy to use, making it a popular choice for deploying machine learning models as REST endpoints.</p>
</section>
<section id="features-3" class="level5">
<h5 class="anchored" data-anchor-id="features-3"><strong>Features</strong></h5>
<ul>
<li>Provides high performance using asynchronous programming.</li>
<li>Auto-generates API documentation via OpenAPI standards.</li>
<li>Supports seamless integration with Python-based ML libraries like Scikit-learn, PyTorch, and TensorFlow.</li>
</ul>
</section>
<section id="example-3" class="level5">
<h5 class="anchored" data-anchor-id="example-3"><strong>Example</strong></h5>
<p>A fintech startup deploys a linear regression model using FastAPI to predict stock prices based on market indicators. The lightweight API serves predictions to a front-end dashboard used by financial analysts, with sub-second latency.</p>
</section>
<section id="when-preferred-3" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred-3"><strong>When Preferred</strong></h5>
<ul>
<li>Best for rapid prototyping of machine learning APIs or lightweight deployments.</li>
<li>Suitable for small to medium-scale applications where simplicity and ease of development are priorities.</li>
<li>Useful for internal testing and experimentation before moving to larger-scale deployments.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MLflow
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview-4" class="level5">
<h5 class="anchored" data-anchor-id="overview-4"><strong>Overview</strong></h5>
<p><a href="https://mlflow.org/">MLflow</a> is a comprehensive machine learning lifecycle platform that integrates model tracking, reproducibility, and deployment. Its model-serving capabilities simplify transitioning from experimentation to production.</p>
</section>
<section id="features-4" class="level5">
<h5 class="anchored" data-anchor-id="features-4"><strong>Features</strong></h5>
<ul>
<li>Provides direct deployment of models as REST APIs.</li>
<li>Supports exporting models to various formats for compatibility with other tools.</li>
<li>Tracks and manages deployed model versions for streamlined monitoring and updates.</li>
</ul>
</section>
<section id="example-4" class="level5">
<h5 class="anchored" data-anchor-id="example-4"><strong>Example</strong></h5>
<p>A retail analytics company deploys a demand forecasting model using MLflow. The platform ensures that deployed models are linked to their training runs, providing traceability for model decisions and making it easy to redeploy improved versions.</p>
</section>
<section id="when-preferred-4" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred-4"><strong>When Preferred</strong></h5>
<ul>
<li>Ideal for teams already using MLflow for experimentation and tracking.</li>
<li>Best for managing multiple model versions and maintaining clear lineage.</li>
<li>Suitable for small to medium-scale deployments that don’t require advanced orchestration.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
BentoML
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview-5" class="level5">
<h5 class="anchored" data-anchor-id="overview-5"><strong>Overview</strong></h5>
<p><a href="https://bentoml.com/">BentoML</a> is a specialized framework for packaging and deploying machine learning models as optimized APIs. It focuses on simplifying the process of creating production-grade endpoints.</p>
</section>
<section id="features-5" class="level5">
<h5 class="anchored" data-anchor-id="features-5"><strong>Features</strong></h5>
<ul>
<li>Packages models along with their dependencies into standalone bundles.</li>
<li>Offers customizable REST or gRPC APIs for serving models.</li>
<li>Provides tools for optimizing inference performance through pipeline support.</li>
</ul>
</section>
<section id="example-5" class="level5">
<h5 class="anchored" data-anchor-id="example-5"><strong>Example</strong></h5>
<p>A logistics company deploys a delivery route optimization model using BentoML. The system bundles the model with preprocessing logic, ensuring efficient and consistent predictions across the fleet’s onboard devices.</p>
</section>
<section id="when-preferred-5" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred-5"><strong>When Preferred</strong></h5>
<ul>
<li>Best for applications with complex preprocessing or post-processing requirements.</li>
<li>Suitable for teams seeking high-performance APIs for inference with minimal setup.</li>
<li>Ideal for serving multiple models within a single application.</li>
</ul>
</section>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
AWS SageMaker
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="overview-6" class="level5">
<h5 class="anchored" data-anchor-id="overview-6"><strong>Overview</strong></h5>
<p><a href="https://aws.amazon.com/sagemaker/">AWS SageMaker</a> is a fully managed machine learning service that simplifies the deployment and scaling of models. It integrates deeply with other AWS services to provide a seamless workflow.</p>
</section>
<section id="features-6" class="level5">
<h5 class="anchored" data-anchor-id="features-6"><strong>Features</strong></h5>
<ul>
<li>Supports one-click deployment of models as scalable endpoints.</li>
<li>Monitors endpoint performance and integrates with other AWS tools for analytics and security.</li>
<li>Offers cost-effective options like multi-model endpoints.</li>
</ul>
</section>
<section id="example-6" class="level5">
<h5 class="anchored" data-anchor-id="example-6"><strong>Example</strong></h5>
<p>A media company deploys a content recommendation model on SageMaker. The model predicts user preferences in real time and scales automatically to serve millions of users globally during peak streaming hours.</p>
</section>
<section id="when-preferred-6" class="level5">
<h5 class="anchored" data-anchor-id="when-preferred-6"><strong>When Preferred</strong></h5>
<ul>
<li>Ideal for organizations heavily invested in the AWS ecosystem.</li>
<li>Best for scaling ML models globally with minimal operational overhead.</li>
<li>Suitable for use cases requiring integration with other AWS tools like Lambda or S3.</li>
</ul>
</section>
</div>
</div>
</div>
<p>The right tool depends on several factors:</p>
<ul>
<li><strong>Inference Requirements</strong>: Tools like TensorFlow Serving excel in low-latency real-time inference, while BentoML handles complex pipelines effectively.</li>
<li><strong>Scalability</strong>: Kubernetes is the go-to choice for large-scale, distributed deployments, whereas FastAPI is perfect for lightweight use cases.</li>
<li><strong>Integration Needs</strong>: AWS SageMaker is ideal for teams already using AWS services, while Docker is better for hybrid environments.</li>
<li><strong>Ecosystem Fit</strong>: MLflow works seamlessly for teams already using it for experiment tracking.</li>
</ul>
<p>By understanding your deployment requirements and the strengths of these frameworks, you can select the right combination to ensure your models are deployed efficiently and effectively.</p>
</section>
<section id="sec-model-deploy-example" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="sec-model-deploy-example"><span class="header-section-number">8.4</span> Hands-On Example: Deploying a Machine Learning Model</h2>
<p>In this hands-on example, we’ll build on the apple demand forecasting model from previous chapters, demonstrating how to deploy it for <strong>real-time</strong> or <strong>batch inference</strong> using <strong>MLFlow</strong>, <strong>FastAPI</strong>, <strong>Streamlit</strong> and <strong>Docker</strong>. This example provides a practical, simplified view of how to bring a trained model into production-like environments.</p>
<p>The goal of this exercise is to simulate a complete deployment pipeline that includes:</p>
<ul>
<li>Retrieving the trained model from MLflow, ensuring versioning and reproducibility.</li>
<li>Setting up a REST API to enable batch predictions on uploaded datasets.</li>
<li>Containerizing the application using Docker for portability and scalability.</li>
</ul>
<p>While this example focuses on a relatively straightforward deployment approach, the principles and tools we’ll use—such as creating endpoints, handling model artifacts, and containerization—are foundational to real-world deployment strategies. In production, deployments often involve additional complexities, such as real-time inference, load balancing, and integration with orchestration tools like Kubernetes. However, starting with this hands-on example will help you grasp the basics and understand the core workflow of model deployment.</p>
<p>By the end of this example, you’ll:</p>
<ul>
<li>Deploy a FastAPI-based REST API for batch predictions.</li>
<li>Understand how Docker enables scalable and reproducible deployments.</li>
<li>Gain insight into connecting deployment strategies to the considerations discussed earlier in this chapter.</li>
</ul>
<p>Let’s dive in and take the first step toward deploying the apple demand forecasting model!</p>
<div class="callout callout-style-simple callout-warning">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>This section is more technical than previous hands-on examples, as it involves setting up a FastAPI, integrating it with an MLflow-registered model, and containerizing the application with Docker. I encourage you to challenge yourself by actively following along with the implementation. However, if some of these steps feel overwhelming, don’t worry—at a minimum, carefully reading through this section will provide valuable insights into the complexities of deploying machine learning models. Even this simplified example demonstrates the many moving parts involved in transitioning a model from experimentation to a fully deployed system. Understanding these challenges is a crucial step toward mastering ModelOps and collaborating effectively with engineering teams in real-world deployments. 🚀</p>
</div>
</div>
</div>
<section id="prerequisites" class="level3">
<h3 class="anchored" data-anchor-id="prerequisites">Prerequisites</h3>
<p>Before diving into the hands-on example, ensure that you have the following prerequisites in place:</p>
<section id="required-tools-and-libraries" class="level4">
<h4 class="anchored" data-anchor-id="required-tools-and-libraries">1. Required Tools and Libraries</h4>
<p>To replicate this hands-on exercise on your end, you’ll need to Docker installed. You can either install <a href="https://docs.docker.com/get-docker/">Docker Desktop</a> or <a href="https://docs.docker.com/engine/install/">Docker Engine</a>. To verify you have Docker installed you can always run <code>docker version</code> in your terminal.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> version</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Client:</span> Docker Engine <span class="at">-</span> Community</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> <span class="ex">Version:</span>           27.5.1</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a> <span class="ex">API</span> version:       1.47</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a> <span class="ex">Go</span> version:        go1.23.5</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a> <span class="ex">Git</span> commit:        9f9e405801</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a> <span class="ex">Built:</span>             Tue Jan 21 23:46:20 2025</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a> <span class="ex">OS/Arch:</span>           darwin/amd64</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a> <span class="ex">Context:</span>           default</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You’ll also need the following packages installed.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="va">fastapi</span><span class="op">=</span>=0.115.7</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="va">mlflow</span><span class="op">=</span>=2.12.2</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="va">numpy</span><span class="op">=</span>=1.26.4</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="va">pandas</span><span class="op">=</span>=2.1.4</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="ex">python-multipart==0.0.20</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="ex">scikit-learn==1.5.1</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="va">streamlit</span><span class="op">=</span>=1.37.1</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="va">uvicorn</span><span class="op">=</span>=0.34.0</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="va">xgboost</span><span class="op">=</span>=2.1.3</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>While this example provides step-by-step instructions, having a basic understanding of FastAPI and Docker concepts will be helpful. Learn more at:</p>
<ul>
<li>FastAPI: <a href="https://fastapi.tiangolo.com/">fastapi.tiangolo.com</a></li>
<li>Docker: <a href="https://docs.docker.com/">docs.docker.com</a></li>
</ul>
</div>
</div>
</div>
</section>
<section id="access-to-the-versioned-models" class="level4">
<h4 class="anchored" data-anchor-id="access-to-the-versioned-models">2. Access to the Versioned Models</h4>
<p>You’ll also need to ensure you have completed the previous hands-on examples where the apple demand forecasting model was trained, logged and versioned to MLflow. If you haven’t, refer to <a href="06-modelops-experimenting.html#sec-model-experimentation-exercise" class="quarto-xref">Section&nbsp;<span>6.7</span></a> and <a href="07-modelops-versioning.html#sec-model-version-example" class="quarto-xref">Section&nbsp;<span>7.4</span></a>.</p>
</section>
<section id="dataset-for-predictions" class="level4">
<h4 class="anchored" data-anchor-id="dataset-for-predictions">3. Dataset for Predictions</h4>
<p>For these examples, we’ll use simulated test data to feed into our deployed model to make predictions. The following represents a single input that could be fed into our model to mimic a real-time prediction. You can use this <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/test_data.csv"><code>test_data.csv</code></a> file.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-create-test-data.ipynb" data-notebook-cellid="cell-1">
<div id="cell-1" class="cell" data-tags="[&quot;single-input&quot;]" data-execution_count="2">
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">average_temperature</th>
<th data-quarto-table-cell-role="th">rainfall</th>
<th data-quarto-table-cell-role="th">weekend</th>
<th data-quarto-table-cell-role="th">holiday</th>
<th data-quarto-table-cell-role="th">price_per_kg</th>
<th data-quarto-table-cell-role="th">promo</th>
<th data-quarto-table-cell-role="th">previous_days_demand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1000</td>
<td>28.566576</td>
<td>1.405188</td>
<td>1</td>
<td>0</td>
<td>1.542647</td>
<td>0</td>
<td>1640.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>And the following represents multiple inputs that could be fed into our model to mimic a batch prediction. You can use this <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/test_batch_data.csv"><code>test_batch_data.csv</code></a> file.</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-create-test-data.ipynb" data-notebook-cellid="cell-3">
<div id="cell-3" class="cell" data-tags="[&quot;batch-input&quot;]" data-execution_count="4">
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">average_temperature</th>
<th data-quarto-table-cell-role="th">rainfall</th>
<th data-quarto-table-cell-role="th">weekend</th>
<th data-quarto-table-cell-role="th">holiday</th>
<th data-quarto-table-cell-role="th">price_per_kg</th>
<th data-quarto-table-cell-role="th">promo</th>
<th data-quarto-table-cell-role="th">previous_days_demand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>30.584727</td>
<td>12.212080</td>
<td>0</td>
<td>0</td>
<td>1.333262</td>
<td>0</td>
<td>989.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>15.465069</td>
<td>7.372440</td>
<td>1</td>
<td>0</td>
<td>1.956357</td>
<td>0</td>
<td>989.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>10.786525</td>
<td>1.700924</td>
<td>1</td>
<td>0</td>
<td>2.198421</td>
<td>0</td>
<td>1110.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>23.648154</td>
<td>2.684491</td>
<td>0</td>
<td>0</td>
<td>1.861633</td>
<td>0</td>
<td>1256.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>13.861391</td>
<td>2.013523</td>
<td>0</td>
<td>0</td>
<td>2.781717</td>
<td>0</td>
<td>906.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>14.674082</td>
<td>2.281109</td>
<td>0</td>
<td>0</td>
<td>2.455626</td>
<td>0</td>
<td>965.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>20.558692</td>
<td>1.871252</td>
<td>0</td>
<td>0</td>
<td>2.276917</td>
<td>0</td>
<td>879.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>25.138812</td>
<td>4.342813</td>
<td>0</td>
<td>0</td>
<td>0.730149</td>
<td>0</td>
<td>855.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>22.308853</td>
<td>0.599279</td>
<td>1</td>
<td>0</td>
<td>2.880621</td>
<td>0</td>
<td>960.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>26.102977</td>
<td>1.971698</td>
<td>1</td>
<td>0</td>
<td>1.058829</td>
<td>1</td>
<td>1135.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Feel free to generate your own test data using this <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/model-create-test-data.ipynb">notebook</a>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="retrieving-the-trained-model-from-mlflow" class="level3">
<h3 class="anchored" data-anchor-id="retrieving-the-trained-model-from-mlflow">Retrieving the Trained Model from MLflow</h3>
<p>Before deploying the model, we need to retrieve it from the MLflow Model Registry. In the previous chapter, we saw how we can version our trained apple demand forecasting model to MLflow.</p>
<div id="fig-mlflow-current-registered-models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlflow-current-registered-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/model-retrieval-registry.png" class="img-fluid figure-img"></p>
<figcaption>Current Models Registered</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlflow-current-registered-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: I currently have 4 models versions. Depending on what you did in the previous chapter you may have more or less models versioned.
</figcaption>
</figure>
</div>
<p>Now, that these model’s are registered, we can load the latest version of this model programmatically and make predictions with it. The following code<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> shows an example of loading the latest model and making a prediction using simulated test data:</p>
<div class="quarto-embed-nb-cell" data-notebook="/home/runner/work/uc-bana-7075/uc-bana-7075/ModelOps/model-retrieval.ipynb" data-notebook-title="Example Model Retrieval for Prediction" data-notebook-cellid="cell-1">
<div id="cell-1" class="cell" data-tags="[&quot;model-retrieval&quot;]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Requirements</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> apple_data <span class="im">import</span> generate_apple_sales_data_with_promo_adjustment</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set experiment name</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"Forecasting Apple Demand"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model name and version</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">"apple_demand"</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>MODEL_VERSION <span class="op">=</span> <span class="st">"latest"</span>  <span class="co"># Can specify 1, 2, etc. or "latest"</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model from MLflow Model Registry</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>model_uri <span class="op">=</span> <span class="ss">f"models:/</span><span class="sc">{</span>MODEL_NAME<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>MODEL_VERSION<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> mlflow.pyfunc.load_model(model_uri)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create test data for model input</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> generate_apple_sales_data_with_promo_adjustment(base_demand<span class="op">=</span><span class="dv">1_000</span>, n_rows<span class="op">=</span><span class="dv">1_001</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>test_data <span class="op">=</span> test_data.drop(columns<span class="op">=</span>[<span class="st">'demand'</span>, <span class="st">'date'</span>]).iloc[[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate predictions</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>model.predict(test_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>array([970.05157731])</code></pre>
</div>
</div>
</div>
<p>Notice that you can load a specific model version (i.e.&nbsp;version 1, version 3) or simply retrieve the <strong>latest version</strong>. This is very convenient as it allows us to always deploying the latest approved model or a specific version of the model. Moreover, programmatically retrieving the logged/versioned model helps ensure consistency between models used during experimentation and deployment environments.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>In addition to loading a specific model version or the “latest” version, MLflow allows you to retrieve models based on <strong>tags</strong> (e.g., <code>deployment_status=production</code>) or <strong>aliases</strong> (e.g., <code>champion</code>). This makes it easier to dynamically load models without hardcoding version numbers, ensuring that deployments always pull the most relevant and validated model. Tags help categorize models based on their lifecycle stage, while aliases provide a flexible way to reference key models like the current production or staging version. This approach enhances automation, reproducibility, and scalability in model deployment workflows.</p>
</div>
</div>
</div>
<p>Now that we know how to retrieve a registered model, the next step is to <strong>deploy it as a REST API using FastAPI</strong>.</p>
</section>
<section id="deploy-a-fastapi-based-rest-api-for-batch-predictions" class="level3">
<h3 class="anchored" data-anchor-id="deploy-a-fastapi-based-rest-api-for-batch-predictions">Deploy a FastAPI-based REST API for batch predictions</h3>
<p>As machine learning models move from experimentation to production, they must be accessible to the systems and users that rely on them. One of the most common ways to expose a machine learning model is through a <strong>REST API</strong>, which allows external applications to send input data and receive predictions. In this example, we use FastAPI to deploy our model but realize there are other options as well (i.e.&nbsp;Flask).</p>
<p>For a grocery chain retailer, having a deployed API means that various business systems such as inventory management, supply chain logistics, and automated ordering systems can dynamically request demand forecasts and adjust operations accordingly. Instead of manually running a demand forecasting model every day, store managers, warehouse systems, or enterprise resource planning (ERP) tools can <strong>programmatically request demand predictions</strong> based on real-time market conditions, weather data, and sales history.</p>
<p>For example:</p>
<ul>
<li>A <strong>warehouse system</strong> could call this API every morning to predict how many apples each store is expected to sell in the coming days. This allows the retailer to optimize deliveries and reduce waste.</li>
<li>A <strong>pricing system</strong> could query the API and adjust apple prices dynamically based on predicted demand, running promotions when demand is low or adjusting supply when demand spikes.</li>
<li>A <strong>dashboard</strong> used by regional managers might fetch daily demand forecasts via this API to monitor trends and make data-driven stocking decisions.</li>
</ul>
<p>Rather than requiring manual calculations or data science expertise at each store, this API could allow automated, real-time access to apple demand forecasts across the retailer’s entire network.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding APIs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>As a <strong>data scientist</strong>, you may not be responsible for deploying models at scale, but understanding how APIs work helps you bridge the gap between model development and practical business impact. This section walks you through building and testing a simple API locally, showing how predictions can be served dynamically.</p>
<p>Even if you don’t plan to build production APIs, reading through this section will help you appreciate the complexities involved in model deployment. At a minimum, understanding API-based deployment will allow you to better collaborate with software engineers and DevOps teams, ensuring your models are integrated seamlessly into the company’s workflows.</p>
</div>
</div>
</div>
<p>Our first task is to create a new Python script called <code>fastapi_app.py</code><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> and add the following code.</p>
<p>Note the key components in this code includes:</p>
<ul>
<li>FastAPI initialization (<code>app = FastAPI()</code>)</li>
<li>Loading the model from MLflow (<code>mlflow.pyfunc.load_model()</code>)</li>
<li>Define the expected input schema (<code>InputData()</code>)</li>
<li>Real-time &amp; batch prediction capabilities (<code>predict_single()</code>, <code>predict_batch()</code>)</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastapi <span class="im">import</span> FastAPI, HTTPException, UploadFile, File</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlflow.pyfunc</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> io</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize FastAPI app</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>app <span class="op">=</span> FastAPI()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Set experiment name</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>mlflow.set_experiment(<span class="st">"Forecasting Apple Demand"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the trained model from MLflow</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>MODEL_URI <span class="op">=</span> <span class="st">"models:/apple_demand@champion"</span>  <span class="co"># Replace with your model name and alias</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> mlflow.pyfunc.load_model(MODEL_URI)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the expected input schema for a single prediction</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> InputData(BaseModel):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    average_temperature: <span class="bu">float</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    rainfall: <span class="bu">float</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    weekend: <span class="bu">int</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    holiday: <span class="bu">int</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    price_per_kg: <span class="bu">float</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    promo: <span class="bu">int</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    previous_days_demand: <span class="bu">float</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="at">@app.post</span>(<span class="st">"/predict"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict_single(input_data: List[InputData]):</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Endpoint for real-time predictions with a single input."""</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert input to DataFrame</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame([data.<span class="bu">dict</span>() <span class="cf">for</span> data <span class="kw">in</span> input_data])</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make predictions</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model.predict(df)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"predictions"</span>: predictions.tolist()}</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> HTTPException(status_code<span class="op">=</span><span class="dv">500</span>, detail<span class="op">=</span><span class="bu">str</span>(e))</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="at">@app.post</span>(<span class="st">"/predict_batch"</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="cf">async</span> <span class="kw">def</span> predict_batch(<span class="bu">file</span>: UploadFile <span class="op">=</span> File(...)):</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Endpoint for batch predictions using a CSV file."""</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Read the uploaded CSV file</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        contents <span class="op">=</span> <span class="cf">await</span> <span class="bu">file</span>.read()</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        df <span class="op">=</span> pd.read_csv(io.StringIO(contents.decode(<span class="st">"utf-8"</span>)))</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validate required columns</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        required_features <span class="op">=</span> [<span class="st">"average_temperature"</span>, <span class="st">"rainfall"</span>, <span class="st">"weekend"</span>, <span class="st">"holiday"</span>, <span class="st">"price_per_kg"</span>, <span class="st">"promo"</span>, <span class="st">"previous_days_demand"</span>]</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">all</span>(feature <span class="kw">in</span> df.columns <span class="cf">for</span> feature <span class="kw">in</span> required_features):</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>            missing_cols <span class="op">=</span> <span class="bu">set</span>(required_features) <span class="op">-</span> <span class="bu">set</span>(df.columns)</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> HTTPException(status_code<span class="op">=</span><span class="dv">400</span>, detail<span class="op">=</span><span class="ss">f"Missing columns: </span><span class="sc">{</span>missing_cols<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make batch predictions</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model.predict(df)</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"predictions"</span>: predictions.tolist()}</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> HTTPException(status_code<span class="op">=</span><span class="dv">500</span>, detail<span class="op">=</span><span class="bu">str</span>(e))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once you’ve added this code to the <code>fastapi_app.py</code> file, we can test it locally. First, launch the API using the following:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uvicorn</span> fastapi_app:app <span class="at">--host</span> 0.0.0.0 <span class="at">--port</span> 5000 <span class="at">--reload</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is Uvicorn and Why Are We Using It?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Uvicorn is an ASGI (Asynchronous Server Gateway Interface) server that runs FastAPI applications.</li>
<li>Unlike traditional WSGI servers (e.g., Flask’s built-in server), Uvicorn is:
<ul>
<li><strong>Asynchronous</strong>: Handles multiple requests efficiently.</li>
<li><strong>Lightweight</strong>: Optimized for performance.</li>
<li><strong>Production-Ready</strong>: Used for scalable API deployments.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<p>When you run the command, the server will start:</p>
<pre class="vbnet"><code>INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)</code></pre>
<p>Once the server is running, you can test it a couple different ways. First, you could test using <code>curl</code> like the following. This would imitate a real-time request to the model for a prediction based on the inputs.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-X</span> POST <span class="st">"http://127.0.0.1:5000/predict"</span> <span class="dt">\</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">-H</span> <span class="st">"Content-Type: application/json"</span> <span class="dt">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">-d</span> <span class="st">'[</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="st">           {</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="st">             "average_temperature": 28.5,</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="st">             "rainfall": 1.4,</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="st">             "weekend": 0,</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="st">             "holiday": 0,</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="st">             "price_per_kg": 1.54,</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="st">             "promo": 0,</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="st">             "previous_days_demand": 1313.0</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="st">           }</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="st">         ]'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Expected response:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"predictions"</span><span class="fu">:</span> <span class="ot">[</span><span class="fl">970.0515773108884</span><span class="ot">]</span><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Batch predictions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Likewise, you could use <code>curl</code> to make a batch prediction and reference a .csv file that contains multiple inputs requiring predictions:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-X</span> POST <span class="st">"http://127.0.0.1:5000/predict_batch"</span> <span class="dt">\</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">-H</span> <span class="st">"Content-Type: multipart/form-data"</span> <span class="dt">\</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">-F</span> <span class="st">"file=@test_batch_data.csv"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Expected response:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"predictions"</span><span class="fu">:</span> <span class="ot">[</span><span class="fl">970.8774897603781</span><span class="ot">,</span><span class="fl">951.4406298526695</span><span class="ot">,</span><span class="fl">918.2921839400934</span><span class="ot">,</span><span class="fl">938.5186636642713</span><span class="ot">,</span><span class="fl">922.5670241420959</span><span class="ot">,</span><span class="fl">1196.1975513217837</span><span class="ot">,</span><span class="fl">1198.097164109545</span><span class="ot">,</span><span class="fl">993.6193679149401</span><span class="ot">,</span><span class="fl">906.596099831946</span><span class="ot">,</span><span class="fl">1221.0938533472397</span><span class="ot">]</span><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>An alternative approach is to use FastAPI’s interactive user interface. If you go to http://127.0.0.1:5000/docs (assuming your server is running), you’ll come to the UI which shows details on the <code>/predict</code> and <code>/predict_batch</code> endpoints, along with other information.</p>
<div id="fig-fastapi-ui" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fastapi-ui-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/fastapi-ui.png" class="img-fluid figure-img"></p>
<figcaption>FastAPI UI</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fastapi-ui-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: The FastAPI UI
</figcaption>
</figure>
</div>
<p>This interactive UI allows you to test the endpoints directly from the browser! For example, if I go to the <code>/predict_batch</code> endpoint and hit <strong>Try it out</strong>, I can choose this <a href="TBD"><code>test_batch_data.csv</code></a> that contains 10 inputs for our model and then hit <strong>Execute</strong> and I will get the 10 predictions for each observation.</p>
<div id="fig-fastapi-ui-batch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fastapi-ui-batch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/fastapi-ui-batch.png" class="img-fluid figure-img"></p>
<figcaption>FastAPI UI</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fastapi-ui-batch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: The interactive UI allows you to test out the endpoints. For example, this shows an example of using the <code>/predict_batch</code> endpoint on 10 example inputs provided in a <code>test_batch_data.csv</code> file.
</figcaption>
</figure>
</div>
</section>
<section id="adding-a-streamlit-ui-for-interactive-predictions" class="level3">
<h3 class="anchored" data-anchor-id="adding-a-streamlit-ui-for-interactive-predictions">Adding a Streamlit UI for Interactive Predictions</h3>
<p>When developing a <strong><em>new</em></strong> ML application, one of the biggest challenges is ensuring that stakeholders, business leaders, and end users can interact with and understand the model’s outputs. Often, at the early stages of ML development, there is no existing user interface, and asking non-technical users to interact with an API or raw model outputs can be unrealistic.</p>
<p>This is where <a href="https://docs.streamlit.io/"><strong>Streamlit</strong></a> becomes an invaluable tool.</p>
<p>Streamlit allows you to quickly prototype an interactive UI for your ML model without needing extensive frontend development skills. With just a few lines of Python, you can create a fully functional web-based interface that enables users to explore how the model works, test different inputs, and visualize results.</p>
<section id="how-streamlit-enhances-ml-prototyping-and-deployment" class="level4">
<h4 class="anchored" data-anchor-id="how-streamlit-enhances-ml-prototyping-and-deployment">How Streamlit Enhances ML Prototyping and Deployment</h4>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Business Users &amp; Analysts: Easy, Accessible Model Predictions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Business stakeholders such as product managers, marketing teams, or financial analysts often need to understand how a machine learning model behaves before making strategic decisions. However, they typically do not have the technical expertise to run Python scripts or query an API.</p>
<p>With Streamlit, you can provide a simple, interactive dashboard where business users can:</p>
<ul>
<li><strong>Upload CSVs</strong> and get batch predictions without writing code.</li>
<li><strong>Manually input values</strong> and see how predictions change.</li>
<li><strong>Visualize insights</strong> through tables and charts that explain model behavior.</li>
</ul>
<p>By giving non-technical users an easy way to interact with your model, you foster better collaboration between data science teams and business stakeholders, ensuring alignment on how the model can be leveraged.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data Scientists: Testing Model Behavior Before Production
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>For data scientists, understanding how a model behaves under different conditions is critical before moving it to production. Debugging models using raw code or CLI-based scripts can be cumbersome, and API-based testing is not always intuitive.</p>
<p>A <strong>Streamlit interface</strong> allows data scientists to:</p>
<ul>
<li><strong>Visually inspect model outputs</strong> for various test cases.</li>
<li><strong>Debug predictions</strong> by modifying input parameters dynamically.</li>
<li><strong>Compare different model versions</strong> before selecting one for deployment.</li>
</ul>
<p>By using Streamlit, data scientists can identify edge cases, uncover biases, and validate performance more efficiently than through static reports or raw log outputs.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-28-contents" aria-controls="callout-28" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Internal Tools: Rapid Experimentation Before Full Integration
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-28" class="callout-28-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>When deploying an ML model within an organization, integrating it into an existing web or mobile application can take time. Engineers and developers often need to align APIs, databases, and UI components before the model becomes accessible to end users.</p>
<p>Instead of waiting for full integration, a Streamlit-based internal tool can:</p>
<ul>
<li>Act as a <strong>temporary UI</strong> for stakeholders to test the model.</li>
<li>Provide <strong>early feedback</strong> on how the model performs in real-world use cases.</li>
<li><strong>Help engineers understand API requirements</strong> before building a production-ready frontend.</li>
</ul>
<p>By using Streamlit as a stepping stone, teams can validate the usefulness of an ML model before committing significant resources to integrating it into an enterprise application.</p>
</div>
</div>
</div>
<p>Streamlit bridges the gap between data science, engineering, and business stakeholders by making ML models interactive and accessible. Whether you are prototyping a new application, debugging a model, or providing a temporary interface for internal use, Streamlit allows you to rapidly deploy a functional UI in minutes.</p>
<p>In the next section, we will walk through how to integrate Streamlit with FastAPI to create an interactive frontend for ML predictions.</p>
</section>
<section id="streamlit-app-for-model-predictions" class="level4">
<h4 class="anchored" data-anchor-id="streamlit-app-for-model-predictions">Streamlit App for Model Predictions</h4>
<p>First, we’ll create a new Python script called <code>streamlit_app.py</code><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. This script should be in the sameproject directory as the <code>fastapi_app.py</code> script you created in the last section.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">📂</span> project_directory/</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="ex">│──</span> 📄 fastapi_app.py        <span class="co"># FastAPI backend for ML model inference</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="ex">│──</span> 📄 streamlit_app.py      <span class="co"># Streamlit frontend for UI-based interaction</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>streamlit_app.py</code> script will contain the following code:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> streamlit <span class="im">as</span> st</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># FastAPI backend URL. Used to send requests from Streamlit</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># to FastAPI for model predictions.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>FASTAPI_URL <span class="op">=</span> <span class="st">"http://127.0.0.1:5000"</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>st.title(<span class="st">"Apple Demand Forecasting - ML Model UI"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">#################################</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># === Single Input Prediction ===</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co">#################################</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Users can manually enter feature values for each feature</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>st.subheader(<span class="st">"Single Input Prediction"</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>average_temperature <span class="op">=</span> st.number_input(<span class="st">"Average Temperature (°C)"</span>, value<span class="op">=</span><span class="fl">28.5</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>rainfall <span class="op">=</span> st.number_input(<span class="st">"Rainfall (mm)"</span>, value<span class="op">=</span><span class="fl">1.4</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>weekend <span class="op">=</span> st.selectbox(<span class="st">"Is it a weekend?"</span>, [<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>holiday <span class="op">=</span> st.selectbox(<span class="st">"Is it a holiday?"</span>, [<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>price_per_kg <span class="op">=</span> st.number_input(<span class="st">"Price per Kg ($)"</span>, value<span class="op">=</span><span class="fl">1.54</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>promo <span class="op">=</span> st.selectbox(<span class="st">"Promotion Available?"</span>, [<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>previous_days_demand <span class="op">=</span> st.number_input(<span class="st">"Previous Days Demand"</span>, value<span class="op">=</span><span class="dv">1313</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># This button triggers a request to FastAPI's /predict endpoint.</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> st.button(<span class="st">"Predict Demand"</span>):</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    input_data <span class="op">=</span> [{</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="st">"average_temperature"</span>: average_temperature,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">"rainfall"</span>: rainfall,</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">"weekend"</span>: weekend,</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">"holiday"</span>: holiday,</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"price_per_kg"</span>: price_per_kg,</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">"promo"</span>: promo,</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">"previous_days_demand"</span>: previous_days_demand</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    }]</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> requests.post(<span class="ss">f"</span><span class="sc">{</span>FASTAPI_URL<span class="sc">}</span><span class="ss">/predict"</span>, json<span class="op">=</span>input_data)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The response is displayed on the Streamlit UI.</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> response.status_code <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> response.json()[<span class="st">"predictions"</span>][<span class="dv">0</span>]</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        st.success(<span class="ss">f"Predicted Demand: </span><span class="sc">{</span>prediction<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        st.error(<span class="st">"Error fetching prediction. Check FastAPI logs."</span>)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="co"># === Batch Prediction with File Upload ===</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Users can upload a CSV file containing multiple rows of input data.</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>st.subheader(<span class="st">"Batch Prediction via CSV Upload"</span>)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>uploaded_file <span class="op">=</span> st.file_uploader(<span class="st">"Upload CSV file"</span>, <span class="bu">type</span><span class="op">=</span>[<span class="st">"csv"</span>])</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> uploaded_file:</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(uploaded_file)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    st.write(<span class="st">"Uploaded Data Preview:"</span>, df.head())</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The file is sent to the FastAPI /predict_batch endpoint.</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> st.button(<span class="st">"Get Batch Predictions"</span>):</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        files <span class="op">=</span> {<span class="st">"file"</span>: uploaded_file.getvalue()}</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> requests.post(<span class="ss">f"</span><span class="sc">{</span>FASTAPI_URL<span class="sc">}</span><span class="ss">/predict_batch"</span>, files<span class="op">=</span>files)</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Predictions are added to the dataset and displayed on the UI.</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> response.status_code <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> response.json()[<span class="st">"predictions"</span>]</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>            df[<span class="st">"Predicted Demand"</span>] <span class="op">=</span> predictions</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>            st.subheader(<span class="st">"Predictions:"</span>)</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>            st.write(df)</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>            st.error(<span class="st">"Error processing batch prediction."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-fastapi-streamlit-together" class="level4">
<h4 class="anchored" data-anchor-id="sec-fastapi-streamlit-together">Running FastAPI and Streamlit Together</h4>
<p>Since FastAPI serves as the backend for model inference and Streamlit acts as the frontend UI, both applications need to run simultaneously and communicate with each other. There are several ways to achieve this depending on whether you’re in a development or production environment.</p>
<ol type="1">
<li><strong>Running them separately in different terminal windows</strong> (best for local development).</li>
<li><strong>Running both from a single Python script using multiprocessing</strong> (useful for quick testing).</li>
<li><strong>Containerizing them with Docker Compose</strong> (best for deployment).</li>
</ol>
<p>We’ll demo the first two approaches and then the next section will discuss the third.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-29-contents" aria-controls="callout-29" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Option 1: Running Separately (Recommended for Development)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-29" class="callout-29-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The simplest way to run FastAPI and Streamlit is to start them in separate terminal windows.</p>
<p><strong>Step 1: Start the FastAPI Backend</strong></p>
<p>Navigate to the directory where your <code>fastapi_app.py</code> file is located and run:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uvicorn</span> fastapi_app:app <span class="at">--host</span> 0.0.0.0 <span class="at">--port</span> 5000 <span class="at">--reload</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Step 2: Start the Streamlit Frontend</strong></p>
<p>Open another terminal window, navigate to the directory where <code>streamlit_app.py</code> is located, and run:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">streamlit</span> run streamlit_app.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This starts the Streamlit UI on <code>http://127.0.0.1:8501</code> and will send API requests to FastAPI to retrieve predictions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/streamlit-ui.png" class="img-fluid figure-img"></p>
<figcaption>Streamlit UI</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-30-contents" aria-controls="callout-30" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Option 2: Running FastAPI and Streamlit from a Single Script
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-30" class="callout-30-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>If you want to start both applications from one script, you can use Python’s <code>multiprocessing</code> module to run them in parallel. You’ll need to create a new script titled <code>fastapi_streamlit.py</code><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> that is in the same directory as your other app scripts.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">📂</span> project_directory/</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="ex">│──</span> 📄 fastapi_app.py        <span class="co"># FastAPI backend for ML model inference</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="ex">│──</span> 📄 streamlit_app.py      <span class="co"># Streamlit frontend for UI-based interaction</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="ex">│──</span> 📄 fastapi_streamlit.py  <span class="co"># Run both FastAPI and Streamlit concurrently</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>fastapi_streamlit.py</code> should have the following code.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> multiprocessing</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> signal</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> uvicorn</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_fastapi():</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Start the FastAPI app"""</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    uvicorn.run(<span class="st">"fastapi_app:app"</span>, host<span class="op">=</span><span class="st">"0.0.0.0"</span>, port<span class="op">=</span><span class="dv">5000</span>, <span class="bu">reload</span><span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_streamlit():</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Start the Streamlit app"""</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    os.system(<span class="st">"streamlit run streamlit_app.py"</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shutdown_handler(signum, frame):</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gracefully terminate child processes"""</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Shutting down FastAPI and Streamlit..."</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    p1.terminate()</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    p2.terminate()</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    p1.join()</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    p2.join()</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    sys.exit(<span class="dv">0</span>)  <span class="co"># Exit the script cleanly</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run FastAPI and Streamlit in parallel processes</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    multiprocessing.set_start_method(<span class="st">"spawn"</span>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    p1 <span class="op">=</span> multiprocessing.Process(target<span class="op">=</span>run_fastapi, daemon<span class="op">=</span><span class="va">True</span>)  <span class="co"># Set daemon mode</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    p2 <span class="op">=</span> multiprocessing.Process(target<span class="op">=</span>run_streamlit, daemon<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    p1.start()</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    p2.start()</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capture termination signals</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    signal.signal(signal.SIGINT, shutdown_handler)  <span class="co"># Handle Ctrl+C</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    signal.signal(</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        signal.SIGTERM, shutdown_handler</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Handle termination (e.g., Docker stop)</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        p1.join()</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        p2.join()</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyboardInterrupt</span>:</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>        shutdown_handler(<span class="va">None</span>, <span class="va">None</span>)  <span class="co"># Gracefully shut down on manual interrupt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In your terminal window, navigate to the directory where <code>fastapi_streamlit.py</code> is located, and run:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> fastapi_streamlit.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This will start both applications automatically so that as you provide inputs to the Streamlit frontend UI, it will automatically send the inputs to the FastAPI backend.</p>
</div>
</div>
</div>
</section>
</section>
<section id="containerizing-the-application" class="level3">
<h3 class="anchored" data-anchor-id="containerizing-the-application">Containerizing the Application</h3>
<p>Containerization is a critical practice in modern software deployment that ensures <strong>portability, reproducibility, and scalability</strong>. By packaging an application with all its dependencies into a Docker container, we eliminate issues related to environment inconsistencies, making it easy to deploy on any system, cloud platform, or cluster.</p>
<p>The primary benefits of containerizing a solution includes:</p>
<ul>
<li><strong>Portability</strong> – Containers run consistently across different environments (local, cloud, on-premises).</li>
<li><strong>Reproducibility</strong> – Ensures that dependencies, libraries, and configurations remain consistent.</li>
<li><strong>Scalability</strong> – Containers can be efficiently orchestrated using tools like <strong>Kubernetes</strong>.</li>
<li><strong>Isolation</strong> – Each container runs independently, preventing dependency conflicts.</li>
<li><strong>Efficient Resource Utilization</strong> – Containers are lightweight compared to traditional virtual machines.</li>
</ul>
<p>For this project, we will containerize both the FastAPI backend and the Streamlit frontend to ensure a smooth and efficient deployment.</p>
<section id="project-directory-structure" class="level4">
<h4 class="anchored" data-anchor-id="project-directory-structure">Project Directory Structure</h4>
<p>To fully support FastAPI, Streamlit, and MLflow, our project will be structured as follows:</p>
<pre><code>📂 model-deploy/
│── 📄 fastapi_app.py         # FastAPI backend
│── 📄 streamlit_app.py       # Streamlit frontend
│── 📄 fastapi_streamlit.py   # Script to run both apps together
│── 📄 requirements.txt       # Python dependencies
│── 📄 Dockerfile             # Defines how to build the container
│── 📄 .dockerignore          # Excludes unnecessary files from the Docker build
│── 📄 docker-compose.yml     # Adds MLflow environment variable and volume containing local path</code></pre>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>You can find this directory and all the files discussed here: <a href="https://github.com/bradleyboehmke/uc-bana-7075/tree/main/ModelOps/model-deploy">https://github.com/bradleyboehmke/uc-bana-7075/tree/main/ModelOps/model-deploy</a>.</p>
</div>
</div>
</div>
</section>
<section id="dockerfile" class="level4">
<h4 class="anchored" data-anchor-id="dockerfile">Dockerfile</h4>
<p>The <code>Dockerfile</code> defines how our container is built, specifying the <strong>Python version</strong>, copying necessary files, installing dependencies, and setting the startup command.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode txt code-with-copy"><code class="sourceCode default"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a># Use official Python image</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>FROM python:3.12</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a># Set the working directory in the container</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>WORKDIR /app</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a># Copy all application files to the container</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>COPY . /app</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a># Install dependencies</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>RUN pip install --no-cache-dir -r requirements.txt</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a># Expose necessary ports for FastAPI (5000) and Streamlit (8501)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>EXPOSE 5000 8501</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a># Start both FastAPI and Streamlit using the Python script</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>CMD ["python", "fastapi_streamlit.py"]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="dockerignore-file" class="level4">
<h4 class="anchored" data-anchor-id="dockerignore-file">.dockerignore File</h4>
<p>This prevents unnecessary files (like <code>__pycache__</code> and <code>.git</code>) from being copied into the container, reducing build size.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode txt code-with-copy"><code class="sourceCode default"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>__pycache__/</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>*.pyc</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>*.pyo</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>*.log</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>.git/</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="using-docker-compose-to-manage-containers" class="level4">
<h4 class="anchored" data-anchor-id="using-docker-compose-to-manage-containers">Using Docker Compose to Manage Containers</h4>
<p>Instead of running the container manually, we use Docker Compose to define and manage the service. The <code>docker-compose.yml</code> file makes it easier to configure the application and allows us to mount local directories for data persistence.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">services</span><span class="kw">:</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">model-deploy</span><span class="kw">:</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">image</span><span class="kw">:</span><span class="at"> model-deploy-app</span><span class="co">  # Set a custom, shorter image name</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">build</span><span class="kw">:</span><span class="at"> .</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="st">"5000:5000"</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="st">"8501:8501"</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">environment</span><span class="kw">:</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> MLFLOW_TRACKING_URI=file:///app/mlflow_registry/mlruns</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> /Users/b294776/Desktop/workspace/training/UC/uc-bana-7075/ModelOps/mlruns:/app/mlflow_registry/mlruns</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Key Features of <code>docker-compose.yml</code> includes:</p>
<ul>
<li><strong>Volume Mounting (volumes)</strong>: We mount the local MLflow experiment directory to the container so that the FastAPI app inside the container can access models stored on the host machine.</li>
<li><strong>Environment Variables (environment)</strong>: We set <code>MLFLOW_TRACKING_URI</code>, allowing the containerized application to locate MLflow experiments.</li>
<li><strong>Port Mapping (ports)</strong>: The container exposes FastAPI (5000) and Streamlit (8501) so they can be accessed from the host machine.</li>
</ul>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Since we are hosting MLflow locally, we need to explicitly reference our local MLflow experiment path in <code>docker-compose.yml</code>. However, in a real-world scenario, organizations typically host MLflow models in a central storage location (e.g., AWS S3, Azure Blob Storage, or an MLflow server). In such a setup, instead of using a local volume, we would pass the cloud storage path to <code>MLFLOW_TRACKING_URI</code>, like:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">environment</span><span class="kw">:</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> MLFLOW_TRACKING_URI=s3://my-mlflow-bucket/mlruns</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This makes the system cloud-compatible and scalable, allowing multiple services to retrieve models from a shared registry.</p>
</div>
</div>
</div>
</section>
<section id="building-and-running-the-docker-container" class="level4">
<h4 class="anchored" data-anchor-id="building-and-running-the-docker-container">Building and Running the Docker Container</h4>
<p>Now that we have everything set up, we’re ready to build and run our Docker image. Navigate to the <strong>model-deploy</strong> directory:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /path/to/model-deploy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and run the following to build the Docker image:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> build <span class="at">--no-cache</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that your Docker image is built you can start the container with:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> up</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now:</p>
<ul>
<li>FastAPI should be available at <strong><code>http://127.0.0.1:5000/docs</code></strong></li>
<li>Streamlit UI should be accessible at <strong><code>http://127.0.0.1:8501</code></strong></li>
</ul>
<p>To stop the container, run:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker-compose</span> down</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="scaling-with-kubernetes" class="level4">
<h4 class="anchored" data-anchor-id="scaling-with-kubernetes">Scaling with Kubernetes</h4>
<p>Once the application is containerized, the next step toward a production-ready deployment is <em>scalability</em>. In real-world environments, machine learning models often need to handle varying workloads, ranging from a few requests per second in development to thousands per second in production. Kubernetes, an industry-standard container orchestration platform, provides a robust and flexible way to manage and scale containerized applications.</p>
<p>One of the key benefits of Kubernetes is its ability to manage multiple replicas of services such as FastAPI to handle higher request volumes. In a production setting, rather than running a single instance of FastAPI, Kubernetes can deploy multiple replicas of the FastAPI service, ensuring that requests are distributed efficiently across different pods. This improves availability and ensures the system can handle spikes in traffic without degrading performance.</p>
<p>In addition to horizontal scaling, Kubernetes also provides load balancing through its Ingress Controller or built-in LoadBalancer service. When multiple replicas of the FastAPI service are running, a Kubernetes Ingress Controller can act as a gateway that efficiently distributes incoming requests to different FastAPI pods. This ensures no single instance becomes overwhelmed, improving both response time and system reliability.</p>
<p>Kubernetes also allows seamless model versioning by dynamically pulling new model artifacts from a centralized model registry. This allows FastAPI pods running in Kubernetes to dynamically load the latest version of a model without requiring a full redeployment of the application.</p>
<p>Another significant advantage of using Kubernetes is its auto-scaling capabilities. Kubernetes can automatically scale up the number of running FastAPI instances when CPU or memory usage surpasses a certain threshold. Conversely, during periods of low activity, Kubernetes can scale down instances to reduce infrastructure costs, making the system both cost-effective and efficient.</p>
<p>Finally, in deployments that require high availability and fault tolerance, Kubernetes provides built-in self-healing mechanisms. If a FastAPI or Streamlit pod crashes unexpectedly, Kubernetes will automatically restart the service to maintain uptime. This feature is particularly valuable in production environments where unexpected failures can impact business operations.</p>
<p>In summary, Kubernetes offers a powerful way to manage and scale ML model deployments, ensuring that applications are highly available, performant, and cost-effective. By incorporating Kubernetes into this workflow, organizations can transition from local, manually managed deployments to scalable, production-grade infrastructure that dynamically adapts to changing workloads.</p>
</section>
</section>
<section id="final-thoughts" class="level3">
<h3 class="anchored" data-anchor-id="final-thoughts">Final Thoughts</h3>
<p>By containerizing the FastAPI backend, Streamlit frontend, and MLflow model registry, we have created a fully functional machine learning deployment pipeline that is portable, scalable, and reproducible. The use of Docker ensures that both services can run in any environment, eliminating compatibility issues and making it easier to share and deploy models.</p>
<p>Throughout this example, we have demonstrated how FastAPI serves as an API layer for model inference, while Streamlit provides a user-friendly interface for business users and data scientists to interact with the model. Additionally, MLflow serves as a lightweight experiment tracking and model registry solution, enabling seamless model management.</p>
<p>For development and testing, running FastAPI and Streamlit together using Docker Compose provides an efficient way to simulate a production-like setup on a local machine. This allows for quick iteration and debugging while maintaining separation between services. However, as machine learning applications grow and need to support high traffic, Kubernetes becomes the next logical step. It allows organizations to dynamically scale services, manage load balancing, and ensure high availability with minimal manual intervention.</p>
<p>Overall, this approach provides a realistic, scalable solution for machine learning model deployment, bringing together some of the best tools in the ModelOps ecosystem. By leveraging containerization, API-driven model serving, and interactive UI design, this setup can be extended to enterprise-level deployments with minimal modifications. Whether the goal is to deploy models in a local setting for internal use or scale up to handle thousands of requests per second in production, the tools and techniques demonstrated here provide a strong foundation for operationalizing machine learning models effectively.</p>
</section>
</section>
<section id="strategies-for-updating-models-in-production" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="strategies-for-updating-models-in-production"><span class="header-section-number">8.5</span> Strategies for <strong>Updating</strong> Models in Production</h2>
<p>Thus far, we’ve discussed the process of deploying a machine learning model into production. However, deployment is not a one-and-done task. Once a model is in production, you will often need to make updates and redeploy new versions of the model. These updates might be necessary due to retraining on fresh data, improvements in model architecture, changes in business requirements, or performance degradation caused by data drift.</p>
<p>Effectively managing these updates while ensuring a smooth transition is critical to maintaining reliability and minimizing risk. There are several widely used strategies to introduce new models into production while balancing performance, user experience, and operational stability:</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-31-contents" aria-controls="callout-31" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Canary Deployment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-31" class="callout-31-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Description</strong>: This approach involves releasing the new model to a small subset of users or traffic while the majority continues using the current production model. The performance of the new model is closely monitored, and based on the results, the deployment can either be scaled up or rolled back.</p>
<p><strong>When to Use</strong>: Canary deployments are ideal for environments where testing in production with minimal risk is a priority. For example, an e-commerce platform could test a new recommendation model with 5% of users to verify its effectiveness without disrupting the overall user experience.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Allows real-world testing with minimal impact.</li>
<li>Provides the flexibility to revert quickly if issues arise.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Requires robust monitoring and the ability to manage traffic routing.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-32-contents" aria-controls="callout-32" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Blue-Green Deployment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-32" class="callout-32-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Description</strong>: In a blue-green deployment, both the old (blue) and new (green) models are run in parallel. The system routes traffic to the new model only after it has been fully validated. This strategy ensures zero downtime, as the old model remains available until the new one is deemed stable.</p>
<p><strong>When to Use</strong>: This strategy is well-suited for high-stakes applications where downtime is unacceptable, such as financial fraud detection systems or real-time healthcare monitoring.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Eliminates downtime during updates.</li>
<li>Simplifies rollback processes since the old model is still live.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Resource-intensive, as it requires maintaining two fully operational systems simultaneously.</li>
<li>Can introduce complexity in managing environments.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-33-contents" aria-controls="callout-33" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Shadow Deployment
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-33" class="callout-33-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Description</strong>: In this strategy, the new model operates alongside the current production model but does not impact users. Instead, the new model receives a copy of the production data for inference. This approach is used to evaluate the new model’s performance against the current model in a real-world setting without influencing the end-user experience.</p>
<p><strong>When to Use</strong>: Shadow deployments are particularly useful for testing how a new model would perform under production conditions without any risk to users. For example, a logistics company could shadow-deploy a new route optimization model to analyze its predictions compared to the current model.</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Provides a risk-free way to validate the new model under live conditions.</li>
<li>Enables performance comparisons using production data.</li>
</ul>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Requires infrastructure to duplicate traffic and collect results.</li>
<li>Can increase operational costs due to redundant processing.</li>
</ul>
</div>
</div>
</div>
<p>When updating models in production, it’s essential to balance the risks and benefits of each strategy. Monitoring the new model’s performance under real-world conditions is critical to identifying any issues early and ensuring the model meets business and user requirements. Gradually scaling the deployment also helps stabilize operations, especially for systems handling critical tasks.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A Note on Model Evaluation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Once a model is in production, evaluating the performance of new models compared to existing ones becomes a vital part of the lifecycle. Common approaches to evaluate new models include <strong>A/B testing</strong>, where users are randomly assigned to the old or new model to compare outcomes, and <strong>multi-armed bandits</strong>, which dynamically allocate traffic to the best-performing model. These methods are often used in conjunction with deployment strategies like canary or shadow deployments to assess the new model’s impact.</p>
<p>While the topic of model evaluation is beyond the scope of this chapter, resources such as the <a href="https://developers.google.com/machine-learning/guides/rules-of-ml">Google Machine Learning Guide</a> and these articles on <a href="https://www.optimizely.com/optimization-glossary/ab-testing/">A/B testing</a> and <a href="https://www.optimizely.com/optimization-glossary/multi-armed-bandit/">Multi-armed bandits</a> provide valuable insights for those interested in learning more.</p>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">8.6</span> Summary</h2>
<p>In this chapter, we explored the critical aspects of <strong>model deployment</strong>, building on previous discussions of model tracking and versioning. We began by outlining key considerations for deploying machine learning models, covering factors such as inference type, deployment environments, and collaboration with engineering teams. From there, we delved into deployment strategies, comparing batch and real-time inference while also examining different production update strategies like Canary, Blue-Green, and Shadow Deployments.</p>
<p>To bring these concepts to life, we walked through a <strong>hands-on example</strong>, demonstrating how to deploy a trained apple demand forecasting model using <strong>FastAPI</strong>, <strong>Streamlit</strong> and <strong>Docker</strong>. This included retrieving a versioned model from MLflow, developing an API to serve predictions, creating a front-end UI to interact with FastAPI and view the predictions, and containerizing the application for a reliable and scalable deployment pipeline.</p>
<p>While deployment ensures a model is accessible in production, the work does not stop there. Once deployed, models must be <strong>monitored continuously</strong> to detect performance degradation, data drift, and unexpected failures. In the next chapter, we will focus on <strong>model monitoring</strong>, covering best practices, tools, and strategies to ensure that models continue to deliver accurate and reliable predictions in real-world conditions.</p>
</section>
<section id="exercise" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="exercise"><span class="header-section-number">8.7</span> Exercise</h2>
<p>In this exercise, you will build upon the California housing price prediction task from previous exercises<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> and take the next step: <strong>deploying your trained model as an API and integrating a user-friendly Streamlit app as a front-end interface</strong>. This will simulate a real-world scenario where users can interact with the model through an application.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-35-contents" aria-controls="callout-35" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 1: Conceptual Design
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-35" class="callout-35-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Before jumping into implementation, think about how this model would be used in a real-world setting. Answer the following questions:</p>
<ol type="1">
<li><strong>Inference Type</strong>:
<ul>
<li>Would batch inference or real-time inference be more appropriate for a housing price prediction model?</li>
<li>Who are the potential users of this model (e.g., real estate analysts, homebuyers, realtors), and how might they interact with it?</li>
</ul></li>
<li><strong>Deployment Strategy</strong>:
<ul>
<li>If you needed to update the model in production, which strategy (Canary, Blue-Green, or Shadow deployment) would you choose and why?</li>
<li>How would you ensure that the deployed model is reliable and scalable?</li>
</ul></li>
<li><strong>System Integration</strong>:
<ul>
<li>How could this model integrate into a real estate application or property valuation tool?</li>
<li>What external systems (e.g., databases, business intelligence dashboards) might need access to the model’s predictions?</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-36-contents" aria-controls="callout-36" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 2: Hands-On Experimentation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-36" class="callout-36-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Now, you will deploy your trained California housing price model as a FastAPI-based REST API, create a Streamlit front-end UI to interact with FastAPI, and containerize it with Docker.</p>
<ol type="1">
<li><strong>Retrieve Your Trained Model</strong>
<ul>
<li>Load your previously trained California housing price model from MLflow.</li>
<li>Ensure that you are retrieving the correct version from the MLflow model registry.</li>
</ul></li>
<li><strong>Build a FastAPI Application</strong>
<ul>
<li>Create a FastAPI app that exposes an endpoint (<code>/predict</code>) to receive requests and return predicted house prices.</li>
<li>Allow the API to accept both a single input (for real-time inference) and a CSV file (for batch inference).</li>
</ul></li>
<li><strong>Create a Streamlit UI for Model Interaction</strong>
<ul>
<li>Build a Streamlit app that serves as a user-friendly front end to interact with the model.</li>
<li>The app should:
<ul>
<li>Allow users to input features manually for real-time predictions.</li>
<li>Enable uploading a CSV file for batch predictions.</li>
<li>Display the model’s predictions in a clear and structured format.</li>
</ul></li>
<li>The Streamlit app should send requests to the FastAPI backend and display the returned predictions.</li>
</ul></li>
<li><strong>Containerize the API with Docker</strong>
<ul>
<li>Write a Dockerfile to package the FastAPI and Streamlit application.</li>
<li>Build and run the Docker container locally to ensure the API is functioning correctly.</li>
</ul></li>
<li><strong>Test the End-to-End System</strong>
<ul>
<li>Run the FastAPI server and the Streamlit app simultaneously.</li>
<li>Test making predictions through both the Streamlit UI and direct API calls (via <code>curl</code> or <code>requests</code>).</li>
<li>Try making a prediction for a single home and for multiple homes using a CSV file.</li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-37-contents" aria-controls="callout-37" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Part 3: Reflection on Deployment Considerations
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-37" class="callout-37-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Now that you’ve deployed your model, reflect on the process:</p>
<ol type="1">
<li><strong>Challenges &amp; Optimizations</strong>
<ul>
<li>What challenges did you face while deploying the model?</li>
<li>How could you optimize this deployment for scalability and performance?</li>
</ul></li>
<li><strong>Design Principles in Deployment</strong>
<ul>
<li>Review the ML system design principles discussed in <a href="01-intro-ml-system.html#sec-design-principles" class="quarto-xref">Section&nbsp;<span>1.3</span></a>.</li>
<li>How does your deployment incorporate principles like modularity, scalability, reproducibility, or adaptability/flexibility?</li>
<li>What design trade-offs did you encounter, and how would you improve your deployment using these principles?</li>
</ul></li>
</ol>
</div>
</div>
</div>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>See the <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/modelops-requirements.txt">modelops-requirements.txt</a> file.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>See this notebook: <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/model-retrieval.ipynb">https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/model-retrieval.ipynb</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>See this python script: <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/fastapi_app.py">https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/fastapi_app.py</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See this python script: <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/streamlit_app.py">https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/streamlit_app.py</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See this python script: <a href="https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/fastapi_streamlit.py">https://github.com/bradleyboehmke/uc-bana-7075/blob/main/ModelOps/fastapi_streamlit.py</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>See <a href="06-modelops-experimenting.html#sec-model-experimentation-exercise" class="quarto-xref">Exercise&nbsp;<span>6.7</span></a> and <a href="07-modelops-versioning.html#sec-model-version-exercise" class="quarto-xref">Exercise&nbsp;<span>7.6</span></a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./07-modelops-versioning.html" class="pagination-link" aria-label="Model Versioning and Reproducibility">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Model Versioning and Reproducibility</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./09-modelops-monitoring.html" class="pagination-link" aria-label="Model Monitoring">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Monitoring</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/edit/main/08-modelops-deployment.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/bradleyboehmke/uc-bana-7075/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>